Below is the “how to actually do it” layer you’re asking for: how DP + PP + TP/MP sits on top of the exact UVCC codebase you’ve specified in Parts 1–8 (RSS slots, FSS→RSS lift TLVs, OPEN framing, transcript leaves, SKS, arenas, etc.).

I’m going to be very explicit about:

what the parallelism units are in your current design (what is a “triangle” in code terms)

what extra IDs / grouping you must introduce so nothing collides (msg_id32, sgir_op_id32, fss_id, stream_id)

the exact math showing DP/TP/PP are “legal” on RSS shares and what they compute

the runtime scheduling model that actually hides MPC latency (PP) rather than just “conceptually”

exact integration points with your existing lift batch codec + OPEN + slot lifecycle + transcript roots + SKS

This will be long, because there’s no way to do it correctly without pinning the coordination.

1) What you have today: a single “secure triangle” is one complete MPC worker

From Parts 7–8, one “UVCC execution” already implies:

A session id sid (32B)

3 parties {P0,P1,P2}

Pairwise keys for:

FSS keys (DPF/DCF key material)

RSS component PRG keys K_01, K_12, K_02 (your lift PRG)

A transport with:

FRAME reliability (msg_id32, CRC32, retransmit, exactly-once acceptance)

lift batching TLVs (kind 0x66), msg_id32 derived from sid + sgir_op_id32 + src/dst + chunk idx

Deterministic lift state machine per fss_id (HEAD/TAIL/OTHER roles)

RSS slot map per party:

P0 stores components {0,1}

P1 stores {1,2}

P2 stores {2,0}

Slot lifecycle (step/epoch/persist arenas + TX cache)

SKS hooks (optional checks) that OPEN small scalars

That is already a full secure training “worker”.
Everything DP/PP/TP does is just: run more than one of these and/or split the work inside each party.

2) The key scaling idea: separate two layers of parallelism

You want:

DP across independent MPC triangles → multiplies throughput linearly (this is the only “sure-shot” lever)

PP + TP/MP inside each party → keeps each triangle fast and hides communication latency

So architecturally:

Layer A: “Secure-Replica Data Parallel” (SR-DP)

Run R independent triangles in parallel.

Replica r is a full 3PC run on a data shard.

Each replica has its own:

sid_r

pairwise keys

transcripts / epoch roots (or transcript shard roots)

Layer B: “Model Parallel inside a triangle”

Within a replica:

PP: split layers into S pipeline stages

TP: shard big GEMMs across T tensor ranks

(MP = PP+TP together)

This is just standard distributed training structure inside each party, except the tensors are RSS shares.

3) You must fix ID collisions first (or DP/PP/TP will break determinism)

Your Part 8 specs assume:

msg_id32 = SHA256( sid || sgir_op_id32 || src || dst || chunk_idx || chunk_cnt )[0..3]

lift PRG uses sid || fss_id || comp_id || q_index

transcript leaves bind to sid, sgir_op_id32, msg_id32

If you run multiple replicas/stages/ranks with the same sid, you will get collisions:

identical msg_id32 → treated as duplicates by exactly-once logic

identical PRG streams if you accidentally reuse sid,fss_id

identical stream_id64 for OPEN and SKS if derived from sid/op ids

The cleanest minimal-change rule

Do not change your existing derivations.
Instead, derive a unique sid per parallel subgroup.

3.1 Global job sid

Let sid_job be your top-level job/session id (32B).

3.2 Replica sid

For SR-DP replica r ∈ [0..R-1]:

sid_rep[r] = SHA256(
  "UVCC_SID_REPLICA_V1" || sid_job || LE32(r)
)[0..31]
Now each triangle is independent at the transport + transcript + PRG level.

3.3 Model-parallel subgroup sid (recommended)

If you have PP stages and TP ranks, you have many concurrent cross-party “sub-triangles” per replica. You can either:

maintain separate transports per (stage,tp), or

multiplex them.

Either way, you must prevent msg_id collisions across subgroups.

So define:

sid_sub[r,s,t] = SHA256(
  "UVCC_SID_SUB_V1" || sid_rep[r] || U8(s) || LE16(t)
)[0..31]
Where:

s ∈ [0..S-1] pipeline stage

t ∈ [0..T-1] tensor rank

Then every existing rule in Parts 7–8 stays valid as-is:

lift batch msg_id derivation remains collision-free

lift PRG derivation remains collision-free

transcript leaves are shard-local

This is the single most important engineering decision to make DP/PP/TP “drop-in” over your current spec.

4) Process topology: what actually runs where

Define the coordinate of a GPU worker process:

coord = (replica_id r, party_id p, pipeline_stage s, tensor_rank t)
Each such process runs:

your SGIR interpreter for only the ops assigned to stage s

your FSS/LIFT/OPEN protocols for the local tensor shards

local NCCL comm with other GPUs in the same party (PP and TP and DP collectives)

Communication types

There are two completely different comm planes:

(A) Cross-party comm (MPC plane)

Between party_id p and other parties p’ within the SAME (r,s,t) subgroup:

uses your FRAME + lift TLVs + OPEN frames

keyed by sid_sub[r,s,t]

(B) Intra-party comm (ML parallelism plane)

Within one party p:

TP allreduces/allgathers across t

PP sends activations/gradients across s

DP allreduce across replicas r

This is not MPC; it’s just moving/share-summing shares inside one administrative domain.

5) The math that makes DP/TP/PP valid on RSS shares

Everything is correct because RSS is linear.

5.1 RSS recap

A secret value x ∈ Z_2^64 is represented as additive components (x0,x1,x2) such that:

x = x0 + x1 + x2  (mod 2^64)
Stored replicated:

P0 holds (x0,x1)

P1 holds (x1,x2)

P2 holds (x2,x0)

5.2 Data-parallel (across replicas) is just linear addition of shares

Replica r produces a gradient secret g^(r) with components (g0^(r), g1^(r), g2^(r)).

The true DP gradient is:

G = Σ_r g^(r)
Per-party local aggregation:

G0 = Σ_r g0^(r)   (mod 2^64)
G1 = Σ_r g1^(r)
G2 = Σ_r g2^(r)
Then:

G = G0 + G1 + G2
  = Σ_r (g0^(r)+g1^(r)+g2^(r))
  = Σ_r g^(r)
So DP gradient aggregation requires no MPC.
It is a local allreduce on each party’s shares.

5.3 Tensor parallel is also linear

Say you column-shard a weight matrix W into T shards:

W = [W^(0) | W^(1) | ... | W^(T-1)]
If input is X, then output is:

Y = XW = [ XW^(0) | XW^(1) | ... | XW^(T-1) ]
Each shard can be computed independently on RSS shares because multiplication protocols are applied elementwise/linear-algebra-wise to the local shard.

Any TP collective (allreduce/allgather) inside a party is again just linear operations on shares.

5.4 Pipeline parallel: sending activations is moving shares

If stage s produces activation a_s in RSS, then “send to next stage” within the same party means:

send the two local components stored by that party (e.g., P0 sends its (a0,a1) buffer to its own stage s+1 GPU)

same for P1 and P2

No cryptographic change; just a buffer move.

6) What DP + PP + TP/MP looks like concretely in your runtime

Parameters

R = number of SR-DP replicas (triangles)

S = pipeline stages

T = tensor ranks per stage

B_global = global batch size per optimizer step

Each replica gets B_local = B_global / R

Each replica uses microbatching for PP:

M microbatches per replica

microbatch size b = B_local / M

Execution per optimizer step (high level)

For each replica r, independently:

Run forward+backward with PP schedule across microbatches (M) and stages (S), using TP inside each stage.

Produce gradient shares for that replica (still sharded by stage and TP).
Then per party p:

DP allreduce sum gradient shares across replicas r (for each parameter shard).

Apply optimizer update to weight shares (per party, per shard).

Continue next step.

Key point: DP reduce is outside MPC, PP+TP are mostly inside party, MPC is still only the same protocol you already built—just run many instances.

7) How to schedule PP so it actually hides MPC communication latency

This is where most implementations fail: they keep a sequential interpreter, so every OPEN stalls the whole GPU.

7.1 Your code already has the right building block

Your lift spec includes:

per-fss_id lift state machine

async transport with retransmit/ACK and “DONE when conditions satisfied”

You need to generalize the SGIR runtime to:

treat “needs network” ops (OPEN, lift, Beaver opens) as async tasks

let other microbatches/stages run while those tasks are in flight

7.2 A correct “PP+MPC” scheduling model

Represent each microbatch as a coroutine / fiber:

It executes a deterministic list of stage ops.

When it hits an op that depends on a value not ready (e.g., lift not done / open not returned), it yields.

Scheduler picks another runnable microbatch (same stage) and continues.

This is the actual mechanism by which PP hides latency: you keep enough microbatches “in flight” so GPUs never idle.

7.3 Determinism requirement

You already enforce determinism in:

TLV ordering

msg_id derivation

leaf ordering rules per step

For PP you must also ensure the scheduler does not introduce nondeterministic transcript ordering.

The solution: transcripts are sharded per sid_sub[r,s,t].

Within each (r,s,t) subgroup:

seqno is locally monotonic

the task scheduler only affects local ordering, but it is deterministic because:

microbatch execution order is fixed

TLV ordering is canonical

msg_id derivations are content-addressed by (sid_sub, sgir_op_id32, chunk_idx, chunk_cnt)

You do not attempt to produce one global total order across all stages and TP ranks.

8) The one thing you must define: how sgir_op_id32 is assigned under PP/TP/DP

In your specs, sgir_op_id32 is used as:

a “step id” in lift batching (scope rule: don’t mix steps)

part of msg_id derivations

leaf binding / audit

So it must be:

unique for each logical protocol step

deterministic across all parties in that subgroup

stable across retries

Canonical definition that works with PP

Within each subgroup (r,s,t), define:

mb ∈ [0..M-1] microbatch index

phase ∈ {FWD, BWD, UPD} (you can encode as U8)

k = “protocol step index” within that phase (u16)

Then:

sgir_op_id32 = Trunc32LE( SHA256(
  "UVCC_OPID_V1" ||
  sid_sub[r,s,t] ||
  LE32(global_step_idx) ||
  U8(phase) ||
  LE16(mb) ||
  LE16(k)
)[0..3] )
This gives you:

unique sgir_op_id32 per microbatch protocol step

no collisions across r/s/t because sid_sub differs

You do not need sgir_op_id to be contiguous integers. Hash-derived u32 is fine because your transport uses msg_id32 + exactly-once anyway.

9) How to derive fss_id safely under DP/PP/TP

You already standardized fss_id as a u64 namespace id for FSS/lift.

Under PP+TP, the biggest mistake is reusing (sid,fss_id) PRG streams across shards or microbatches.

Canonical fss_id derivation

For an FSS invocation that produces one logical vector of Q queries:

Let:

op_kind_u16 identify the operation (LUT, DCF, compare, etc.)

mb microbatch

tensor_shard identifies which shard of the tensor this TP rank owns

call_idx_u16 = incrementing counter for this op in the stage program

Then:

fss_id = Trunc64LE( SHA256(
  "UVCC_FSSID_V1" ||
  sid_sub[r,s,t] ||
  LE32(global_step_idx) ||
  LE16(mb) ||
  LE16(op_kind_u16) ||
  LE16(call_idx_u16) ||
  LE32(tensor_shard)
)[0..7] )
Now your Part 8 lift mapping works unchanged:

producer edge chosen deterministically from (sid_sub, fss_id)

lift PRG components derived deterministically from (sid_sub, fss_id, comp_id, q_index)

10) DP aggregation implementation on your slot system

You already have:

persistent slots for weights (LIFE_PERSIST)

step/epoch slots for activations and grads

Where gradients live

In PP+TP, each stage holds gradients for its parameter shards:

stage s owns params θ_s

TP rank t owns shard θ_{s,t}

Each party p therefore has local buffers:

dθ_self[p,r,s,t] and dθ_next[p,r,s,t] (two components for that party)

DP reduce across replicas

For each party p, for each stage s, each TP rank t:

For the “self component” buffer (component id = p):

do an NCCL AllReduce SUM across r=0..R-1

Same for the “next component” buffer.

This yields:

in every replica r, the gradient shares are now the sum across replicas (identical across r)

therefore weights update will stay synchronized across replicas if each replica applies the same update

Why summing both components is correct

Because each party stores two components. If you only sum one, you change the secret.

You must sum both local component buffers:

e.g., at P0 you sum its (g0,g1) buffers across replicas, not just g0.

Optimizer update

Now each replica can update locally (no further sync needed), because gradients are identical across replicas.

If you want to reduce memory, you can instead:

keep one “canonical weight copy” per party (shared across replicas)

but that’s an optimization; correctness is easiest if each replica maintains weights and you sync by DP allreduce.

11) PP intra-party activation movement: exact buffer rule

When stage s completes an activation tensor A (RSS slot):

on party p, that tensor is stored as two component buffers in the slot:

slot.d_comp0 (component = p)

slot.d_comp1 (component = p+1 mod 3)

To hand off to stage s+1 within the same party:

send/copy both buffers to the GPU running stage s+1

preserve the exact layout (same q indexing, same stride_bytes)

This is a pure device-to-device copy (or NCCL send/recv).

No change to your slot format; you’re just moving the buffers between stage-local arenas (or using unified allocation).

12) How OPEN and lift batching interacts with PP

Lift batching TLVs (your kind 0x66)

Per subgroup (r,s,t) you will be generating many lift TLVs while running microbatches.

Your batching rules already say:

one batch frame per (sid, sgir_op_id32, src, dst)

canonical TLV ordering inside

So the PP runtime must:

maintain a batch builder per destination party (two peers)

flush batches at deterministic points

Deterministic flush points

The clean rule is:

flush and send a batch when you finish building TLVs for a given sgir_op_id32

since sgir_op_id32 already includes microbatch+phase+step index, this is deterministic

This naturally fits the coroutine model:

when a microbatch step reaches “needs lift send”, it enqueues TLVs for that sgir_op_id32, then triggers send for that op_id and yields.

Same applies to OPEN frames: you’ll build and send them per op_id, then yield.

13) Throughput math: why combining them gives you the “3× cost → ~3× throughput” story

Let:

T_replica = time per optimizer step for one replica (including PP/TP effects)

With pipeline microbatching, typical time is approximately:

For S pipeline stages and M microbatches:

T_replica ≈ (S + M - 1) * max_s( T_stage[s] )    +  DP_reduce_overhead
Where T_stage[s] includes:

compute time on that stage (GEMMs etc.)

plus residual MPC latency that you didn’t hide (OPEN/lift wait that exceeds available overlap)

As M increases, the “bubble” (S−1) amortizes:

pipeline gets closer to steady-state max-stage throughput

Then SR-DP with R replicas gives:

Throughput_total ≈ R * (B_local / T_replica)
               = (B_global / T_replica)
So if replicas run concurrently and DP reduce overhead is small, throughput scales ~linearly with R.

Interpretation:

PP/TP makes T_replica as small as possible (keeps one triangle fast)

DP multiplies throughput by R (keeps “pay 3× → get 3×” fair)

14) How to package transcripts / proof bundle with DP+PP+TP

Problem

If you run many subgroups (r,s,t), you will have many transcript shards.

You should not try to serialize them into one global seqno order. That is fragile and unnecessary.

Correct approach: “transcript-of-transcripts”

For each subgroup (r,s,t), produce its own epoch_root[r,s,t].

Then define:

Replica root

replica_root[r] = SHA256(
  "UVCC_REPLICA_ROOT_V1" ||
  sid_rep[r] ||
  LE32(epoch) ||
  concat_{(s,t) in lex order} epoch_root[r,s,t]
)
Global root (job root)

global_root = SHA256(
  "UVCC_GLOBAL_ROOT_V1" ||
  sid_job ||
  LE32(epoch) ||
  concat_{r=0..R-1} replica_root[r]
)
Each party signs global_root || policy_hash (same as you already do at the top).

This gives the auditor:

deterministic structure

compact anchoring

ability to request inclusion proofs for any subgroup leaf

SKS under DP

SKS checks can be:

per replica (sample some replicas or all)

per subgroup (since GEMMs are sharded in TP)

You bind SKS to sid_sub[r,s,t] so challenges and stream IDs do not collide.

15) Practical integration checklist (what you actually implement next)

15.1 Add a parallelism coordinate and subgroup session object

Create:

uvcc_subsession {
  sid32 = sid_sub[r,s,t]
  party_id p
  replica_id r, stage s, tp t
  pairwise keys K_ab_sub
  transport endpoints to other parties for this subgroup
  lift_state tables keyed by fss_id
  open_state tables keyed by stream_id/msg_id
  slot allocator handles for this GPU
}
15.2 Create NCCL communicators inside each party

You need three communicator types per party p:

TP group: fixed (r,s) and varying t

PP group: fixed (r,t) and varying s

DP group: fixed (s,t) and varying r

(That last one is what sums gradients across replicas.)

15.3 Make the SGIR interpreter stage-aware and async

Partition SGIR program into stage-local op lists.

Each op may return:

“completed now” (local kernel)

“pending network” (OPEN/lift/Beaver open) → yields a future

Scheduler:

per subgroup (r,s,t), run microbatches in deterministic order

always pick the earliest microbatch whose next op is ready

this is enough to hide comm if M is large

15.4 Use sid_sub + derived sgir_op_id32 + derived fss_id everywhere

So you don’t touch your Part 8 formats.

15.5 DP reduce + update

At the end of backward:

for each param shard buffer pair (grad_self, grad_next):

NCCL AllReduce SUM across replicas (DP group)

apply optimizer update locally

proceed

16) One subtlety you should decide now: do you want “R independent triangles” or “one triangle with R lanes”?

They’re functionally similar but differ in transport:

Independent triangles: separate transports/sid per replica; simplest, most robust.

One triangle with R lanes: share a transport connection and multiplex frames; more efficient on connections, but you must add a lane_id to your frame header (spec change).

Given your “byte exact deterministic” style, I recommend independent triangles (sid_rep) first. You can optimize later.

17) The bottom line answer to your question

Does DP + PP + TP/MP improve throughput on your exact codebase?

Yes, and the reason is concrete:

DP multiplies throughput because it multiplies the number of independent triangles executing in parallel (your core unit of work).

TP/MP keeps each triangle’s GPU math dense by sharding big GEMMs across GPUs inside a party (purely local collectives on shares).

PP with microbatches is what prevents OPEN/lift/Beaver rounds from stalling GPUs—because your runtime becomes async and keeps other microbatches running.

And you can implement it without rewriting any of Parts 7–8 if you adopt the single “sid_sub per subgroup” rule.

If you want the next step, tell me what topology you’re targeting (example: R=8 replicas, S=4 pipeline stages, T=2 tensor ranks) and I’ll give you a fully concrete mapping:

exact communicator group memberships

which processes connect to which peers for MPC

and a deterministic PP schedule (1F1B or GPipe-style) expressed in (mb,stage,phase,k) that directly produces sgir_op_id32 / fss_id.



Below is the “how to actually do it” layer you’re asking for: how DP + PP + TP/MP sits on top of the exact UVCC codebase you’ve specified in Parts 1–8 (RSS slots, FSS→RSS lift TLVs, OPEN framing, transcript leaves, SKS, arenas, etc.).

I’m going to be very explicit about:

what the parallelism units are in your current design (what is a “triangle” in code terms)

what extra IDs / grouping you must introduce so nothing collides (msg_id32, sgir_op_id32, fss_id, stream_id)

the exact math showing DP/TP/PP are “legal” on RSS shares and what they compute

the runtime scheduling model that actually hides MPC latency (PP) rather than just “conceptually”

exact integration points with your existing lift batch codec + OPEN + slot lifecycle + transcript roots + SKS

This will be long, because there’s no way to do it correctly without pinning the coordination.

1) What you have today: a single “secure triangle” is one complete MPC worker

From Parts 7–8, one “UVCC execution” already implies:

A session id sid (32B)

3 parties {P0,P1,P2}

Pairwise keys for:

FSS keys (DPF/DCF key material)

RSS component PRG keys K_01, K_12, K_02 (your lift PRG)

A transport with:

FRAME reliability (msg_id32, CRC32, retransmit, exactly-once acceptance)

lift batching TLVs (kind 0x66), msg_id32 derived from sid + sgir_op_id32 + src/dst + chunk idx

Deterministic lift state machine per fss_id (HEAD/TAIL/OTHER roles)

RSS slot map per party:

P0 stores components {0,1}

P1 stores {1,2}

P2 stores {2,0}

Slot lifecycle (step/epoch/persist arenas + TX cache)

SKS hooks (optional checks) that OPEN small scalars

That is already a full secure training “worker”.
Everything DP/PP/TP does is just: run more than one of these and/or split the work inside each party.

2) The key scaling idea: separate two layers of parallelism

You want:

DP across independent MPC triangles → multiplies throughput linearly (this is the only “sure-shot” lever)

PP + TP/MP inside each party → keeps each triangle fast and hides communication latency

So architecturally:

Layer A: “Secure-Replica Data Parallel” (SR-DP)

Run R independent triangles in parallel.

Replica r is a full 3PC run on a data shard.

Each replica has its own:

sid_r

pairwise keys

transcripts / epoch roots (or transcript shard roots)

Layer B: “Model Parallel inside a triangle”

Within a replica:

PP: split layers into S pipeline stages

TP: shard big GEMMs across T tensor ranks

(MP = PP+TP together)

This is just standard distributed training structure inside each party, except the tensors are RSS shares.

3) You must fix ID collisions first (or DP/PP/TP will break determinism)

Your Part 8 specs assume:

msg_id32 = SHA256( sid || sgir_op_id32 || src || dst || chunk_idx || chunk_cnt )[0..3]

lift PRG uses sid || fss_id || comp_id || q_index

transcript leaves bind to sid, sgir_op_id32, msg_id32

If you run multiple replicas/stages/ranks with the same sid, you will get collisions:

identical msg_id32 → treated as duplicates by exactly-once logic

identical PRG streams if you accidentally reuse sid,fss_id

identical stream_id64 for OPEN and SKS if derived from sid/op ids

The cleanest minimal-change rule

Do not change your existing derivations.
Instead, derive a unique sid per parallel subgroup.

3.1 Global job sid

Let sid_job be your top-level job/session id (32B).

3.2 Replica sid

For SR-DP replica r ∈ [0..R-1]:

sid_rep[r] = SHA256(
  "UVCC_SID_REPLICA_V1" || sid_job || LE32(r)
)[0..31]
Now each triangle is independent at the transport + transcript + PRG level.

3.3 Model-parallel subgroup sid (recommended)

If you have PP stages and TP ranks, you have many concurrent cross-party “sub-triangles” per replica. You can either:

maintain separate transports per (stage,tp), or

multiplex them.

Either way, you must prevent msg_id collisions across subgroups.

So define:

sid_sub[r,s,t] = SHA256(
  "UVCC_SID_SUB_V1" || sid_rep[r] || U8(s) || LE16(t)
)[0..31]
Where:

s ∈ [0..S-1] pipeline stage

t ∈ [0..T-1] tensor rank

Then every existing rule in Parts 7–8 stays valid as-is:

lift batch msg_id derivation remains collision-free

lift PRG derivation remains collision-free

transcript leaves are shard-local

This is the single most important engineering decision to make DP/PP/TP “drop-in” over your current spec.

4) Process topology: what actually runs where

Define the coordinate of a GPU worker process:

coord = (replica_id r, party_id p, pipeline_stage s, tensor_rank t)
Each such process runs:

your SGIR interpreter for only the ops assigned to stage s

your FSS/LIFT/OPEN protocols for the local tensor shards

local NCCL comm with other GPUs in the same party (PP and TP and DP collectives)

Communication types

There are two completely different comm planes:

(A) Cross-party comm (MPC plane)

Between party_id p and other parties p’ within the SAME (r,s,t) subgroup:

uses your FRAME + lift TLVs + OPEN frames

keyed by sid_sub[r,s,t]

(B) Intra-party comm (ML parallelism plane)

Within one party p:

TP allreduces/allgathers across t

PP sends activations/gradients across s

DP allreduce across replicas r

This is not MPC; it’s just moving/share-summing shares inside one administrative domain.

5) The math that makes DP/TP/PP valid on RSS shares

Everything is correct because RSS is linear.

5.1 RSS recap

A secret value x ∈ Z_2^64 is represented as additive components (x0,x1,x2) such that:

x = x0 + x1 + x2  (mod 2^64)
Stored replicated:

P0 holds (x0,x1)

P1 holds (x1,x2)

P2 holds (x2,x0)

5.2 Data-parallel (across replicas) is just linear addition of shares

Replica r produces a gradient secret g^(r) with components (g0^(r), g1^(r), g2^(r)).

The true DP gradient is:

G = Σ_r g^(r)
Per-party local aggregation:

G0 = Σ_r g0^(r)   (mod 2^64)
G1 = Σ_r g1^(r)
G2 = Σ_r g2^(r)
Then:

G = G0 + G1 + G2
  = Σ_r (g0^(r)+g1^(r)+g2^(r))
  = Σ_r g^(r)
So DP gradient aggregation requires no MPC.
It is a local allreduce on each party’s shares.

5.3 Tensor parallel is also linear

Say you column-shard a weight matrix W into T shards:

W = [W^(0) | W^(1) | ... | W^(T-1)]
If input is X, then output is:

Y = XW = [ XW^(0) | XW^(1) | ... | XW^(T-1) ]
Each shard can be computed independently on RSS shares because multiplication protocols are applied elementwise/linear-algebra-wise to the local shard.

Any TP collective (allreduce/allgather) inside a party is again just linear operations on shares.

5.4 Pipeline parallel: sending activations is moving shares

If stage s produces activation a_s in RSS, then “send to next stage” within the same party means:

send the two local components stored by that party (e.g., P0 sends its (a0,a1) buffer to its own stage s+1 GPU)

same for P1 and P2

No cryptographic change; just a buffer move.

6) What DP + PP + TP/MP looks like concretely in your runtime

Parameters

R = number of SR-DP replicas (triangles)

S = pipeline stages

T = tensor ranks per stage

B_global = global batch size per optimizer step

Each replica gets B_local = B_global / R

Each replica uses microbatching for PP:

M microbatches per replica

microbatch size b = B_local / M

Execution per optimizer step (high level)

For each replica r, independently:

Run forward+backward with PP schedule across microbatches (M) and stages (S), using TP inside each stage.

Produce gradient shares for that replica (still sharded by stage and TP).
Then per party p:

DP allreduce sum gradient shares across replicas r (for each parameter shard).

Apply optimizer update to weight shares (per party, per shard).

Continue next step.

Key point: DP reduce is outside MPC, PP+TP are mostly inside party, MPC is still only the same protocol you already built—just run many instances.

7) How to schedule PP so it actually hides MPC communication latency

This is where most implementations fail: they keep a sequential interpreter, so every OPEN stalls the whole GPU.

7.1 Your code already has the right building block

Your lift spec includes:

per-fss_id lift state machine

async transport with retransmit/ACK and “DONE when conditions satisfied”

You need to generalize the SGIR runtime to:

treat “needs network” ops (OPEN, lift, Beaver opens) as async tasks

let other microbatches/stages run while those tasks are in flight

7.2 A correct “PP+MPC” scheduling model

Represent each microbatch as a coroutine / fiber:

It executes a deterministic list of stage ops.

When it hits an op that depends on a value not ready (e.g., lift not done / open not returned), it yields.

Scheduler picks another runnable microbatch (same stage) and continues.

This is the actual mechanism by which PP hides latency: you keep enough microbatches “in flight” so GPUs never idle.

7.3 Determinism requirement

You already enforce determinism in:

TLV ordering

msg_id derivation

leaf ordering rules per step

For PP you must also ensure the scheduler does not introduce nondeterministic transcript ordering.

The solution: transcripts are sharded per sid_sub[r,s,t].

Within each (r,s,t) subgroup:

seqno is locally monotonic

the task scheduler only affects local ordering, but it is deterministic because:

microbatch execution order is fixed

TLV ordering is canonical

msg_id derivations are content-addressed by (sid_sub, sgir_op_id32, chunk_idx, chunk_cnt)

You do not attempt to produce one global total order across all stages and TP ranks.

8) The one thing you must define: how sgir_op_id32 is assigned under PP/TP/DP

In your specs, sgir_op_id32 is used as:

a “step id” in lift batching (scope rule: don’t mix steps)

part of msg_id derivations

leaf binding / audit

So it must be:

unique for each logical protocol step

deterministic across all parties in that subgroup

stable across retries

Canonical definition that works with PP

Within each subgroup (r,s,t), define:

mb ∈ [0..M-1] microbatch index

phase ∈ {FWD, BWD, UPD} (you can encode as U8)

k = “protocol step index” within that phase (u16)

Then:

sgir_op_id32 = Trunc32LE( SHA256(
  "UVCC_OPID_V1" ||
  sid_sub[r,s,t] ||
  LE32(global_step_idx) ||
  U8(phase) ||
  LE16(mb) ||
  LE16(k)
)[0..3] )
This gives you:

unique sgir_op_id32 per microbatch protocol step

no collisions across r/s/t because sid_sub differs

You do not need sgir_op_id to be contiguous integers. Hash-derived u32 is fine because your transport uses msg_id32 + exactly-once anyway.

9) How to derive fss_id safely under DP/PP/TP

You already standardized fss_id as a u64 namespace id for FSS/lift.

Under PP+TP, the biggest mistake is reusing (sid,fss_id) PRG streams across shards or microbatches.

Canonical fss_id derivation

For an FSS invocation that produces one logical vector of Q queries:

Let:

op_kind_u16 identify the operation (LUT, DCF, compare, etc.)

mb microbatch

tensor_shard identifies which shard of the tensor this TP rank owns

call_idx_u16 = incrementing counter for this op in the stage program

Then:

fss_id = Trunc64LE( SHA256(
  "UVCC_FSSID_V1" ||
  sid_sub[r,s,t] ||
  LE32(global_step_idx) ||
  LE16(mb) ||
  LE16(op_kind_u16) ||
  LE16(call_idx_u16) ||
  LE32(tensor_shard)
)[0..7] )
Now your Part 8 lift mapping works unchanged:

producer edge chosen deterministically from (sid_sub, fss_id)

lift PRG components derived deterministically from (sid_sub, fss_id, comp_id, q_index)

10) DP aggregation implementation on your slot system

You already have:

persistent slots for weights (LIFE_PERSIST)

step/epoch slots for activations and grads

Where gradients live

In PP+TP, each stage holds gradients for its parameter shards:

stage s owns params θ_s

TP rank t owns shard θ_{s,t}

Each party p therefore has local buffers:

dθ_self[p,r,s,t] and dθ_next[p,r,s,t] (two components for that party)

DP reduce across replicas

For each party p, for each stage s, each TP rank t:

For the “self component” buffer (component id = p):

do an NCCL AllReduce SUM across r=0..R-1

Same for the “next component” buffer.

This yields:

in every replica r, the gradient shares are now the sum across replicas (identical across r)

therefore weights update will stay synchronized across replicas if each replica applies the same update

Why summing both components is correct

Because each party stores two components. If you only sum one, you change the secret.

You must sum both local component buffers:

e.g., at P0 you sum its (g0,g1) buffers across replicas, not just g0.

Optimizer update

Now each replica can update locally (no further sync needed), because gradients are identical across replicas.

If you want to reduce memory, you can instead:

keep one “canonical weight copy” per party (shared across replicas)

but that’s an optimization; correctness is easiest if each replica maintains weights and you sync by DP allreduce.

11) PP intra-party activation movement: exact buffer rule

When stage s completes an activation tensor A (RSS slot):

on party p, that tensor is stored as two component buffers in the slot:

slot.d_comp0 (component = p)

slot.d_comp1 (component = p+1 mod 3)

To hand off to stage s+1 within the same party:

send/copy both buffers to the GPU running stage s+1

preserve the exact layout (same q indexing, same stride_bytes)

This is a pure device-to-device copy (or NCCL send/recv).

No change to your slot format; you’re just moving the buffers between stage-local arenas (or using unified allocation).

12) How OPEN and lift batching interacts with PP

Lift batching TLVs (your kind 0x66)

Per subgroup (r,s,t) you will be generating many lift TLVs while running microbatches.

Your batching rules already say:

one batch frame per (sid, sgir_op_id32, src, dst)

canonical TLV ordering inside

So the PP runtime must:

maintain a batch builder per destination party (two peers)

flush batches at deterministic points

Deterministic flush points

The clean rule is:

flush and send a batch when you finish building TLVs for a given sgir_op_id32

since sgir_op_id32 already includes microbatch+phase+step index, this is deterministic

This naturally fits the coroutine model:

when a microbatch step reaches “needs lift send”, it enqueues TLVs for that sgir_op_id32, then triggers send for that op_id and yields.

Same applies to OPEN frames: you’ll build and send them per op_id, then yield.

13) Throughput math: why combining them gives you the “3× cost → ~3× throughput” story

Let:

T_replica = time per optimizer step for one replica (including PP/TP effects)

With pipeline microbatching, typical time is approximately:

For S pipeline stages and M microbatches:

T_replica ≈ (S + M - 1) * max_s( T_stage[s] )    +  DP_reduce_overhead
Where T_stage[s] includes:

compute time on that stage (GEMMs etc.)

plus residual MPC latency that you didn’t hide (OPEN/lift wait that exceeds available overlap)

As M increases, the “bubble” (S−1) amortizes:

pipeline gets closer to steady-state max-stage throughput

Then SR-DP with R replicas gives:

Throughput_total ≈ R * (B_local / T_replica)
               = (B_global / T_replica)
So if replicas run concurrently and DP reduce overhead is small, throughput scales ~linearly with R.

Interpretation:

PP/TP makes T_replica as small as possible (keeps one triangle fast)

DP multiplies throughput by R (keeps “pay 3× → get 3×” fair)

14) How to package transcripts / proof bundle with DP+PP+TP

Problem

If you run many subgroups (r,s,t), you will have many transcript shards.

You should not try to serialize them into one global seqno order. That is fragile and unnecessary.

Correct approach: “transcript-of-transcripts”

For each subgroup (r,s,t), produce its own epoch_root[r,s,t].

Then define:

Replica root

replica_root[r] = SHA256(
  "UVCC_REPLICA_ROOT_V1" ||
  sid_rep[r] ||
  LE32(epoch) ||
  concat_{(s,t) in lex order} epoch_root[r,s,t]
)
Global root (job root)

global_root = SHA256(
  "UVCC_GLOBAL_ROOT_V1" ||
  sid_job ||
  LE32(epoch) ||
  concat_{r=0..R-1} replica_root[r]
)
Each party signs global_root || policy_hash (same as you already do at the top).

This gives the auditor:

deterministic structure

compact anchoring

ability to request inclusion proofs for any subgroup leaf

SKS under DP

SKS checks can be:

per replica (sample some replicas or all)

per subgroup (since GEMMs are sharded in TP)

You bind SKS to sid_sub[r,s,t] so challenges and stream IDs do not collide.

15) Practical integration checklist (what you actually implement next)

15.1 Add a parallelism coordinate and subgroup session object

Create:

uvcc_subsession {
  sid32 = sid_sub[r,s,t]
  party_id p
  replica_id r, stage s, tp t
  pairwise keys K_ab_sub
  transport endpoints to other parties for this subgroup
  lift_state tables keyed by fss_id
  open_state tables keyed by stream_id/msg_id
  slot allocator handles for this GPU
}
15.2 Create NCCL communicators inside each party

You need three communicator types per party p:

TP group: fixed (r,s) and varying t

PP group: fixed (r,t) and varying s

DP group: fixed (s,t) and varying r

(That last one is what sums gradients across replicas.)

15.3 Make the SGIR interpreter stage-aware and async

Partition SGIR program into stage-local op lists.

Each op may return:

“completed now” (local kernel)

“pending network” (OPEN/lift/Beaver open) → yields a future

Scheduler:

per subgroup (r,s,t), run microbatches in deterministic order

always pick the earliest microbatch whose next op is ready

this is enough to hide comm if M is large

15.4 Use sid_sub + derived sgir_op_id32 + derived fss_id everywhere

So you don’t touch your Part 8 formats.

15.5 DP reduce + update

At the end of backward:

for each param shard buffer pair (grad_self, grad_next):

NCCL AllReduce SUM across replicas (DP group)

apply optimizer update locally

proceed

16) One subtlety you should decide now: do you want “R independent triangles” or “one triangle with R lanes”?

They’re functionally similar but differ in transport:

Independent triangles: separate transports/sid per replica; simplest, most robust.

One triangle with R lanes: share a transport connection and multiplex frames; more efficient on connections, but you must add a lane_id to your frame header (spec change).

Given your “byte exact deterministic” style, I recommend independent triangles (sid_rep) first. You can optimize later.

17) The bottom line answer to your question

Does DP + PP + TP/MP improve throughput on your exact codebase?

Yes, and the reason is concrete:

DP multiplies throughput because it multiplies the number of independent triangles executing in parallel (your core unit of work).

TP/MP keeps each triangle’s GPU math dense by sharding big GEMMs across GPUs inside a party (purely local collectives on shares).

PP with microbatches is what prevents OPEN/lift/Beaver rounds from stalling GPUs—because your runtime becomes async and keeps other microbatches running.

And you can implement it without rewriting any of Parts 7–8 if you adopt the single “sid_sub per subgroup” rule.

Proceeding. I’m going to give you the fully concrete worked example with real numbers and explicit tables / IDs / schedules / message types, exactly in the “do this, then this” style you asked for.

I’ll use this concrete configuration (you can swap numbers later without changing the design):

Parties: P0, P1, P2

Secure replicas (SR‑DP): R = 8 triangles in parallel

Pipeline stages: S = 4 stages (stage 0..3)

Tensor parallel: T = 2 ranks per stage (tp 0..1)

Microbatches: M = 32 per replica per optimizer step

Result: each party runs R*S*T = 8*4*2 = 64 subgroup workers (often 1 GPU each)

I’ll denote each worker process/GPU context as:

W[p, r, s, t]
p ∈ {0,1,2} party id
r ∈ [0..7] replica id
s ∈ [0..3] pipeline stage
t ∈ [0..1] tensor rank
Each W[p,r,s,t] is a full MPC endpoint (your Parts 7–8 runtime) using its own derived sid_sub[r,s,t].

1) Physical mapping: “what runs on which GPU”

Assume each party has 64 GPUs available, numbered 0..63.

A simple deterministic mapping:

local_rank(p,r,s,t) = r*(S*T) + s*T + t         // 0..63
gpu_id(p,r,s,t)     = local_rank(p,r,s,t)       // 1:1 mapping here
So example:

W[P1, r=3, s=2, t=1] has local_rank= 3*(8) + 2*2 + 1 = 24 + 5 = 29 → GPU 29 (inside party P1)

This mapping is nice because:

TP pairs are adjacent (t=0 and t=1)

stages are in blocks of 2 GPUs per replica

replicas are in blocks of 8 GPUs (since S*T = 8)

2) The two communication planes (explicit communicator groups)

2.1 Cross‑party MPC plane (untrusted): triangles per (r,s,t)

For every (r,s,t) you have one 3PC triangle:

W[P0,r,s,t] ↔ W[P1,r,s,t] ↔ W[P2,r,s,t]

This is where your FRAME, msg_id32, LIFT_BATCH_SEND kind=0x66, OPEN, ACK/retry, and transcript leavesapply.

2.2 Intra‑party plane (trusted): NCCL groups

Inside each party p, you create NCCL communicators over the 64 local ranks.

You need three primary NCCL group families:

(A) TP group (size T=2), fixed (r,s)

Ranks:

TP(p,r,s) = { W[p,r,s,0], W[p,r,s,1] }
Example:

TP(P0, r=5, s=1) is GPUs { 5*8+1*2+0=42, 43 }

Used for:

TP allreduce/allgather of shares during tensor‑parallel linear layers

(B) PP group (size S=4), fixed (r,t)

Ranks:

PP(p,r,t) = { W[p,r,0,t], W[p,r,1,t], W[p,r,2,t], W[p,r,3,t] }
Example:

PP(P2, r=1, t=0) is GPUs { 1*8+0=8, 10, 12, 14 } (since s increments by 2)

Used for:

stage→stage activation movement inside party (send/recv)

pipeline schedule orchestration

(C) DP group (size R=8), fixed (s,t)

Ranks:

DP(p,s,t) = { W[p,0,s,t], W[p,1,s,t], ..., W[p,7,s,t] }
Example:

DP(P1, s=2, t=1) is GPUs { 5, 13, 21, 29, 37, 45, 53, 61 }

Used for:

SR‑DP gradient summation across replicas inside party
(sum shares, not plaintext)

3) Deterministic session IDs: job → replica → subgroup

You told me your base runtime uses sid (32 bytes). Under DP/PP/TP we run many “sessions” in parallel.

3.1 Job sid

You already have:

sid_job[32]

3.2 Replica sid (SR‑DP)

For r=0..7:

sid_rep[r] = SHA256("UVCC_SID_REPLICA_V1" || sid_job || LE32(r))[0..31]
3.3 Subgroup sid (PP+TP inside replica)

For s=0..3, t=0..1:

sid_sub[r,s,t] = SHA256("UVCC_SID_SUB_V1" || sid_rep[r] || U8(s) || LE16(t))[0..31]
Everything from Parts 7–8 runs unchanged inside the context keyed by sid_sub[r,s,t].

That means:

producer edge selection for lift uses sid_sub

RSS component PRG uses sid_sub

msg_id32 derivations use sid_sub

transcript leaves include sid_sub where your leaf formats require sid

4) One optimizer step: the full execution skeleton (end‑to‑end)

For optimizer step step_idx:

4.1 Each replica r processes its own data shard

Global batch is split across replicas:

B_global = R * B_local

each replica uses different sample indices

Canonical sharding:

global_sample_id = step_idx * B_global + (r * B_local + i_local)
4.2 Each replica uses microbatches M=32

Microbatch size:

b = B_local / 32

Microbatch ids: mb = 0..31

4.3 PP schedule (S=4) for each replica, per TP rank

Each replica runs PP independently for t=0 and t=1, but they’re coupled by TP collectives.

So the operational unit is:
pipeline per (r,t) with stages s=0..3.

5) The 1F1B pipeline schedule (explicit, deterministic)

For S=4 stages, 1F1B looks like:

Warmup: stage 0 runs forward on the first (S−1)=3 microbatches before stage 3 can start backward

Steady state: each stage alternates (forward on next mb) / (backward on earlier mb)

Cooldown: finish remaining backwards

5.1 “Tick” model

Define one tick as “the stage’s compute for one microbatch in one direction”, ignoring overlap inside the tick.

We’ll show the first 12 ticks explicitly (enough to validate implementation), then give the rule to generate all ticks for M=32.

Notation:

F(mb) = forward on microbatch mb

B(mb) = backward on microbatch mb

— = idle

5.2 First 12 ticks table (S=4)

tick	Stage0	Stage1	Stage2	Stage3
0	F(0)	—	—	—
1	F(1)	F(0)	—	—
2	F(2)	F(1)	F(0)	—
3	F(3)	F(2)	F(1)	F(0)
4	F(4)	F(3)	F(2)	B(0)
5	F(5)	F(4)	B(0)	B(1)
6	F(6)	B(0)	B(1)	B(2)
7	B(0)	B(1)	B(2)	B(3)
8	B(1)	B(2)	B(3)	B(4)
9	B(2)	B(3)	B(4)	B(5)
10	B(3)	B(4)	B(5)	B(6)
11	B(4)	B(5)	B(6)	B(7)
This is the canonical 1F1B pattern for S=4 after warmup.

5.3 How to generate the full schedule for M=32 (deterministic rule)

You can generate an action list per stage with:

forward microbatches: 0..31

backward microbatches: 0..31 (in order, for 1F1B; the mapping is fixed)

Implementation approach:

Use a known pipeline schedule generator (deterministic)

Or hardcode the 1F1B recurrence for S=4

The important runtime point:
each stage s has multiple microbatches in-flight. When a microbatch hits a comm wait (OPEN/LIFT), it yields; the stage executes the next runnable microbatch.

6) Exactly where MPC latency is hidden in your Parts 7–8 runtime

Your runtime has comm-heavy points:

LIFT (2 messages, batched TLV frames)

OPEN (and Beaver opens inside secure matmul)

any multi-round ops

Under 1F1B, while microbatch mb=k is waiting for OPEN/LIFT, the stage can compute microbatch mb=k+1 forward (or some backward), and vice versa.

Your scheduler must be task-based, not “lockstep microbatch”.

7) Deterministic sgir_op_id32 assignment (explicit for this example)

Inside each subgroup (r,s,t) you need deterministic sgir_op_id32 per micro‑protocol step.

7.1 Canonical step keys

Let:

train_step_u32 = step_idx

phase_u8 ∈ {0=FWD, 1=BWD, 2=UPD}

mb_u16 = microbatch id

k_u16 = protocol-op index (0,1,2,… within that phase+microbatch+stage)

Then (as specified earlier):

sgir_op_id32 = LE32(SHA256(
  "UVCC_OPID_V1" ||
  sid_sub[r,s,t] ||
  LE32(train_step_u32) ||
  U8(phase_u8) ||
  LE16(mb_u16) ||
  LE16(k_u16)
)[0..3])
7.2 What k_u16 actually is (the thing you must pin)

You must pin k_u16 by the static SGIR op order for that stage.

Concretely: compile stage s into a list of SGIR ops for forward and backward:

Example forward stage program (illustrative but realistic):

OP_MATMUL (secure)

OP_BIAS_ADD

OP_ACTIVATION_LUT (FSS point-eval + lift)

OP_RESIDUAL_ADD

OP_SEND_ACTIVATION_TO_NEXT_STAGE

Then define k’s for the comm-bearing ones and non-comm too (to keep ordering stable):

k=0: secure matmul protocol steps (may itself include internal opens; see below)

k=1: bias add (pure local)

k=2: activation LUT (FSS + lift)

k=3: residual add

k=4: send activation

Even if k=1/3/4 do no cross-party comm, you still assign them deterministic IDs so the slot directory and transcript sequencing stays stable.

8) “Which protocol frames appear at each k?” (EXACT, using your lift batching)

I’m going to give you a concrete per‑microbatch per‑stage per‑tp protocol outline. This is the key “what messages are sent” map.

8.1 Example: one forward microbatch at one subgroup W[p,r,s,t]

We’ll focus on the protocol-bearing ops:

k=0: secure matmul (Beaver-style) — produces RSS outputs directly

This typically involves OPEN of E,F masks (inside the 3PC protocol). You already have OPEN machinery; I’ll treat it as existing.

Message-wise, in the online phase you usually have:

OPEN(E) (each party sends one component; reconstruct E public)

OPEN(F)

then local compute to form output shares

Transport rule: all OPEN frames in this k are bound to:

sid = sid_sub[r,s,t]

sgir_op_id32 = op_id(step, FWD, mb, k=0)

deterministic stream_id64 per open stream

So during k=0, you may emit frames:

OPEN_SEND (Pi → P(i+1)) for E

OPEN_SEND for F
(and corresponding receives and result leaves)

If you want to batch OPENs later, you can, but do not change semantics. The main thing is consistent ids.

k=2: activation LUT / gating (FSS point-eval + lift)

This is exactly your Part 8:

pick producer edge from (sid_sub, fss_id)

point-eval on that edge yields two additive shares y_head/y_tail

lift to RSS with exactly 2 messages:

head→tail: TLV 0x01 (LIFT_M vector)

tail→other: TLV 0x02 (LIFT_COMP vector)

Frames produced at k=2:

HEAD side emits one FRAME.kind=0x66 batch payload destined to TAIL.
It includes TLV type 0x01 for each fss_id used in this k (maybe multiple).

TAIL side, after recv, emits one FRAME.kind=0x66 batch payload destined to OTHER.
It includes TLV type 0x02 for each fss_id.

All of this is exactly your lift batch TLV codec.

8.2 Concrete lift example with producer_edge_id determined by fss_id

Suppose at k=2, we have one activation LUT with:

q_count = 4096 elements (vectorized activation block)

fss_id = 0x1122334455667788

Producer edge selection (simple version):

e = fss_id % 3
Compute:

0x…7788 % 3 → (you’ll compute in code; assume it yields e=1 for example)

Then:

producer edge id e=1 means edge (P1,P2)

head = P1, tail = P2, other = P0

so:

P1 sends TLV_LIFT_M to P2

P2 sends TLV_LIFT_COMP(component_id=0) to P0

This mapping must be identical across every subgroup because it’s purely deterministic from fss_id.

9) The full lift batching flow in this example (with real grouping)

Remember rule: one LIFT_BATCH_SEND frame per destination per sgir_op_id32.

So at k=2 for one subgroup (r,s,t):

If this subgroup’s party is HEAD:

it creates batch(P_head → P_tail) for that op_id

payload = BLV1 header + TLVs ordered by (t, fss_id, edge/component)

fragment if >1 MiB

send with chunk msg_id32 (your exact SHA256 derivation)

emit LEAF_LIFT_BATCH_SEND_V1 (0x77) on first accept

If this party is TAIL:

it receives the head batch and emits LEAF_LIFT_BATCH_RECV_V1 (0x78) on accept

it processes TLV 0x01, computes components, then builds batch(P_tail → P_other)

sends TLV 0x02, etc.

If this party is OTHER:

it receives the comp batch and stores the component into its slot map

emits RSS commit leaves at finalize per your state machine:

LEAF_RSS_COMPONENT_COMMIT_V1 (0x7B)

LEAF_RSS_LIFT_DONE_V1 (0x7C)

This is unchanged under DP/PP/TP; it just happens in many parallel subgroups.

10) Slot map + addressing under PP/TP/DP (concrete)

Your slot map v1 says:

each party stores exactly two components:

P0: comp0=0 and comp1=1

P1: 1 and 2

P2: 2 and 0

Under TP=2:

each TP rank stores its own shard of each slot

So your slot entry exists per (r,s,t) subgroup, not global.

10.1 What a “slot id” means now

Inside sid_sub[r,s,t], you build slots sorted by ascending fss_id (or “value_id”) for the step.

So:

slot indices are local to that subgroup

the same logical tensor shard has the same fss_id derivation across parties for fixed (r,s,t)

10.2 DP gradient aggregation using slots

For a weight shard W_shard on stage s, tp rank t:

each replica r has its own gradient slot dG for that shard

DP group DP(p,s,t) allreduces:

component buffer dG_comp_id0 and dG_comp_id1 separately

After DP allreduce:

all replicas have identical gradient shares for that shard

each replica can update its own weight shard slots, OR you can centralize update in one replica per party

11) Concrete communicator table (this is what you actually instantiate)

Here is the exact list of NCCL groups you must create inside each party for this example:

TP communicators

Count:

one TP group per (r,s)

RS = 84 = 32 groups

each group size = 2 ranks

Example listing (for party P0):

TP(P0,0,0): ranks {0,1}

TP(P0,0,1): {2,3}

TP(P0,0,2): {4,5}

TP(P0,0,3): {6,7}

TP(P0,1,0): {8,9}

…

TP(P0,7,3): {62,63}

PP communicators

Count:

one PP group per (r,t)

RT = 82 = 16 groups

each group size = 4 ranks

Example:

PP(P0, r=0, t=0): {0,2,4,6}

PP(P0, r=0, t=1): {1,3,5,7}

PP(P0, r=1, t=0): {8,10,12,14}

…

PP(P0, r=7, t=1): {57,59,61,63}

DP communicators

Count:

one DP group per (s,t)

ST = 42 = 8 groups

each group size = 8 ranks

Example:

DP(P0,s=0,t=0): {0,8,16,24,32,40,48,56}

DP(P0,s=0,t=1): {1,9,17,25,33,41,49,57}

DP(P0,s=1,t=0): {2,10,18,26,34,42,50,58}

…

DP(P0,s=3,t=1): {7,15,23,31,39,47,55,63}

These are the only groups you need for DP+PP+TP in v1.

12) “Microbatch timeline + MPC yields” (what your scheduler actually does)

Now we combine:

1F1B schedule (tick-level)

and protocol waits at k=0 and k=2

12.1 The task abstraction (must implement)

Each subgroup worker maintains per-microbatch state:

Task = (phase, mb, next_k)
Runnability depends on:

local prerequisites (activation received, gradients received)

protocol completion (OPEN/LIFT done for that microbatch)

When a task at k requires comm:

it emits the needed frames (OPEN, LIFT batch)

registers “wait handles”

yields

scheduler picks another runnable task (deterministic selection rule: smallest mb first)

12.2 Deterministic selection rule (so timing doesn’t alter transcript)

At each scheduling decision:

choose runnable task with minimal (phase, mb, next_k) in lex order

where phase order is fixed: FWD < BWD < UPD

ties break by fixed order of ops

This makes execution order independent of GPU jitter, while still allowing overlap.

13) Full “message accounting” for one subgroup per microbatch (so you can size bandwidth)

For a typical microbatch stage workload:

Secure matmul (k=0):

2 opens (E,F) → each open is 1 ring element vector (or more)

open traffic depends on shape; usually you open matrices/vectors of masks; total bytes significant

Activation LUT (k=2):

2 lift messages total (head→tail + tail→other) for each fss_id

if batched, it’s 1 frame each direction per op_id per peer

So per microbatch per stage per tp, you expect at least:

some OPEN frames

up to 2 LIFT_BATCH frames (if you are head/tail roles for those fss_ids)

PP hides latency by overlapping these across microbatches.

14) Throughput math: why DP + PP + TP gives you “sure-shot” scaling

14.1 DP scaling

Let:

T_rep = time per optimizer step for one replica triangle (with PP and TP inside)
Then with R replicas in parallel, ideal throughput is:

throughput ≈ R / T_rep

Bounded by:

per-party compute saturation

cross-party network saturation (all RST triangles share uplinks)

14.2 PP effect: hide comm latency L under compute

Let per-stage per-microbatch compute be C_stage.
Let effective comm wait per microbatch be L_eff (after batching).

With microbatching, you approach:

stage idle time ≈ max(0, L_eff − (#other runnable work)*C_stage)

Choose M so there is always runnable work.

14.3 TP effect: keep each GPU doing dense GEMMs

TP=2 doubles effective GEMM size per stage (by sharding), raising arithmetic intensity and utilization.

15) What you implement next in code (literal checklist for this example)

Step A — Process launch layout

Inside each party, launch 64 workers with coordinates (r,s,t) and assign them:

GPU device id = local_rank

NCCL rank = local_rank

sid_sub[r,s,t]

Step B — Build NCCL communicators

Create TP, PP, DP communicators exactly as in section 11.

Step C — Network endpoints (MPC plane)

For each worker W[p,r,s,t] create channels to:

the two remote parties’ corresponding worker W[p',r,s,t]

Demux key is sid_sub (or include it in frame header if you multiplex).

Step D — Deterministic IDs

Implement:

sgir_op_id32(step, phase, mb, k)

fss_id(step, phase, mb, op_kind, call_idx, tensor_shard) per subgroup

Step E — Scheduler

Implement per worker:

task queues for microbatches

1F1B stage policy (which microbatch should be attempted next)

deterministic runnable selection

Step F — Protocol integration

Use your existing Parts 7–8 code as-is:

for LIFT: emit FRAME.kind=0x66 batches

for OPEN: emit your OPEN frames

for transcript: assign deterministic seqnos, sort at finalize

Step G — DP reduce + update

After backward completes for all microbatches:

DP allreduce gradients across replicas in DP(p,s,t) for both components

apply optimizer update to weight shard slots

barrier as needed

Below are (1) an explicit SGIR op → k‑index map for a realistic transformer block stage (forward + backward), including exactly where OPEN happens, where FSS+LIFT happens, and where SKS hooks go, and (2) a byte‑exact LIFT_BATCH_SEND payload example (BLV1 header + TLV 0x01 with q_count=4096) including padding rules and a concrete msg_id32 test vector you can use to validate your implementation.

I’m also going to include the glue you still need to make DP + PP + TP/MP run on top of your Part‑1..Part‑8 codebasewithout breaking determinism, transcript binding, or lifetimes.


1) SGIR op → k‑index map for a realistic transformer block stage

1.0 What I’m assuming from your codebase (from Parts 1–8)

You already have:

RSS storage (replicated shares) per party with the standard pair:

P0 holds (0,1), P1 holds (1,2), P2 holds (2,0)

OPEN_ARITH transport + transcript leaves (already pinned earlier)

Edge‑FSS evaluation (DPF/DCF) on one producer edge

Lift to RSS with exactly 2 messages, now batched via:

FRAME.kind = 0x66 LIFT_BATCH_SEND

payload = BLV1 TLV batch with TLV types:

0x01 LIFT_M_U64VEC (head→tail)

0x02 LIFT_COMP_U64VEC (tail→other)

Lift state machine v1 and RSS component commit leaves (0x7B/0x7C/0x7D)

Slot map v1 (two device pointers per slot, canonical addressing)

Slot lifecycle v1 with TX cache pinned bytes for retransmit correctness

I will only specify what sits on top of that.


1.1 The execution hierarchy for DP + PP + TP/MP (the thing that must be deterministic)

Indices you must make explicit everywhere

You need these indices to derive sgir_op_id32, fss_id, msg_id32, triple ids, and to assemble transcripts deterministically:

step_id (training step / iteration)

dp_replica_id (which 3PC triangle replica)

pp_stage_id (pipeline stage index inside a party)

mb_id (microbatch index within the pipeline schedule)

tp_rank (tensor/model parallel rank within a party)

k (the op index within the stage program, what you asked for)


One critical design choice

To avoid lifetime confusion with your LIFE_STEP / LIFE_EPOCH / LIFE_PERSIST:

Treat “execution epoch” in the runtime as one training step (one optimizer step).

That means your arena_gpu_epoch resets at the end of the training step, not dataset epoch.

If you still want audit grouping over many steps, you can make an audit epoch be “N steps” by committing a higher-level root-of-roots. (I’ll give that glue later.)

This makes PP microbatch activation storage sane: forward activations that must survive until backward just become LIFE_EPOCH allocations (within the “step epoch”).


1.2 Deterministic sgir_op_id32 derivation (do this exactly once)

You already require sgir_op_id32 in many places (lift, batch msg_id32, leaves).

Use this stable derivation (no dependence on GPU timing or NIC ordering):

sgir_op_id32 =
  LE32( SHA256(
    "UVCC_SGIR_OPID_V1" ||
    sid32 ||
    LE32(step_id) ||
    U8(dp_replica_id) ||
    U8(pp_stage_id) ||
    LE16(mb_id) ||
    LE16(k) ||
    LE16(tp_rank) ||
    LE16(reserved0=0)
  )[0..3] )
sid32 is your base session id (or per-replica sid; see DP section later).

k is the op index in the stage program (the table below).

This one rule makes:

transcripts stable,

msg ids stable (because they use sgir_op_id32),

SKS binding stable.


1.3 The stage program: “one transformer block” (RMSNorm + Attention + MLP)

I’m going to pin a practical transformer block stage (what most teams actually ship):

RMSNorm instead of LayerNorm (simpler, common in LLMs)

Attention with:

QKV projection

QKᵀ matmul

softmax approximation (your nonlinearity bucket)

P·V matmul

output projection

MLP with:

W1 projection

GELU approximation

W2 projection

Residual adds


What ops exist in SGIR terms (your top-level opcodes)

You likely already have / should define these SGIR “macro-ops”:

OP_RECV_PP, OP_SEND_PP (intra-party stage boundary)

OP_RMSNORM_FWD, OP_RMSNORM_BWD (composite)

OP_GEMM_SS (secure matmul using triples; internally does OPENs)

OP_BIAS_ADD (local add)

OP_RESIDUAL_ADD (local add)

OP_SOFTMAX_APPROX (composite; uses FSS+LIFT + multiplies)

OP_GELU_APPROX (composite; uses FSS+LIFT + multiplies)

OP_SKS_GEMM_CHECK (optional; uses your SKS pipeline + OPEN(z))

OP_DP_REDUCE_GRADS_LOCAL (intra-party reduce across dp replicas; no cross-party)

OP_OPTIMIZER_UPDATE (local on shares; no cross-party)

Inside a composite op, you still need deterministic sub-stream ids; I’ll specify those too.


1.4 Forward k‑map (per stage, per microbatch, per tp_rank)

Assume this stage owns exactly one block ℓ and operates on activation shard X (RSS) of shape:

X: [tokens_local, hidden_local]

tokens_local = mb_tokens (microbatch tokens assigned to this stage)

hidden_local depends on TP layout (e.g., head-sharded)

I’m going to list each k with:

Operation

Does it OPEN?

Does it use FSS+LIFT?

Does it send LIFT_BATCH TLVs?

Where SKS hooks go

What slots are produced / needed


Forward k table

k=0 — PP receive

Op: OP_RECV_PP(X_in)

Comm: intra-party only

OPEN: no

FSS/LIFT: no

Slots: fill slot for X (LIFE_EPOCH; needed for backward)


k=1 — RMSNorm1 forward (composite)

Op: OP_RMSNORM_FWD(X → Xn1)

OPEN: yes (because it contains multiplications)

FSS/LIFT: yes if you implement rsqrt via LUT/FSS

Slots: Xn1 (LIFE_EPOCH)

Inside RMSNorm1 (deterministic sub-structure):

compute ms = mean(x^2)

elementwise square uses secure multiply → OPEN inside mul protocol

compute inv = rsqrt(ms + eps)

nonlinear: either polynomial or LUT; if LUT, you’ll do FSS+LIFT

compute Xn1 = x * inv * gamma

secure multiplies → OPEN

Recommended sub-stream tags inside this op (for OPENs):

OPEN for muls: stream tags "RMS1_MUL0", "RMS1_MUL1", … (deterministic indexing)

If LUT: FSS id(s) derived from (sgir_op_id32,k,call_idx)


k=2 — QKV projection GEMM

Op: OP_GEMM_SS(Xn1, Wqkv → Tqkv)

OPEN: YES (Beaver triple opens for E and F)

FSS/LIFT: no

SKS hook point: after this GEMM (optional)

Slots: Tqkv (LIFE_EPOCH)

Inside OP_GEMM_SS (what “OPEN” means concretely):
Standard Beaver-style:

consume triple (A0,B0,C0) matching the GEMM tile/shard

compute secret shares E = A - A0, F = B - B0

OPEN(E) and OPEN(F) (to all three parties)

compute C = C0 + E*B0 + A0*F + E*F (domain arithmetic)

So per GEMM instance:

OPEN count: 2 opens (E and F), each is one OPEN_ARITH.

Deterministic stream ids for these internal OPENs:
Derive two stream ids:

stream_id64 = H("...GEMM_E...") and H("...GEMM_F...") using sgir_op_id32 + fixed tags.
(You already have a stream_id scheme; just ensure tags are fixed.)


k=3 — SKS check for QKV GEMM (optional but recommended)

Op: OP_SKS_GEMM_CHECK(kind=GEMM, target=C=Tqkv, A=Xn1, B=Wqkv)

OPEN: YES (SKS opens z)

FSS/LIFT: no (SKS challenges are public; SKS compute uses matvec + secgemm n=1 which internally OPENs E/F too)

Slots: z_self[t] temp, plus u,v,w scratch (LIFE_STEP or LIFE_EPOCH depending on your scheduling)

Where SKS sits:

It is logically after k=2 completes for this microbatch.

It must emit the SKS challenge leaf (or your SKS binding) deterministically.

Then it runs SKS compute and does OPEN(z) with SKS stream_id.


k=4 — Bias add + reshape/split Q,K,V

Op: OP_BIAS_ADD(Tqkv → QKV) then OP_VIEW_SPLIT(QKV → Q,K,V)

OPEN: no

FSS/LIFT: no

Slots: Q,K,V (LIFE_EPOCH)


k=5 — Attention scores GEMM: S = Q · Kᵀ

Op: OP_GEMM_SS(Q, K^T → S)

OPEN: YES (2 opens inside GEMM)

FSS/LIFT: no

Slots: S (LIFE_EPOCH)


k=6 — Scale scores by public constant (1/sqrt(d))

Op: OP_MUL_PUBLIC(S → Sscaled)

OPEN: no (public scaling is local on shares)

FSS/LIFT: no

Slots: Sscaled (can be in-place)


k=7 — Softmax approximation (composite)

Op: OP_SOFTMAX_APPROX(Sscaled → P)

OPEN: usually YES (because normalization uses multiplications), but do not OPEN values

FSS/LIFT: YES (exp/reciprocal approximations)

Lift TLVs: YES (this op is the main consumer of your LIFT_BATCH TLV pipeline)

Slots: P (LIFE_EPOCH)

What softmax approx usually contains (in MPC-friendly form):
A practical secure softmax is built from:

range reduction / clipping (comparisons) → can be DCF-based

approximate exp (LUT/piecewise poly) → FSS+LIFT

sum across row (linear)

reciprocal of sum (LUT/piecewise poly) → FSS+LIFT

multiply exp * invsum (secure multiply) → OPEN inside multiply protocol

So this k=7 composite op typically produces many FSS outputs (vectors) that must be lifted to RSS using your batched lift.


k=8 — Attention weighted sum GEMM: O = P · V

Op: OP_GEMM_SS(P, V → O)

OPEN: YES

FSS/LIFT: no

Slots: O (LIFE_EPOCH)


k=9 — Output projection GEMM: Y = O · Wo

Op: OP_GEMM_SS(O, Wo → Y)

OPEN: YES

FSS/LIFT: no

SKS hook point: after this GEMM (optional)

Slots: Y (LIFE_EPOCH)


k=10 — SKS check for output projection (optional)

Op: OP_SKS_GEMM_CHECK(target=Y, A=O, B=Wo)

OPEN: YES (OPEN z)

FSS/LIFT: no


k=11 — Residual add: X_out1 = X_in + Y

Op: OP_RESIDUAL_ADD(X_in, Y → X1)

OPEN: no

FSS/LIFT: no

Slots: X1 (LIFE_EPOCH)


k=12 — RMSNorm2 forward (composite)

same structure as k=1

produces Xn2


k=13 — MLP GEMM1: H = Xn2 · W1

Op: OP_GEMM_SS(Xn2, W1 → H)

OPEN: YES

SKS hook point: optional


k=14 — SKS check for MLP1 (optional)

Op: OP_SKS_GEMM_CHECK(target=H, A=Xn2, B=W1)

OPEN: YES


k=15 — Bias add

Op: OP_BIAS_ADD(H → H1)

OPEN: no


k=16 — GELU approximation (composite)

Op: OP_GELU_APPROX(H1 → G)

OPEN: typically YES (multiplies), but no opens of secret values

FSS/LIFT: YES (LUT/piecewise)

Lift TLVs: YES

Slots: G (LIFE_EPOCH)


k=17 — MLP GEMM2: Z = G · W2

Op: OP_GEMM_SS(G, W2 → Z)

OPEN: YES

SKS hook point: optional


k=18 — SKS check for MLP2 (optional)

Op: OP_SKS_GEMM_CHECK(target=Z, A=G, B=W2)

OPEN: YES


k=19 — Bias add

Op: OP_BIAS_ADD(Z → Z1)

OPEN: no


k=20 — Residual add: X_out = X1 + Z1

Op: OP_RESIDUAL_ADD(X1, Z1 → X_out)

OPEN: no

Slots: X_out (LIFE_EPOCH; may be needed for backward if this stage owns next block too)


k=21 — PP send

Op: OP_SEND_PP(X_out)

Comm: intra-party only


1.5 Backward k‑map (realistic, but pinned enough to implement now)

Backward is bigger. You still want a deterministic k schedule, so here’s the practical split:

You do one backward pass per microbatch in reverse pipeline order.

For simplicity, I’m assuming you checkpoint and store activations needed for backward in LIFE_EPOCH.


Key rule

Every GEMM in backward is still OP_GEMM_SS, so it still triggers internal OPEN(E/F).
Every nonlinearity derivative is either:

polynomial (no FSS), or

LUT (FSS+LIFT) like forward.


Backward k table (high-level but implementable)

I’ll index backward k starting at 100 to keep forward/backward disjoint (you can also use separate program phases; this is just to make it unambiguous).


k=100 — PP recv grad

Op: OP_RECV_PP(dX_out) (gradient from next stage)


k=101 — Residual split for final add

local:

dX1 += dX_out

dZ1 += dX_out


k=102 — Bias grad for MLP2 bias

reduce sum over batch dimension (linear)


k=103 — GEMM backward for MLP2

Given Z1 = G·W2 + b2:

dW2 = Gᵀ · dZ1 → OP_GEMM_SS(G^T, dZ1 → dW2) (OPEN inside)

dG = dZ1 · W2ᵀ → OP_GEMM_SS(dZ1, W2^T → dG) (OPEN inside)

(Yes, these are 2 GEMMs; TP layout decides whether they’re row/col parallel within party.)


k=104 — GELU’ approx (composite)

Op: OP_GELU_DERIV_APPROX(H1 → gelu'(H1)) (FSS+LIFT)

then dH1 = dG ⊙ gelu'(H1) (secure multiply → OPEN inside mul)

So:

FSS+LIFT: YES

OPEN: YES (mul)


k=105 — Bias grad for MLP1 bias

linear


k=106 — GEMM backward for MLP1

Given H1 = Xn2·W1 + b1:

dW1 = Xn2ᵀ · dH1 → OP_GEMM_SS(Xn2^T, dH1 → dW1) (OPEN)

dXn2 = dH1 · W1ᵀ → OP_GEMM_SS(dH1, W1^T → dXn2) (OPEN)


k=107 — RMSNorm2 backward (composite)

involves:

reductions (linear)

multiplications (OPEN inside mul)

rsqrt derivative (likely LUT/FSS + LIFT)

So:

OPEN: YES

FSS+LIFT: YES

output: dX1 updated (since RMSNorm2 input was X1)


k=108 — Residual split for first add

Given X1 = X_in + Y:

local:

dX_in += dX1

dY += dX1


k=109 — GEMM backward for output projection

Given Y = O·Wo + bo:

dWo = Oᵀ · dY → GEMM (OPEN)

dO = dY · Woᵀ → GEMM (OPEN)


k=110 — GEMM backward for attention weighted sum

Given O = P·V:

dP = dO · Vᵀ → GEMM (OPEN)

dV = Pᵀ · dO → GEMM (OPEN)


k=111 — Softmax backward (composite)

This is heavy, but implementable as:

dSscaled = softmax_backward(P, dP) using secure arithmetic + approximations

It will include:

multiplications (OPEN inside mul)

possibly LUT/FSS (if your softmax forward used approximations that require re-eval)

So:

OPEN: YES

FSS+LIFT: often YES


k=112 — Scale backward

dS = dSscaled * (1/sqrt(d)) (public mul)


k=113 — GEMM backward for QKᵀ

Given S = Q·Kᵀ:

dQ = dS · K → GEMM (OPEN)

dK = dSᵀ · Q → GEMM (OPEN)


k=114 — Merge QKV grads and GEMM backward for QKV projection

Given Tqkv = Xn1·Wqkv + bqkv:

form dTqkv from dQ,dK,dV (views)

dWqkv = Xn1ᵀ · dTqkv → GEMM (OPEN)

dXn1 = dTqkv · Wqkvᵀ → GEMM (OPEN)


k=115 — RMSNorm1 backward (composite)

OPEN: YES

FSS+LIFT: YES (if rsqrt approx)

output: dX_in updated


k=116 — PP send grad

OP_SEND_PP(dX_in) to previous stage


1.6 Where FSS+LIFT happens (exactly) in this stage

In the k-map above, FSS+LIFT appears in:

k=1 RMSNorm1 (rsqrt)

k=7 softmax approx (exp, inv)

k=12 RMSNorm2 (rsqrt)

k=16 GELU approx

plus backward derivatives k=104,107,111,115


What “FSS+LIFT happens” means operationally

For each such composite op:

choose fss_id(s) for each FSS call deterministically

determine producer edge e from (sid,fss_id) (your edge selection rule)

run edge‑FSS eval on that edge to produce 2-party additive shares

run Lift v1 batched:

head→tail: TLV 0x01 (m vector)

tail→other: TLV 0x02 (component vector)

store into RSS slot components using your slot map v1

emit RSS commit/done leaves at finalize (your lift state machine v1)


Deterministic fss_id derivation (you need this now)

You want fss_id stable across parties and independent of batching decisions:

fss_id =
  LE64( SHA256(
    "UVCC_FSSID_V1" ||
    sid32 ||
    LE32(sgir_op_id32) ||
    LE16(fss_call_idx) ||
    LE16(tp_rank) ||
    LE16(pp_stage_id) ||
    LE16(reserved0=0)
  )[0..7] )
fss_call_idx is a small integer you assign inside the composite op in fixed order (exp0, exp1, inv0, …).

This makes edge selection and PRG components deterministic.


1.7 Where SKS hooks go (exactly) in this stage

You insert SKS after GEMMs you care about.

For the stage above, the usual “worth checking” set is:

QKV projection GEMM (k=2)

output projection GEMM (k=9)

MLP1 GEMM (k=13)

MLP2 GEMM (k=17)

So SKS ops sit at:

k=3, 10, 14, 18


Important: SKS is not “free”

SKS itself contains:

matvecs (secret×public)

one secret×secret n=1 multiply (your secgemm hook)

a final OPEN(z) on a scalar (or t scalars)

So it adds extra OPENs (internally) and one explicit OPEN(z) per checked GEMM.

If you want throughput, you typically:

sample checks (not every GEMM), or

run checks only on some stages / some steps, or

run checks only in forward.


2) Byte‑exact example: one LIFT_BATCH_SEND payload (BLV1 + TLV 0x01, q_count=4096)

You asked for:

BLV1 header

TLV 0x01 payload with q_count=4096

padding rules

exact msg_id32 derivation inputs (and a concrete test vector)


2.1 Example parameters (fixed test vector)

Use these exact values to test your codec + msg_id32 derivation:

sid32 = 32 bytes: 00 01 02 ... 1F
(i.e., sid32[i]=i)

sgir_op_id32 = 0x11223344

sender/receiver: src_party=1 (P1) → dst_party=2 (P2)

chunking: chunk_idx16=0, chunk_cnt16=1 (single chunk)

we send one TLV:

TLV type t=0x01 (LIFT_M_U64VEC_V1)

TLV flags f=0x00

fss_id = 0x1122334455667788

producer_edge_id = 1

q_count = 4096

m_u64[q] = your computed masked shares (not specified here; they’re runtime data)


2.2 msg_id32 derivation (exact preimage + computed result)

Per your rule:

msg_id32 = LE32( SHA256(
  "UVCC_MSGID_LIFTBATCH_V1" ||
  sid32 ||
  LE32(sgir_op_id32) ||
  U8(src_party) || U8(dst_party) ||
  LE16(chunk_idx16) || LE16(chunk_cnt16)
)[0..3] )

Concrete result for the test vector above

msg_id32 = 0xED23AD38
(because SHA256(preimage)[0..3] = 38 AD 23 ED)


The exact preimage bytes (hex, concatenation order)

ASCII "UVCC_MSGID_LIFTBATCH_V1"
then sid32 00..1f
then sgir_op_id32 LE = 44 33 22 11
then src=01, dst=02
then chunk_idx=0000, chunk_cnt=0001 LE16.

That’s the deterministic input you should unit-test.


2.3 BLV1 payload layout for TLV 0x01 with q_count=4096

Sizes (important)

Batch header size = 16 bytes

TLV header size = 4 bytes

TLV value length:

len = 16 + 8*q_count

with q_count=4096 → len = 16 + 32768 = 32784 = 0x8010

TLV padding:

pad = (4 - (len & 3)) & 3

here len mod 4 = 0 → pad = 0

Total TLV record bytes:

4 + 32784 + 0 = 32788 = 0x8014

tlv_bytes = 0x8014

Total payload_len = 16 + 0x8014 = 32804 bytes


First 48 bytes of the payload (hex dump)

This is byte‑exact for q_count=4096 and m[0]=0 (I’m showing the first element region; the rest is your 4096 u64s):

0000: 42 4c 56 31 01 00 00 00 01 00 00 00 14 80 00 00
0010: 01 00 10 80 88 77 66 55 44 33 22 11 00 10 00 00
0020: 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
Interpretation:

42 4c 56 31 = "BLV1" magic (0x31564C42 LE)

01 version

00 flags

0000 reserved0

01 00 00 00 tlv_count = 1

14 80 00 00 tlv_bytes = 0x8014

TLV hdr: 01 type, 00 flags, 10 80 len = 0x8010

Value:

fss_id = 0x1122334455667788 LE

q_count = 4096 LE32

producer_edge_id = 1

reserved1/reserved2 = 0

then the m_u64 array begins

No padding bytes follow this TLV because len is already 4-aligned.


2.4 Full byte‑exact payload hexdump for a small sanity test (q_count=4)

This is the best practical way to unit-test your encoder/decoder without printing 32KB in chat.

Use:

same header fields

q_count = 4

producer_edge_id = 1

m = [0, 1, 0xDEADBEEFDEADBEEF, 0x0123456789ABCDEF]

Then the payload bytes are exactly:

0000: 42 4c 56 31 01 00 00 00 01 00 00 00 34 00 00 00
0010: 01 00 30 00 88 77 66 55 44 33 22 11 04 00 00 00
0020: 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
0030: 01 00 00 00 ef be ad de ef be ad de ef cd ab 89
0040: 67 45 23 01
Where:

tlv_bytes = 0x0034 (52)

len = 0x0030 (48)

This is a canonical “golden vector” for your BLV1 codec.


2.5 Payload hash + batch SEND leaf test vector (optional but strongly recommended)

Your batch transcript leaves v1 are:

0x77 = LEAF_LIFT_BATCH_SEND_V1

0x78 = LEAF_LIFT_BATCH_RECV_V1

Leaf bytes:

leaf =
  LE8(leaf_type) ||
  sid32 ||
  LE32(sgir_op_id32) ||
  U8(src_party) || U8(dst_party) ||
  LE32(msg_id32) ||
  SHA256(payload_bytes)[32]
leaf_hash = SHA256(leaf)
For the q_count=4 payload above (and the msg_id32 test vector from §2.2), the computed values are:

msg_id32 = 0xED23AD38

payload_hash = e8bb6a7a93e19c32ebeea7e7ccdaac4478a2168211d2cbb559d74c2492a06949

leaf_hash(0x77||...) = 57fe14cc22d1b8e0bc27b2a8c0505e374a3b99f81d27a85e0e1437ba54a24374

Use this to validate your hashing and leaf serialization exactly.


3) The “on top of this codebase” DP + PP + TP/MP integration you still need

You asked earlier: how DP + PP + TP/MP actually works on top of what you built. Here is the implementation-grade version.


3.1 DP: “secure replicas” = multiple independent 3PC triangles

What DP means here

You run R independent 3PC sessions in parallel:

Replica r has parties (P0_r, P1_r, P2_r)

Each replica processes different microbatches (data-parallel)

Each replica holds shares of the same model weights (same shape, different random shares)


Critical rule: never reuse session keys across replicas

Derive a replica sid:

sid_r = SHA256("UVCC_SID_REPLICA_V1" || sid_root || LE32(r))[0..31]
All:

pairwise PRG keys K_ab,

FSS edge selection,

LIFT msg ids,

SKS challenge seeds (if bound to sid),
must be keyed off sid_r.


Gradient aggregation across replicas (cheap + local)

After backward, each replica has gradient shares dW_r in RSS form.

Within a party (say cloud A / P0), you just sum your local components across replicas:

P0 holds component0 and component1 for each replica

It computes:

dW_sum_comp0 = Σ_r dW_r_comp0

dW_sum_comp1 = Σ_r dW_r_comp1

No cross-party comm for DP reduction is needed, because you are only adding shares locally.

Then every replica’s weights don’t need separate updates; instead:

update a single party-local weight shard using the summed gradient shares.

That’s how you get ~linear throughput scaling with DP replicas.


3.2 PP: pipeline parallelism hides MPC latency (OPEN + lift messages)

What PP changes

PP changes when you execute k=0..k=21 for each microbatch, and where activations live.

It does not change:

the cryptographic protocols,

the bytes on the wire between parties.


What you must implement

A standard pipeline schedule, typically:

1F1B schedule after warmup:

interleave forward microbatches and backward microbatches


Where MPC latency hides

During:

GEMM internal OPEN(E/F)

lift TLV sends (head→tail, tail→other)

SKS OPEN(z) (if enabled)

While one microbatch is awaiting OPEN completion, you keep GPUs busy by executing other k’s for other microbatches (subject to dependency constraints).

This requires:

per-microbatch dependency tracking

at least 2 CUDA streams:

compute stream

comm/pack stream

plus your TX cache rule (already in your lifecycle spec)


3.3 TP/MP: keep per-GPU math dense inside each party (no new crypto)

TP/MP is purely intra-party GPU distribution on shares.


Column-parallel and row-parallel linear layers

Exactly like Megatron-style TP, but every tensor is “RSS components on device”.

Column-parallel GEMM:

split output features across TP ranks

input activations are replicated (allgather within party)

output stays sharded (no reduce)

Row-parallel GEMM:

split input features across TP ranks

each rank produces partial output

outputs are summed via intra-party allreduce (on shares)


Important: cross-party OPEN doesn’t care about TP

OPEN is done on whatever shard you hold.
The only thing you must ensure is that:

each party opens the correct shard bytes in the correct order,

stream_id derivations include tp_rank so transcript binding is unambiguous.

That’s why I included tp_rank in sgir_op_id32 derivation in §1.2.


3.4 How to assemble transcripts across DP replicas + PP stages + TP ranks

If you don’t do this, auditing becomes impossible at scale.


Recommended structure: “root of roots”

For a given training step (your execution epoch):

Each (dp_replica, party) produces its own transcript root:

stable leaf ordering by (mb_id, k, …)

Each party builds:

party_step_root = MerkleRoot( roots_for_all_replicas_sorted_by_replica_id )

Parties sign party_step_root.

For an audit epoch of N steps:

audit_epoch_root = MerkleRoot( step_roots_sorted_by_step_id )

This is the scalable way to keep proofs small and deterministic.


4) What you should implement immediately (concrete checklist)

If you want to start coding today, do this in order:

Encode the forward k-map exactly like §1.4 in your SGIR interpreter:

implement k indexing

implement sgir_op_id32 derivation

Make OP_GEMM_SS internally emit two OPENs (E/F) with deterministic stream tags.

Make one nonlinearity composite op (start with GELU_APPROX) that:

produces FSS point/vec results on one edge

lifts via BLV1 batch TLVs (0x01,0x02)

stores into RSS slots

Implement BLV1 encoder/decoder and validate using the q_count=4 golden dump (§2.4) and msg_id32 test vector (§2.2).

Implement lift finalize leaves (0x7B/0x7C) ordering:

sort fss_id ascending

Only after that:

add PP scheduling (1F1B)

add DP replicas

add TP within party

add SKS checks

You’re right: the plan you pasted already contains ~90–95% of the “end-to-end” design (DP/PP/TP layering, collision-free IDs, deterministic scheduling, communicator groups, a realistic transformer k‑map, and byte-exact BLV1/TLV examples + hashes).

What’s still missing (i.e., the parts you need to actually start implementing without “mystery gaps”) are the runtime plumbing details that sit between Parts 1–8 and the DP/PP/TP plan:

Boot + session bring-up (how every worker learns its (p,r,s,t) coordinate, derives sids/keys, and establishes cross-party channels)

Concrete object model (exact modules/classes and who owns what: transport, tx-cache, slot map, open-state, lift-state, transcript)

OPEN orchestration details (how you schedule/identify/commit internal OPENs inside GEMM + how you batch and finalize them deterministically)

Triple provisioning (how GEMM gets triples at scale without reuse, how to name triple-ids deterministically, and how to stream/cache them)

GPU execution model (streams/events + async scheduler rules that guarantee determinism)

Memory/lifetime mapping (what must be LIFE_STEP vs LIFE_EPOCH vs LIFE_PERSIST under PP)

Failure/retry semantics end-to-end (exactly-once, chunking/reassembly, when tx-cache can be freed)

Test harness (golden vectors, replay determinism tests, fault injection tests)

Below is the complete remaining dump for those missing parts, written so it fits on top of your Parts 1–8 exactly as-is(no format changes required).


0) One “mental model” that makes implementation straightforward

Everything runs inside a Subsession keyed by sid_sub[r,s,t].
Inside each subsession, you already have (Parts 7–8): FRAME reliability + msg_id32, lift TLVs + lift state machine, OPEN framing, transcript leaves, slot map, and arenas.

DP/PP/TP just means you run many subsessions concurrently, and you add intra-party NCCL collectives for TP/PP/DP.

So: implement subsession correctness first, then replicate.


1) Boot and bring-up (what happens before training starts)

You need a deterministic bring-up so that all parties agree on:

who is P0/P1/P2 for this run

how many replicas/stages/tp ranks exist

which worker (r,s,t) connects to which peer (r,s,t) on other parties

which sids and keys are used


1.1 Worker coordinate assignment

Each GPU process must know:

coord = (party_id p, replica r, stage s, tp_rank t)

Do not infer this from network timing. Pass it explicitly via launch args / env vars:

UVCC_PARTY_ID=0|1|2

UVCC_R=... UVCC_S=... UVCC_T=...

UVCC_REPLICA_ID, UVCC_STAGE_ID, UVCC_TP_RANK

UVCC_GPU_ID or infer from local rank


1.2 Derive sids (no collisions, no spec changes)

Use exactly the scheme already in your plan:

sid_job (32B) is the top-level run id

sid_rep[r] = H("UVCC_SID_REPLICA_V1"||sid_job||LE32(r))

sid_sub[r,s,t] = H("UVCC_SID_SUB_V1"||sid_rep[r]||U8(s)||LE16(t))

Implementation detail: store these as opaque bytes and never stringify them (avoid endianness/format bugs).


1.3 Key derivation / key registry

From Parts 1–8 you already have pairwise keys for:

FSS keys

RSS lift PRG keys (K_01, K_12, K_20 or equivalent)

any transport integrity keys (if you have them)

The missing implementation step is a Key Registry per subsession:

KeyRegistry {
  sid_sub: [32]byte
  party_id: u8

  // pairwise (only if this party is on that edge)
  k_to_p0: KeyMaterial
  k_to_p1: KeyMaterial
  k_to_p2: KeyMaterial

  // derived fixed per-sid PRG seeds used by lift PRG
  lift_prg_seed_01, lift_prg_seed_12, lift_prg_seed_20
}
If your Parts 1–8 already define pairwise PRK → subkeys, you just plug sid_sub into that KDF.


1.4 Cross-party channel wiring

For each worker W[p,r,s,t], open exactly two channels:

to W[p_next,r,s,t]

to W[p_prev,r,s,t]

Where p_next=(p+1)%3, p_prev=(p+2)%3.

Important: channel identity must include (r,s,t) deterministically. Two clean options:

Option A (simplest): separate TCP/QUIC connections per (r,s,t)

Option B (fewer sockets): a single connection per party pair, with (sid_sub) included in each FRAME header as a demux key

Your plan already says “avoid spec changes”: so if your FRAME header doesn’t include sid_sub, choose Option A first.


2) The concrete object model (what you actually implement in code)

You need these runtime objects per subsession:


2.1 Subsession (the “triangle worker” instance)

Subsession {
  sid_sub[32]
  party_id p

  transport: Transport          // your FRAME reliability + tx-cache
  transcript: TranscriptWriter  // leaves + merkle
  slots: SlotMap                // RSS slots
  arenas: {step, epoch, persist}

  lift: LiftEngine              // state machine keyed by fss_id
  open: OpenEngine              // state machine keyed by stream_id64

  // batching helpers
  lift_batch_out[peer=2]: LiftBatchBuilder
  open_batch_out[peer=2]: (optional) OpenBatchBuilder

  // async scheduler
  scheduler: StageScheduler
}

2.2 Transport responsibilities (exactly-once)

Transport must provide:

send_frame(kind, dst_party, msg_id32, payload_bytes)

recv_frame() delivering (src_party, kind, msg_id32, payload_bytes)

dedup: if (src,msg_id32) already accepted → drop (or re-ACK)

tx-cache: store bytes until ACKed (or until your protocol says safe)

Crucial missing rule: when can you free tx-cache?

For exactly-once delivery: free after you receive ACK for that msg_id32.

For audit correctness: you also need to ensure transcript leaf has been written before you acknowledge acceptance (or at least before you mark msg accepted).
The cleanest rule:

On receiving a frame: verify CRC → write “RECV leaf” → mark accepted → then ACK.

That ensures “accepted” implies “audited”.


3) OPEN engine details (the missing “how”)

Your plan mentions OPEN, but the key missing part is how to orchestrate OPENs deterministically when they occur inside other ops (like GEMM).


3.1 A practical OPEN API

You want one call site pattern everywhere:

open_id = OpenID(stream_id64, round_idx16)

open.send_share(open_id, payload)   // emits frame(s)
await open.wait_result(open_id)     // yields until reconstructed
Where:

stream_id64 is deterministically derived from (sid_sub, sgir_op_id32, "tag", shard_idx)

round_idx16 is 0,1,2… inside that stream


3.2 Internal OPENs inside GEMM must be named

Inside OP_GEMM_SS, you will generate multiple opens depending on tiling/shards.
Even if conceptually there are only “OPEN(E)” and “OPEN(F)”, the implementation will often do multiple tiles.

So you must define:

gemm_tile_idx (u16 or u32), deterministic from the GEMM tiling plan

stream tags:

"GEMM_E" and "GEMM_F"

stream_id64 = H("OPEN_V1"||sid_sub||sgir_op_id32||tag||LE16(tile_idx))

Then each tile does:

open(E_tile)

open(F_tile)

and commits leaves in that exact order.


3.3 OPEN batching (optional but recommended)

If Parts 1–8 don’t batch OPEN frames, you can still implement them unbatched first.

If you do batch:

batching scope should be (sid_sub, sgir_op_id32, dst_party, stream_tag)

flush point should be: end of k (or end of tile group), deterministic

Do not batch across different sgir_op_id32 if your transcripts assume “step-scoped” grouping.


4) Triple provisioning (this is the biggest practical missing piece)

Your plan assumes Beaver triples exist. To implement, you need a triple store that:

never reuses a triple

can serve triples at GPU speed

is deterministic/auditable in indexing


4.1 Define a deterministic triple id namespace

For each OP_GEMM_SS call you need many triples (tiles). Define:

triple_id64 = Trunc64( H(
  "UVCC_TRIPLE_V1" ||
  sid_sub ||
  LE32(step_id) ||
  LE32(sgir_op_id32) ||
  LE16(tile_idx) ||
  LE16(reserved)
))
Then you consume triples in increasing tile_idx.


4.2 Triple store API

TripleStore {
  // returns device pointers for A0,B0,C0 for one tile
  get_gemm_triple(triple_id64, shape, dtype) -> TripleTile
}

4.3 Where triples come from (choose one v1 path)

Because your spec stack is already huge, pick the simplest v1:

Offline preprocessing generates triples and loads them from disk/object store

Each party stores only its share of the triple (RSS style)

Triples are indexed by triple_id64

Later you can replace with an online triple generator; but v1 should be “load and go”.


4.4 Triple lifetime + caching

Triples are read-only inputs. Put them in:

host pinned memory cache

staged to GPU just-in-time per tile group

keep an LRU per subsession if memory is tight

Determinism rule: caching must not change the order of triple consumption.
(So: cache lookup is fine; speculative prefetch must not skip ahead.)


5) GPU execution model (streams/events that don’t break determinism)

You need PP to hide latency, but you must not let GPU scheduling reorder transcript events.


5.1 Use three CUDA streams per worker

S_compute: GEMMs and elementwise ops

S_pack: packing/unpacking for OPEN/LIFT payloads

S_copy: intra-party PP sends/recvs + TP collectives (NCCL)


5.2 Deterministic “commit points”

Write transcript leaves at commit points, not “whenever the GPU finishes”.

Example:

For OPEN: commit leaf when the OPEN result is logically available (after reconstruct, before releasing dependent ops)

For LIFT: commit done leaf when all required components for that fss_id are stored in slot map

This way the transcript order follows logical program order, not GPU completion timing.


5.3 Scheduler rule that guarantees determinism

Your plan already suggests “pick earliest runnable microbatch”.

Make it strict:

At each scheduling step, pick runnable task with minimum:

(phase_order, mb_id, k)

with phase_order: FWD=0, BWD=1, UPD=2

A task is runnable only if all of its dependencies are satisfied and all required OPEN/LIFT futures are completed.

This gives you overlap without nondeterministic ordering.


6) Memory/lifetime mapping under PP (what lives in STEP vs EPOCH vs PERSIST)

This is the other “implementation gap” that causes most PP projects to fail.


6.1 The rule of thumb

PERSIST: model weights, optimizer state

EPOCH: anything needed across forward→backward of a microbatch (activations/checkpoints)

STEP: ephemeral scratch, pack buffers, temporary opens, partial reductions


6.2 For the transformer block k‑map you posted

Store these in EPOCH (because backward needs them):

inputs to RMSNorms (or store checkpoints)

Q,K,V and/or enough to recompute them

softmax intermediates if your backward needs them

MLP pre-activation (H1) for GELU’ if required

Everything else (pack buffers, lifted temporary vectors that immediately feed a multiply) can be STEP if you recompute as needed.


6.3 Checkpointing vs storing activations

To reduce EPOCH memory:

checkpoint at block boundaries

recompute forward activations during backward (costly but common)

This does not change MPC correctness; only compute.


7) LIFT batching end-to-end: builders, fragmentation, and when to flush

Your plan defines BLV1+TLVs and msg_id32. Missing is the “plumbing”.


7.1 LiftBatchBuilder interface

Maintain one builder per (dst_party) and per (sgir_op_id32):

builder.begin(op_id, dst)
builder.append_tlv(type, flags, value_bytes)
builder.finish() -> list<FramePayloadChunk>
Where:

TLVs must be appended in canonical order (e.g., by fss_id ascending)

finish() performs fragmentation into chunks if needed


7.2 Flush rule

Flush exactly at end of the k that generated the lifts:

if k=7 (softmax) produced TLVs → flush after k=7 TLVs are fully appended

do not keep TLVs “open” across k boundaries

This keeps transcripts and msg_id derivations clean.


7.3 Reassembly

On receive:

buffer chunks by (src,msg_id32) until all chunks arrived

then parse BLV1 once and apply TLVs in order

Exactly-once rule: accept only once per msg_id32; duplicates are ignored after ACK.


8) Failure/retry semantics (complete and implementable)

This is the “end-to-end reliability” missing detail.


8.1 Sender (tx-cache)

For each sent msg_id32:

store payload bytes in tx-cache

start retransmit timer (RTO)

on ACK: mark done + free tx-cache entry


8.2 Receiver (dedup + transcript order)

On receiving a frame:

verify CRC (or integrity)

check dedup table: if already accepted → re-ACK and return

write transcript leaf (RECV) before marking accepted

mark accepted

deliver payload to the protocol engine (open/lift)

ACK

That ensures: “accepted” implies “audited”.


8.3 Restart/resume (v1 policy)

If you don’t yet support resume:

define policy: “crash aborts step; restart from last checkpoint step”

transcripts cover completed steps only

If you do support resume:

persist dedup tables and transcript state per step (nontrivial)

most teams postpone this until correctness is proven


9) End-to-end “one training step” walkthrough (now fully specified)

This is the exact sequence across layers:


9.1 For each subsession (r,s,t) inside each party

For microbatches mb=0..M-1:


Forward

k=0 recv activation (PP intra-party)

run k=1..k=21 as per your k-map

GEMM ops call OpenEngine internally

nonlinear ops call FSS eval then LiftEngine via batch TLVs


Backward

k=100 recv grad (PP intra-party)

run k=101..k=116

produce gradient slots for local param shards

At the end:

grads_ready[r,s,t]=true


9.2 DP reduce (intra-party only)

For each party p, for each (s,t):

for each param shard gradient slot:

allreduce SUM across r=0..R-1 for both stored RSS components

Now all replicas have identical gradient shares (or you keep only one replica’s updated weights; your choice).


9.3 Optimizer update (local on shares)

Update PERSIST weight slots:

read weight shares

apply update using reduced gradient shares

write back weight shares (still RSS)

No MPC comm needed for the update itself (it’s local arithmetic on shares).


9.4 Transcript finalization

Per subsession:

finalize local transcript shard root

Per replica:

replica_root[r] = H(concat roots[r,s,t])

Global:

global_root = H(concat replica_root[r])

Each party signs/commits global_root per your Part 8 policy.


10) Test plan (the missing “how you know it works”)

You already have golden vectors for BLV1. Add these tests:


10.1 Codec tests

BLV1 encode/decode round-trip

q_count=4 golden hexdump test (exact bytes)

q_count=4096 size/padding test (lengths exactly as expected)


10.2 Transport tests

drop 10% packets → retransmit works

duplicates → exactly-once accepts once, acks duplicates

reorder chunks → reassembly still works

corrupt CRC → reject + no transcript leaf + no accept


10.3 Lift engine tests

single fss_id with each producer edge (01/12/20)

multiple fss_id batched in one frame

interleaved fss_id across microbatches (must not collide due to sid_sub and op_id)


10.4 Determinism tests

run twice with same inputs/seeds

confirm:

same msg_id32 sequence

same transcript leaf hashes

same global_root


10.5 End-to-end micro-model test

1 layer MLP, tiny shapes

run 1 step, compare:

reconstructed plaintext (by trusted harness) matches expected arithmetic

gradients match reference within ring arithmetic rules


11) Final answer to your “is anything missing?” question

What you already have in the pasted plan

DP/PP/TP topology and correctness math

collision-free sid strategy

comm planes separation (MPC vs NCCL)

PP scheduling principle (1F1B + async yield)

deterministic sgir_op_id32 and fss_id rules

realistic transformer block k-map

byte-exact BLV1/TLV payloads + golden vectors + hashes

transcript-of-transcripts approach

implement-next checklist


The remaining pieces I just provided (the “last mile”)

exact bring-up wiring and key registry

concrete runtime object model (who owns transport/open/lift/transcript/slots)

OPEN naming and internal GEMM OPEN determinism

triple provisioning and triple-id rules

GPU streams + commit points (so overlap doesn’t break transcript determinism)

lifetime mapping (STEP/EPOCH/PERSIST) under PP

exactly-once + retry + tx-cache free rules

a full end-to-end step walkthrough across all layers

a complete test plan to validate correctness + determinism

That’s the full “remaining dump” needed to start implementing.

Here’s exactly what I’m going to give you (and then I’ll give all of it):

Phase 0 → Phase 6 implementation runbook (7 phases) in the only order that won’t dead-end.

For each phase: a list of modules you must create.

For each module:

Responsibilities + invariants (“must always be true”)

Module-by-module TODO checklist

Function signatures (typed, consistent, ready to turn into headers)

A suggested repo/file layout so you can start coding immediately.

A definition of all core types you’ll use across modules (sid, msg_id32, fss_id, stream_id, etc.).

A final “Definition of Done” checklist + tests you must pass before moving to the next phase.

Everything below assumes you are building on top of your Parts 1–8 exactly:

RSS slot map and lifetimes (STEP/EPOCH/PERSIST)

LIFT batching BLV1 + TLVs (0x01, 0x02) in FRAME kind 0x66

OPEN framing and transcript leaves

msg_id32 derivation exactly per your Part 8 formula (I will only wrap it as a function; no spec changes)

exactly-once acceptance semantics + tx-cache

transcript leaves and Merkle roots

SKS hooks (optional)


Core types (used everywhere)

Use these in a shared header/module so the whole codebase stays consistent.

// uvcc/types.h (pseudocode C++-style)

using u8  = uint8_t;
using u16 = uint16_t;
using u32 = uint32_t;
using u64 = uint64_t;

template<size_t N>
using BytesN = std::array<u8, N>;

using Sid32      = BytesN<32>;   // sid_job, sid_rep, sid_sub
using Hash32     = BytesN<32>;
using MsgId32    = u32;
using SgirOpId32 = u32;
using FssId64    = u64;
using StreamId64 = u64;

struct SpanU8 {
  u8*  p;
  size_t n;
};

struct SpanConstU8 {
  const u8* p;
  size_t n;
};

enum class PartyId : u8 { P0=0, P1=1, P2=2 };

inline PartyId next_party(PartyId p) { return PartyId((u8(p)+1)%3); }
inline PartyId prev_party(PartyId p) { return PartyId((u8(p)+2)%3); }

// Process coordinate
struct Coord {
  PartyId party;   // 0,1,2
  u16 replica;     // r
  u16 stage;       // s
  u16 tp_rank;     // t
};

// Common status/error
enum class Err {
  OK=0,
  Invalid,
  Io,
  Timeout,
  Decode,
  Crypto,
  OutOfMemory,
  ProtocolViolation,
  Internal,
};

struct Status {
  Err code;
  const char* msg;
  static Status ok() { return {Err::OK, ""}; }
  bool ok() const { return code==Err::OK; }
};

template<typename T>
struct Result {
  Status st;
  T val;
};

Phase 0 — Foundations: deterministic IDs, byte coding, hashing, and scaffolding

Phase 0 modules

uvcc/bytes — endian + varint helpers + safe append/read

uvcc/hash — SHA256 wrapper (and HMAC/KDF wrapper if your Parts 1–8 uses it)

uvcc/ids — sid derivation, msg_id32 derivation, stream_id64 derivation, fss_id derivation

uvcc/log — structured logs + tracing ids (sid/op/msg)

uvcc/testkit — golden vector runner + deterministic replay hooks


0.1 uvcc/bytes — TODO + signatures

Responsibilities

Correct LE/BE encode/decode, no UB, no alignment assumptions

Safe “Writer/Reader” with bounds checking

Used by FRAME, BLV1, TLV, transcript leaf encoding

TODO

 Implement ByteWriter (append primitive types LE)

 Implement ByteReader (read primitive types LE)

 Implement align_up(n, 4) helpers

 Add unit tests for every primitive (u16/u32/u64)

Signatures

struct ByteWriter {
  std::vector<u8> buf;

  void put_u8(u8 x);
  void put_u16_le(u16 x);
  void put_u32_le(u32 x);
  void put_u64_le(u64 x);
  void put_bytes(SpanConstU8 s);

  // pad with zeros to alignment (e.g., 4)
  void pad_to(size_t alignment);
};

struct ByteReader {
  SpanConstU8 s;
  size_t off;

  Result<u8>  get_u8();
  Result<u16> get_u16_le();
  Result<u32> get_u32_le();
  Result<u64> get_u64_le();
  Result<SpanConstU8> get_bytes(size_t n);

  Result<void> skip(size_t n);
  size_t remaining() const;
};

size_t align_up(size_t n, size_t a);

0.2 uvcc/hash — TODO + signatures

Responsibilities

Provide SHA256

Provide domain-separated hash wrappers (even if underlying SHA is same)

(If your parts specify) HKDF / HMAC

TODO

 sha256(bytes)->Hash32

 sha256_concat(list)->Hash32 convenience

 Unit tests vs known SHA256 vectors

Signatures

Hash32 sha256(SpanConstU8 data);

Hash32 sha256_concat(std::initializer_list<SpanConstU8> parts);

// Optional if your key schedule uses HKDF/HMAC:
Hash32 hmac_sha256(SpanConstU8 key, SpanConstU8 data);
Hash32 hkdf_extract(SpanConstU8 salt, SpanConstU8 ikm);
Hash32 hkdf_expand(SpanConstU8 prk, SpanConstU8 info, size_t out_len_32max);

0.3 uvcc/ids — TODO + signatures

Responsibilities

No collisions across DP/PP/TP

EXACTLY follow Part 8 formulas internally (wrap them; don’t change)

TODO

 Implement derive_sid_rep, derive_sid_sub

 Implement derive_sgir_op_id32 (your chosen canonical formula)

 Implement derive_msg_id32 using Part 8 preimage rule

 Implement derive_fss_id64

 Implement derive_open_stream_id64

 Unit tests: “same inputs -> same outputs” (golden vectors)

Signatures

Sid32 derive_sid_rep(const Sid32& sid_job, u32 replica);

Sid32 derive_sid_sub(const Sid32& sid_rep, u16 stage, u16 tp_rank);

// Your deterministic op id scheme (you already proposed variants; pick one and freeze it):
SgirOpId32 derive_sgir_op_id32(
  const Sid32& sid_sub,
  u32 step_id,
  u16 microbatch,
  u8  phase,     // 0=FWD,1=BWD,2=UPD
  u16 k          // op index
);

// MUST match Part 8 exactly:
MsgId32 derive_msg_id32(
  const Sid32& sid_sub,
  SgirOpId32 sgir_op_id32,
  PartyId src,
  PartyId dst,
  u16 chunk_idx,
  u16 chunk_cnt
);

// fss_id for each FSS call inside a composite op:
FssId64 derive_fss_id64(
  const Sid32& sid_sub,
  SgirOpId32 sgir_op_id32,
  u16 fss_call_idx
);

// OPEN stream id for internal opens (E/F tiles etc.)
StreamId64 derive_open_stream_id64(
  const Sid32& sid_sub,
  SgirOpId32 sgir_op_id32,
  SpanConstU8 tag,  // e.g. "GEMM_E", "GEMM_F"
  u32 shard_idx     // tile idx / tensor shard
);

0.4 uvcc/testkit — TODO + signatures

Responsibilities

Run golden vectors for BLV1, msg_id32, leaf hashes

Determinism tests (same run twice -> same transcript roots)

TODO

 Implement GoldenVector runner that loads JSON / binary test files

 Add tests for derive_msg_id32 and BLV1 sample payloads

 Add “fault injection knobs” (drop/dup/reorder frames)

Signatures

struct FaultConfig {
  double drop_prob;
  double dup_prob;
  double reorder_prob;
};

void set_fault_config(const FaultConfig& cfg);

Phase 1 — Bring-up: config, topology, subsession construction

Phase 1 modules

uvcc/config — parse env/CLI -> config object

uvcc/topology — map local ranks -> (r,s,t); build NCCL group membership lists

uvcc/keys — KeyRegistry per subsession

uvcc/net — connection manager (per peer, per subsession OR shared + demux)

uvcc/subsession — the container object that wires everything together


1.1 uvcc/config

TODO

 Parse party_id, R,S,T,M, endpoints, ports

 Parse GPU local rank / device id mapping

 Validate divisibility constraints (batch splits etc.)

Signatures

struct UVCCConfig {
  PartyId party;
  u16 R, S, T;
  u16 M;              // microbatches
  Sid32 sid_job;

  // network addressing
  std::string addr_p0;
  std::string addr_p1;
  std::string addr_p2;
  u16 base_port;

  // local GPU/process
  u16 local_rank;
  u16 gpu_id;

  // feature flags
  bool enable_sks;
  bool enable_open_batching;
  bool enable_fault_injection;

  FaultConfig fault_cfg;
};

Result<UVCCConfig> load_config(int argc, char** argv);

1.2 uvcc/topology

Responsibilities

Deterministic mapping between local_rank and (r,s,t)

Generate communicator membership lists for TP/PP/DP groups

TODO

 Implement coord_from_local_rank

 Implement group membership builders

Signatures

Coord coord_from_local_rank(u16 local_rank, u16 R, u16 S, u16 T);

// Intra-party rank id for NCCL
u16 local_rank_from_coord(const Coord& c, u16 R, u16 S, u16 T);

struct NcclGroup {
  std::vector<u16> ranks; // local ranks inside this party
};

NcclGroup make_tp_group(u16 R, u16 S, u16 T, u16 replica, u16 stage);
NcclGroup make_pp_group(u16 R, u16 S, u16 T, u16 replica, u16 tp_rank);
NcclGroup make_dp_group(u16 R, u16 S, u16 T, u16 stage, u16 tp_rank);

1.3 uvcc/keys — KeyRegistry

Responsibilities

Provide all per-subsesson keys needed by:

lift PRG

FSS edge eval keys

transport auth (if any)

No accidental reuse across subsessions

TODO

 Implement KeyRegistry::from_master(...)

 Implement get_pairwise(peer) etc.

Signatures

struct PairwiseKeys {
  Hash32 k_send; // direction-specific if needed
  Hash32 k_recv;
};

struct KeyRegistry {
  Sid32 sid_sub;
  PartyId party;

  PairwiseKeys with_p0;
  PairwiseKeys with_p1;
  PairwiseKeys with_p2;

  // Seeds used by lift PRG for components
  Hash32 lift_seed_01;
  Hash32 lift_seed_12;
  Hash32 lift_seed_20;
};

Result<KeyRegistry> derive_keys_for_subsession(
  const Sid32& sid_sub,
  PartyId party,
  SpanConstU8 master_secret   // however Parts 1–8 define your key bootstrap
);

1.4 uvcc/net — ConnectionManager

Responsibilities

Provide send/recv byte pipes to peers

Provide either:

per-(r,s,t) connection, or

shared connection + demux by sid_sub (if your FRAME supports it)

TODO

 Implement connect/listen for P0<->P1, P1<->P2, P2<->P0

 Provide non-blocking receive loop

Signatures

struct RawConn {
  Status send(SpanConstU8 bytes);
  Result<std::vector<u8>> recv(); // returns one raw packet/chunk
};

struct ConnectionManager {
  Result<RawConn> get_conn(PartyId peer, const Sid32& sid_sub);
};

1.5 uvcc/subsession — wiring

TODO

 Build sid_sub, keys, connections

 Construct Transport, Transcript, SlotMap, OpenEngine, LiftEngine

 Expose tick() to pump network + scheduled tasks

Signatures

struct Subsession;

Result<Subsession*> create_subsession(
  const UVCCConfig& cfg,
  const Coord& coord,
  const Sid32& sid_sub,
  const KeyRegistry& keys,
  ConnectionManager* conns
);

Phase 2 — Transport: FRAME reliability, chunking, tx-cache, dedup, ACKs

Phase 2 modules

uvcc/frame — FRAME header encode/decode (your Part 8 format)

uvcc/transport — exactly-once, tx-cache, retransmit, ACK

uvcc/reassembly — chunk buffering and reassembly for large payloads

uvcc/clock — timers for retransmit (testable)


2.1 uvcc/frame

Responsibilities

Encode/decode your Part 8 FRAME: kind, msg_id32, chunk idx/cnt, payload, CRC32, etc.

TODO

 Implement encode_frame(...)

 Implement decode_frame(...) with CRC verification

 Unit tests: corrupted CRC fails

Signatures

enum class FrameKind : u8 {
  // Use exact constants from Parts 7–8
  LIFT_BATCH = 0x66,
  OPEN       = 0x??, // your spec
  ACK        = 0x??,
  // ...
};

struct Frame {
  FrameKind kind;
  PartyId src;
  PartyId dst;
  MsgId32 msg_id;
  u16 chunk_idx;
  u16 chunk_cnt;
  std::vector<u8> payload;
};

Result<std::vector<u8>> encode_frame(const Frame& f);
Result<Frame> decode_frame(SpanConstU8 bytes);

2.2 uvcc/transport

Responsibilities

Exactly-once accept table (dedup)

Tx-cache by msg_id32 until ACK

Retransmit timers

Delivery of fully reassembled payloads to protocol engines

TODO

 Implement send_reliable(frame)

 Implement receive loop:

decode

dedup

reassembly

deliver

ack

 Provide hook: “write transcript leaf before marking accepted”

Signatures

struct TransportCallbacks {
  // Called after a full payload is reassembled AND accepted exactly-once
  Status (*on_deliver)(const Frame& f, SpanConstU8 full_payload);

  // Called before marking accepted (write transcript leaf here)
  Status (*on_before_accept)(const Frame& f, SpanConstU8 full_payload);
};

struct Transport {
  Sid32 sid_sub;
  PartyId self;

  Status send_frame_reliable(const Frame& f);
  Status poll(); // pump timers + recv queue + retransmit

  // stats
  u64 bytes_sent() const;
  u64 bytes_recv() const;
};

Result<Transport*> create_transport(
  const Sid32& sid_sub,
  PartyId self,
  RawConn* conn_to_prev,
  RawConn* conn_to_next,
  const TransportCallbacks& cbs
);

Phase 3 — Protocol engines: BLV1/TLV, LiftEngine, OpenEngine, FSS wrapper, SKS scaffolding

Phase 3 modules

uvcc/blv1 — BLV1 batch codec + TLV parser

uvcc/lift — LiftEngine state machine keyed by fss_id

uvcc/open — OpenEngine keyed by stream_id64

uvcc/fss — wrapper around your FSS implementation (edge eval)

uvcc/sks — SKS check orchestration (optional but structured now)


3.1 uvcc/blv1

TODO

 Implement encode_blv1(tlvs)->bytes

 Implement decode_blv1(bytes)->tlvs

 Enforce canonical ordering externally (builder must sort)

 Golden tests: q_count=4 hexdump, q_count=4096 lengths

Signatures

struct TLV {
  u8 type;     // 0x01, 0x02, ...
  u8 flags;
  std::vector<u8> value; // length must match header
};

struct BLV1Batch {
  u8 version;
  u8 flags;
  std::vector<TLV> tlvs;
};

Result<std::vector<u8>> blv1_encode(const BLV1Batch& b);
Result<BLV1Batch>       blv1_decode(SpanConstU8 payload);

3.2 uvcc/lift — LiftEngine

Responsibilities

Implements Lift v1 state machine from Parts 7–8

Accepts TLV 0x01 (head->tail) and TLV 0x02 (tail->other)

Uses lift PRG seeds to reconstruct RSS components

Writes components into SlotMap + emits transcript leaves

TODO

 Maintain LiftState per fss_id

 Provide produce_lift_tlvs(...) for HEAD/TAIL roles

 Provide on_lift_batch_recv(...) entry point from Transport

 Emit leaves:

LIFT_BATCH_SEND/RECV

RSS_COMPONENT_COMMIT

RSS_LIFT_DONE

Signatures

enum class LiftRole : u8 { HEAD=0, TAIL=1, OTHER=2 };

struct LiftParams {
  FssId64 fss_id;
  u32 q_count;
  u8 producer_edge_id; // 0,1,2
  // plus any layout/stride metadata from your slot format
};

struct LiftEngine {
  Sid32 sid_sub;
  PartyId self;
  KeyRegistry keys;

  // Called by SGIR op when you have an edge-FSS output on HEAD side.
  Status enqueue_lift_m_vector(
    SgirOpId32 op_id,
    PartyId dst_tail,
    const LiftParams& lp,
    SpanConstU8 m_u64_bytes // 8*q_count bytes
  );

  // Called by Transport after receiving FRAME.kind=0x66
  Status on_lift_batch_recv(const Frame& f, SpanConstU8 payload);

  // Called periodically to flush builders deterministically (end-of-k).
  Status flush_lift_batches(SgirOpId32 op_id);

  // Query readiness (for scheduler dependencies)
  bool is_lift_done(FssId64 fss_id) const;
};

Result<LiftEngine*> create_lift_engine(
  const Sid32& sid_sub,
  PartyId self,
  const KeyRegistry& keys,
  Transport* transport,
  /* SlotMap */ void* slots,
  /* Transcript */ void* transcript
);

3.3 uvcc/open — OpenEngine

Responsibilities

Multi-party open protocol management (whatever Parts 1–8 define)

Exactly-once via Transport; deterministic stream ids

Produces reconstructed public values for dependent compute

TODO

 Implement open send/recv state per (stream_id64, round_idx)

 Implement deterministic leaf commits for OPEN send/recv/done

 Provide Future/pollable completion for scheduler

Signatures

struct OpenRequest {
  StreamId64 stream_id;
  u16 round_idx;
  // share bytes to send (format per your OPEN spec)
  std::vector<u8> my_share;
};

struct OpenResult {
  StreamId64 stream_id;
  u16 round_idx;
  std::vector<u8> opened_value; // public
};

struct OpenEngine {
  Status start_open(SgirOpId32 op_id, const OpenRequest& req);
  bool   is_ready(StreamId64 stream_id, u16 round_idx) const;
  Result<OpenResult> take_result(StreamId64 stream_id, u16 round_idx);

  Status on_open_frame_recv(const Frame& f, SpanConstU8 payload);
};

3.4 uvcc/fss — wrapper

Responsibilities

Runs FSS eval on one producer edge

Produces the head/tail additive shares that feed Lift

Signatures

struct FssEvalInput {
  FssId64 fss_id;
  u32 q_count;
  // any FSS keys/material identifiers
};

struct FssEvalOutput {
  // two-party additive shares (head/tail)
  std::vector<u8> head_share; // 8*q_count
  std::vector<u8> tail_share; // 8*q_count
};

Result<FssEvalOutput> fss_eval_on_edge(
  const KeyRegistry& keys,
  PartyId self,
  u8 producer_edge_id,
  const FssEvalInput& in
);

3.5 uvcc/sks — scaffolding

Responsibilities

Deterministically decide when to run SKS checks

Bind to sid_sub + op_id

Perform SKS compute + OPEN(z)

Signatures

struct SKSConfig {
  bool enabled;
  u32  sample_rate_permille; // e.g. 10 = 1%
};

struct SKSEngine {
  SKSConfig cfg;

  bool should_check(SgirOpId32 op_id) const;

  Status run_gemm_check(
    SgirOpId32 op_id,
    /* references to A,B,C slots */ void* a, void* b, void* c,
    OpenEngine* open
  );
};

Phase 4 — Storage + IR runtime: arenas, slot map, SGIR program, op implementations

Phase 4 modules

uvcc/arena — STEP/EPOCH/PERSIST allocators (+ GPU buffers)

uvcc/slots — RSS SlotMap v1 (2 components per party)

uvcc/transcript — leaf writer + Merkle builder

uvcc/sgir — stage-local program representation + dispatcher

uvcc/ops — implementations of ops (GEMM_SS, LUT+LIFT, bias, residual, rmsnorm, etc.)


4.1 uvcc/arena

Signatures

enum class Life : u8 { STEP=0, EPOCH=1, PERSIST=2 };

struct DeviceBuffer {
  void*  dptr;
  size_t bytes;
};

struct Arena {
  Status reset(); // for STEP/EPOCH as needed
  Result<DeviceBuffer> alloc(Life life, size_t bytes, size_t alignment);
};

4.2 uvcc/slots — SlotMap

Responsibilities

Implements your RSS slot map v1

Each slot stores two device pointers for the two components this party owns

Deterministic slot ids and stable layout metadata

Signatures

struct SlotId {
  u32 id; // or whatever Part 1–8 uses
};

struct Slot {
  SlotId id;
  Life life;

  // The two components stored on this party (e.g., P0 has comp0 & comp1)
  DeviceBuffer comp_a;
  DeviceBuffer comp_b;

  // layout metadata
  u32 elem_bytes;  // 8 for u64 ring
  u32 q_count;
  u32 stride_bytes;
};

struct SlotMap {
  Result<Slot*> create_slot(SlotId id, Life life, u32 q_count, u32 elem_bytes);
  Result<Slot*> get_slot(SlotId id);
};

4.3 uvcc/transcript

Responsibilities

Append leaves in deterministic order

Build Merkle root per subsession

Provide “root-of-roots” assembly later (Phase 6)

Signatures

struct TranscriptWriter {
  Status append_leaf(SpanConstU8 leaf_bytes);
  Hash32 finalize_merkle_root();

  // optional: export leaf list for audit
  std::vector<Hash32> leaf_hashes() const;
};

4.4 uvcc/sgir — program + scheduler hooks

Responsibilities

Represent “ops for one stage” for forward/backward

Provide deterministic k indexing and dependencies

Signatures

enum class Phase : u8 { FWD=0, BWD=1, UPD=2 };

enum class OpCode : u16 {
  RECV_PP,
  SEND_PP,
  GEMM_SS,
  BIAS_ADD,
  RESIDUAL_ADD,
  RMSNORM_FWD,
  RMSNORM_BWD,
  GELU_APPROX,
  SOFTMAX_APPROX,
  SKS_GEMM_CHECK,
  // ...
};

struct Op {
  u16 k;        // fixed per stage program
  Phase phase;
  OpCode code;

  // operand slots (ids or pointers)
  std::vector<SlotId> inputs;
  std::vector<SlotId> outputs;

  // op params (shapes etc.)
  // ...
};

struct StageProgram {
  u16 stage_id;
  std::vector<Op> ops_fwd;
  std::vector<Op> ops_bwd;
  std::vector<Op> ops_upd;
};

4.5 uvcc/ops — the critical op APIs

You need your ops to be written so that:

local GPU compute can run immediately

comm-dependent pieces return a “pending” handle

the scheduler can poll readiness deterministically

Common op result type

enum class OpState : u8 { DONE=0, PENDING=1, ERROR=2 };

struct OpExecResult {
  OpState state;
  Status st;
};

GEMM_SS (secure matmul)

struct GemmParams {
  // shapes, tiling, TP layout details
  u32 m,n,k;
  u32 tile_m, tile_n, tile_k;
  bool row_parallel;
  bool col_parallel;
};

OpExecResult op_gemm_ss(
  const Sid32& sid_sub,
  SgirOpId32 op_id,
  const GemmParams& gp,
  Slot* A, Slot* B, Slot* C,
  /* TripleStore */ void* triples,
  OpenEngine* open,
  TranscriptWriter* transcript
);

Nonlinearity via FSS + LIFT (example: GELU_APPROX)

struct NonlinearParams {
  u16 fss_calls;   // number of FSS invocations inside this op
  u32 q_count;
};

OpExecResult op_gelu_approx_fss_lift(
  const Sid32& sid_sub,
  SgirOpId32 op_id,
  const NonlinearParams& np,
  Slot* X,
  Slot* Y,
  u16 tp_rank,
  u16 stage_id,
  /* FSS */ void* fss,
  LiftEngine* lift,
  TranscriptWriter* transcript
);

Softmax approximation via FSS + LIFT

OpExecResult op_softmax_approx_fss_lift(
  const Sid32& sid_sub,
  SgirOpId32 op_id,
  const NonlinearParams& np,
  Slot* Sscaled,
  Slot* P,
  /* FSS */ void* fss,
  LiftEngine* lift,
  OpenEngine* open, // if internal mult opens
  TranscriptWriter* transcript
);

Bias/residual (purely local on shares)

OpExecResult op_bias_add(Slot* X, Slot* B, Slot* Y);
OpExecResult op_residual_add(Slot* X, Slot* Y, Slot* Z);

Phase 5 — Parallelism runtime: NCCL groups, PP scheduler (1F1B), TP collectives, DP reduce, optimizer

Phase 5 modules

uvcc/nccl — communicator creation + wrappers

uvcc/pp — 1F1B microbatch scheduler (deterministic)

uvcc/tp — tensor-parallel collectives wrappers

uvcc/dp — replica gradient reduce (inside party only)

uvcc/optim — optimizer update on RSS shares

uvcc/runtime — glue loop that runs stage tasks + pumps transport


5.1 uvcc/nccl

struct NcclComm { /* handle */ };

Result<NcclComm> create_nccl_comm(const NcclGroup& g, u16 my_rank_in_party);

Status nccl_allreduce_sum(NcclComm* comm, DeviceBuffer buf);
Status nccl_allgather(NcclComm* comm, DeviceBuffer in, DeviceBuffer out);
Status nccl_send(NcclComm* comm, DeviceBuffer buf, u16 peer_rank);
Status nccl_recv(NcclComm* comm, DeviceBuffer buf, u16 peer_rank);

5.2 uvcc/pp — deterministic 1F1B scheduler

Responsibilities

Provide the next runnable (phase, mb, op_k) for this stage

Track dependencies (recv activation/grad, future completions)

Never allow GPU/network timing to reorder logical transcript order

struct TaskKey {
  Phase phase;
  u16 microbatch;
  u16 k;
};

struct TaskState {
  TaskKey key;
  bool runnable;
  bool done;
};

struct PPScheduler {
  u16 S;
  u16 M;
  u16 stage_id;

  // called when activation/grad arrives or when protocol completes
  void notify_event(/* what happened */);

  // returns next runnable task in lexicographic order (Phase,mb,k)
  Result<TaskKey> pick_next_runnable() const;

  void mark_done(const TaskKey& k);
};

5.3 uvcc/tp

struct TPContext {
  NcclComm comm;
  u16 tp_rank;
  u16 T;
};

// called inside GEMM implementations depending on row/col parallel
Status tp_allreduce_partial(TPContext* tp, DeviceBuffer partial);
Status tp_allgather_inputs(TPContext* tp, DeviceBuffer local, DeviceBuffer gathered);

5.4 uvcc/dp — gradient aggregation across replicas

struct DPContext {
  NcclComm comm; // group across replicas for fixed (s,t)
  u16 R;
};

Status dp_allreduce_gradients(
  DPContext* dp,
  Slot* grad_slot_comp_a,
  Slot* grad_slot_comp_b
);

5.5 uvcc/optim — optimizer on shares

Start with SGD to validate correctness (Adam later).

struct SGDParams {
  double lr;
};

Status sgd_update(
  const SGDParams& p,
  Slot* weight_comp_a, Slot* weight_comp_b,
  Slot* grad_comp_a,   Slot* grad_comp_b
);

5.6 uvcc/runtime — stage runner

This is where everything comes together per worker.

struct WorkerRuntime {
  UVCCConfig cfg;
  Coord coord;
  Sid32 sid_sub;

  Transport* transport;
  TranscriptWriter* transcript;
  SlotMap* slots;
  OpenEngine* open;
  LiftEngine* lift;

  PPScheduler pp_sched;

  // NCCL contexts
  TPContext tp;
  DPContext dp;
  // PP comms are per party; you can wrap send/recv separately

  StageProgram program;

  // one tick pumps network and runs as much compute as allowed
  Status tick_one();
};

Phase 6 — End-to-end step loop + audit bundle + full test suite

Phase 6 modules

uvcc/step — training step orchestrator (per party)

uvcc/audit — transcript-of-transcripts roots + signatures

uvcc/harness — e2e determinism + fault injection + perf


6.1 uvcc/step — training loop (per party)

Responsibilities

Coordinate step barriers inside party (optional)

Ensure DP reduce happens after backward

Ensure transcript roots are finalized and bundled

struct StepRunner {
  u32 step_id;

  Status run_one_step(std::vector<WorkerRuntime*>& workers);
};

6.2 uvcc/audit

struct SubsessionRoot {
  Coord coord;
  Hash32 merkle_root;
};

struct ReplicaRoot {
  u16 replica;
  Hash32 root;
};

struct GlobalAuditBundle {
  Sid32 sid_job;
  u32 step_id;
  Hash32 global_root;
  std::vector<ReplicaRoot> replica_roots;
  // signatures per party (as your Part 8 policy)
};

Hash32 compute_replica_root(u16 replica, const std::vector<SubsessionRoot>& subsession_roots);
Hash32 compute_global_root(const std::vector<ReplicaRoot>& reps);

Result<GlobalAuditBundle> build_audit_bundle(
  const Sid32& sid_job,
  u32 step_id,
  const std::vector<SubsessionRoot>& all_roots
);

6.3 uvcc/harness — required tests before you trust anything

Determinism

same input seeds → same msg_id32 sequence → same transcript roots

Fault injection

drops/dups/reorders → same final model shares and same accepted-leaf structure

Golden vectors

BLV1 encode/decode exact bytes

msg_id32 derived values

Signatures

Status run_unit_tests();
Status run_golden_tests();
Status run_determinism_test(u32 steps);
Status run_fault_injection_test(const FaultConfig& cfg);
Status run_perf_smoke_test();

Suggested repo layout (so you can start coding immediately)

uvcc/
  types.h
  bytes/
    bytes.h bytes.cpp bytes_test.cpp
  hash/
    sha256.h sha256.cpp sha256_test.cpp
  ids/
    ids.h ids.cpp ids_test.cpp
  config/
    config.h config.cpp
  topology/
    topology.h topology.cpp
  keys/
    keys.h keys.cpp
  net/
    net.h net.cpp
  frame/
    frame.h frame.cpp frame_test.cpp
  transport/
    transport.h transport.cpp transport_test.cpp
  blv1/
    blv1.h blv1.cpp blv1_test.cpp
  transcript/
    transcript.h transcript.cpp transcript_test.cpp
  arena/
    arena.h arena.cpp
  slots/
    slots.h slots.cpp
  open/
    open.h open.cpp
  lift/
    lift.h lift.cpp
  fss/
    fss.h fss.cpp
  sks/
    sks.h sks.cpp
  sgir/
    sgir.h sgir.cpp
  ops/
    gemm_ss.h gemm_ss.cpp
    nonlinear.h nonlinear.cpp
  nccl/
    nccl.h nccl.cpp
  pp/
    pp.h pp.cpp
  tp/
    tp.h tp.cpp
  dp/
    dp.h dp.cpp
  optim/
    optim.h optim.cpp
  runtime/
    worker_runtime.h worker_runtime.cpp
  step/
    step.h step.cpp
  audit/
    audit.h audit.cpp
  harness/
    harness.h harness.cpp

“Do not miss anything” — the Definition of Done per phase

Use this as your gate. Don’t advance phases until it passes.


Phase 0 DoD

derive_msg_id32 matches your Part 8 known test vectors

SHA256 correct vs known vectors

ByteReader/Writer correct + bounds-checked


Phase 1 DoD

Every worker prints (coord, sid_sub) and they match across parties

Channels connect and can exchange a ping payload


Phase 2 DoD

Exactly-once: duplicate frames accepted once

Retransmit: drop 20% still completes

Chunking: large payload reassembles correctly


Phase 3 DoD

BLV1/TLV golden dump matches bytes

LiftEngine completes LIFT for a toy vector and stores RSS components into slots

OpenEngine opens a toy scalar/vector correctly and deterministically


Phase 4 DoD

SlotMap allocations match lifecycle rules

SGIR executes a toy graph (bias+residual) deterministically

GEMM_SS runs with triple consumption ordering frozen (even if triples are mocked)


Phase 5 DoD

PP 1F1B schedule runs without deadlocks for M microbatches

TP collectives correct on shares

DP allreduce sums gradients across replicas (both components!)

SGD update changes weights deterministically


Phase 6 DoD

One end-to-end step produces a global audit bundle with stable roots

Determinism test: two runs → same global_root

Fault injection: drops/dups/reorder → same final global_root




Below is exactly what you asked for:

An exact k-index table for a realistic transformer block as a pasteable StageProgram literal (forward + backward), with explicit opcodes, tensor IDs, and per-op params placeholders.

The exact dependency rules each op registers:

which tensors/slots must be ready

which PP inbox events must be present

which protocol futures (OPEN / LIFT) must complete before the op is “DONE”

which next ops are therefore gated (via tensor readiness)

I’m going to assume your runtime is like we discussed:

StageProgram is per stage (PP stage), and runs per microbatch mb.

Weights are Life::PERSIST (one per worker (r,s,t)).

Activations/grads are Life::EPOCH (per microbatch).

Each op completion marks output tensors ready; later ops only depend on “tensor ready”.

Protocol waits are modeled as futures inside the op, i.e. an op can be PENDING until its own OPEN/LIFT futures complete.

A) Pasteable StageProgram literal

This is “literal code” you can paste into your uvcc/sgir layer (names/types match the earlier scaffolding; if your structs differ, this is still the exact content you want to represent).

A.1 Tensor IDs used by this block

// transformer_block_stage_program.h

enum class TensorId : u16 {
  // ---- persistent weights/params (Life::PERSIST) ----
  W_QKV,   B_QKV,
  W_O,     B_O,
  W1,      B1,
  W2,      B2,
  GAMMA1,  // RMSNorm1 scale
  GAMMA2,  // RMSNorm2 scale

  // ---- forward activations (Life::EPOCH, per microbatch) ----
  X_IN,
  INV_RMS1,   // saved for RMSNorm1 backward
  XN1,
  TQKV,
  Q, K, V,
  S, SSCALED,
  P,
  O,
  Y,
  X1,
  INV_RMS2,   // saved for RMSNorm2 backward
  XN2,
  H, H1,
  G,
  Z, Z1,
  X_OUT,

  // ---- backward grads (Life::EPOCH, per microbatch) ----
  DX_OUT,
  DX1,      // gradient wrt X1 (after residual2 split + rmsnorm2)
  DZ1,
  DG,
  DH1,
  DXN2,
  DY,
  DO,
  DP,
  DV,
  DSCALED,
  DS,
  DQ,
  DK,
  DTQKV,
  DXN1,
  DX_IN,

  // ---- weight grads (Life::EPOCH, aggregated later by DP) ----
  DW_QKV, DB_QKV,
  DW_O,   DB_O,
  DW1,    DB1_G,
  DW2,    DB2_G,
  DGAMMA1,
  DGAMMA2,
};
A.2 Op codes + param variants

enum class Phase : u8 { FWD=0, BWD=1, UPD=2 };

enum class OpCode : u16 {
  RECV_PP,
  SEND_PP,

  RMSNORM_FWD,   // produces (Xn, inv_rms) so backward can be arithmetic-only
  RMSNORM_BWD,   // consumes inv_rms + X_in + dXn -> dX + dGamma

  GEMM_SS,       // secure GEMM: internally does OPEN(E/F) per tile
  SKS_GEMM_CHECK, // optional; produces no tensors used by mainline

  BIAS_ADD,
  RESIDUAL_ADD,
  RESIDUAL_SPLIT, // for backprop through residual

  VIEW_SPLIT_QKV, // TQKV -> Q,K,V (views)
  VIEW_MERGE_DQKV, // DQ,DK,DV -> DTQKV (view/concat)

  SCALE_PUBLIC,   // multiply by public scalar

  SOFTMAX_APPROX_FWD, // uses FSS+LIFT internally, then secure multiplies
  SOFTMAX_BWD,        // arithmetic-only from P and dP (no new FSS)
  GELU_APPROX_FWD,    // uses FSS+LIFT internally, then secure multiplies
  GELU_DERIV_APPROX,  // uses FSS+LIFT internally to get gelu'(H1)
  ELEM_MUL,           // elementwise multiply (secret*secret) => internal OPENs

  REDUCE_BIAS_GRAD,   // sum over batch/token dims (linear)
};

struct GemmParams {
  // minimal knobs; fill with your real layout/tiling/TP flags
  u32 m, n, k;
  bool transA;
  bool transB;

  // TP strategy hints (implementation can ignore if fixed)
  bool tp_allgather_A;
  bool tp_allgather_B;
  bool tp_allreduce_C;

  // tiling for “how many OPEN(E/F) futures exist”
  u32 tile_count; // number of tiles this op breaks into (deterministic)
};

struct RMSNormParams {
  u32 q_count;        // number of elements in vector
  float eps;
  bool use_fss_rsqrt; // if true, inv_rms produced via FSS+LIFT inside op
  u16 fss_calls;      // typically 1
};

struct NonlinearParams {
  u32 q_count;
  u16 fss_calls; // how many FSS calls this composite issues
};

struct ScaleParams {
  float scalar;
};

struct SKSParams {
  bool enabled;
  u32  shard_idx; // for deterministic stream tags if you want
};

struct Op {
  u16   k;
  Phase phase;
  OpCode code;

  // tensors are symbolic; runtime maps (mb,tensor)->Slot*
  std::vector<TensorId> in;
  std::vector<TensorId> out;

  // exactly one of these is used depending on opcode
  // (use std::variant in real code; kept simple here)
  GemmParams gemm;
  RMSNormParams rms;
  NonlinearParams nonlin;
  ScaleParams scale;
  SKSParams sks;

  const char* name;
};

struct StageProgram {
  u16 stage_id;
  std::vector<Op> ops_fwd;
  std::vector<Op> ops_bwd;
};
A.3 The exact k-index program

This is the exact forward/backward k table as a literal StageProgram.

// MakeTransformerBlockStageProgram(...) - pasteable literal
static StageProgram MakeTransformerBlockStageProgram(
  u16 stage_id,
  u32 q_count,              // elements in activation vectors for this stage/tp shard
  u32 tile_count_gemm,       // deterministic tiling count for GEMMs in this stage
  bool enable_sks
) {
  StageProgram p;
  p.stage_id = stage_id;

  // -------------------------
  // Forward (Phase::FWD)
  // -------------------------
  p.ops_fwd = {

    // k=0: receive X_in from previous PP stage (or input feeder for stage 0)
    Op{
      .k=0, .phase=Phase::FWD, .code=OpCode::RECV_PP,
      .in={}, .out={TensorId::X_IN},
      .name="FWD_RECV_X_IN"
    },

    // k=1: RMSNorm1: X_in -> (XN1, INV_RMS1)
    Op{
      .k=1, .phase=Phase::FWD, .code=OpCode::RMSNORM_FWD,
      .in={TensorId::X_IN, TensorId::GAMMA1},
      .out={TensorId::XN1, TensorId::INV_RMS1},
      .rms=RMSNormParams{ .q_count=q_count, .eps=1e-5f, .use_fss_rsqrt=true, .fss_calls=1 },
      .name="FWD_RMSNORM1"
    },

    // k=2: GEMM QKV: XN1 * W_QKV -> TQKV
    Op{
      .k=2, .phase=Phase::FWD, .code=OpCode::GEMM_SS,
      .in={TensorId::XN1, TensorId::W_QKV},
      .out={TensorId::TQKV},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=false, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="FWD_GEMM_QKV"
    },

    // k=3: optional SKS check of QKV GEMM (does NOT gate next ops; see deps section)
    Op{
      .k=3, .phase=Phase::FWD, .code=OpCode::SKS_GEMM_CHECK,
      .in={TensorId::XN1, TensorId::W_QKV, TensorId::TQKV},
      .out={},
      .sks=SKSParams{ .enabled=enable_sks, .shard_idx=0 },
      .name="FWD_SKS_QKV"
    },

    // k=4: bias add for QKV
    Op{
      .k=4, .phase=Phase::FWD, .code=OpCode::BIAS_ADD,
      .in={TensorId::TQKV, TensorId::B_QKV},
      .out={TensorId::TQKV}, // in-place OK
      .name="FWD_BIAS_QKV"
    },

    // k=5: view split TQKV -> Q,K,V
    Op{
      .k=5, .phase=Phase::FWD, .code=OpCode::VIEW_SPLIT_QKV,
      .in={TensorId::TQKV},
      .out={TensorId::Q, TensorId::K, TensorId::V},
      .name="FWD_SPLIT_QKV"
    },

    // k=6: GEMM scores: S = Q * K^T
    Op{
      .k=6, .phase=Phase::FWD, .code=OpCode::GEMM_SS,
      .in={TensorId::Q, TensorId::K},
      .out={TensorId::S},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=true,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="FWD_GEMM_QK_T"
    },

    // k=7: scale scores by public scalar
    Op{
      .k=7, .phase=Phase::FWD, .code=OpCode::SCALE_PUBLIC,
      .in={TensorId::S},
      .out={TensorId::SSCALED},
      .scale=ScaleParams{ .scalar=1.0f }, // fill with 1/sqrt(d)
      .name="FWD_SCALE_SCORES"
    },

    // k=8: softmax approx: SSCALED -> P
    Op{
      .k=8, .phase=Phase::FWD, .code=OpCode::SOFTMAX_APPROX_FWD,
      .in={TensorId::SSCALED},
      .out={TensorId::P},
      .nonlin=NonlinearParams{ .q_count=q_count, .fss_calls=2 }, // exp + invsum
      .name="FWD_SOFTMAX_APPROX"
    },

    // k=9: GEMM weighted sum: O = P * V
    Op{
      .k=9, .phase=Phase::FWD, .code=OpCode::GEMM_SS,
      .in={TensorId::P, TensorId::V},
      .out={TensorId::O},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="FWD_GEMM_PV"
    },

    // k=10: GEMM output projection: Y = O * W_O
    Op{
      .k=10, .phase=Phase::FWD, .code=OpCode::GEMM_SS,
      .in={TensorId::O, TensorId::W_O},
      .out={TensorId::Y},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=false, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="FWD_GEMM_O_PROJ"
    },

    // k=11: optional SKS for output projection
    Op{
      .k=11, .phase=Phase::FWD, .code=OpCode::SKS_GEMM_CHECK,
      .in={TensorId::O, TensorId::W_O, TensorId::Y},
      .out={},
      .sks=SKSParams{ .enabled=enable_sks, .shard_idx=1 },
      .name="FWD_SKS_O_PROJ"
    },

    // k=12: add output proj bias
    Op{
      .k=12, .phase=Phase::FWD, .code=OpCode::BIAS_ADD,
      .in={TensorId::Y, TensorId::B_O},
      .out={TensorId::Y}, // in-place
      .name="FWD_BIAS_O"
    },

    // k=13: residual1: X1 = X_IN + Y
    Op{
      .k=13, .phase=Phase::FWD, .code=OpCode::RESIDUAL_ADD,
      .in={TensorId::X_IN, TensorId::Y},
      .out={TensorId::X1},
      .name="FWD_RESIDUAL1"
    },

    // k=14: RMSNorm2: X1 -> (XN2, INV_RMS2)
    Op{
      .k=14, .phase=Phase::FWD, .code=OpCode::RMSNORM_FWD,
      .in={TensorId::X1, TensorId::GAMMA2},
      .out={TensorId::XN2, TensorId::INV_RMS2},
      .rms=RMSNormParams{ .q_count=q_count, .eps=1e-5f, .use_fss_rsqrt=true, .fss_calls=1 },
      .name="FWD_RMSNORM2"
    },

    // k=15: GEMM MLP1: H = XN2 * W1
    Op{
      .k=15, .phase=Phase::FWD, .code=OpCode::GEMM_SS,
      .in={TensorId::XN2, TensorId::W1},
      .out={TensorId::H},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=false, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="FWD_GEMM_MLP1"
    },

    // k=16: optional SKS MLP1
    Op{
      .k=16, .phase=Phase::FWD, .code=OpCode::SKS_GEMM_CHECK,
      .in={TensorId::XN2, TensorId::W1, TensorId::H},
      .out={},
      .sks=SKSParams{ .enabled=enable_sks, .shard_idx=2 },
      .name="FWD_SKS_MLP1"
    },

    // k=17: bias add MLP1: H1 = H + B1
    Op{
      .k=17, .phase=Phase::FWD, .code=OpCode::BIAS_ADD,
      .in={TensorId::H, TensorId::B1},
      .out={TensorId::H1},
      .name="FWD_BIAS_MLP1"
    },

    // k=18: GELU approx: H1 -> G
    Op{
      .k=18, .phase=Phase::FWD, .code=OpCode::GELU_APPROX_FWD,
      .in={TensorId::H1},
      .out={TensorId::G},
      .nonlin=NonlinearParams{ .q_count=q_count, .fss_calls=1 },
      .name="FWD_GELU_APPROX"
    },

    // k=19: GEMM MLP2: Z = G * W2
    Op{
      .k=19, .phase=Phase::FWD, .code=OpCode::GEMM_SS,
      .in={TensorId::G, TensorId::W2},
      .out={TensorId::Z},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=false, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="FWD_GEMM_MLP2"
    },

    // k=20: optional SKS MLP2
    Op{
      .k=20, .phase=Phase::FWD, .code=OpCode::SKS_GEMM_CHECK,
      .in={TensorId::G, TensorId::W2, TensorId::Z},
      .out={},
      .sks=SKSParams{ .enabled=enable_sks, .shard_idx=3 },
      .name="FWD_SKS_MLP2"
    },

    // k=21: bias add MLP2: Z1 = Z + B2
    Op{
      .k=21, .phase=Phase::FWD, .code=OpCode::BIAS_ADD,
      .in={TensorId::Z, TensorId::B2},
      .out={TensorId::Z1},
      .name="FWD_BIAS_MLP2"
    },

    // k=22: residual2: X_OUT = X1 + Z1
    Op{
      .k=22, .phase=Phase::FWD, .code=OpCode::RESIDUAL_ADD,
      .in={TensorId::X1, TensorId::Z1},
      .out={TensorId::X_OUT},
      .name="FWD_RESIDUAL2"
    },

    // k=23: send X_OUT to next PP stage
    Op{
      .k=23, .phase=Phase::FWD, .code=OpCode::SEND_PP,
      .in={TensorId::X_OUT}, .out={},
      .name="FWD_SEND_X_OUT"
    },
  };

  // -------------------------
  // Backward (Phase::BWD)
  // -------------------------
  p.ops_bwd = {

    // k=100: receive dX_out from next PP stage (or loss grad feeder if last stage)
    Op{
      .k=100, .phase=Phase::BWD, .code=OpCode::RECV_PP,
      .in={}, .out={TensorId::DX_OUT},
      .name="BWD_RECV_DX_OUT"
    },

    // k=101: residual2 split: X_OUT = X1 + Z1
    // so dX1 += dX_out ; dZ1 += dX_out
    Op{
      .k=101, .phase=Phase::BWD, .code=OpCode::RESIDUAL_SPLIT,
      .in={TensorId::DX_OUT},
      .out={TensorId::DX1, TensorId::DZ1},
      .name="BWD_RESIDUAL2_SPLIT"
    },

    // k=102: db2 = reduce(dZ1)
    Op{
      .k=102, .phase=Phase::BWD, .code=OpCode::REDUCE_BIAS_GRAD,
      .in={TensorId::DZ1},
      .out={TensorId::DB2_G},
      .name="BWD_DB2"
    },

    // k=103: dW2 = G^T * dZ1
    Op{
      .k=103, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::G, TensorId::DZ1},
      .out={TensorId::DW2},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=true,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=true,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DW2"
    },

    // k=104: dG = dZ1 * W2^T
    Op{
      .k=104, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::DZ1, TensorId::W2},
      .out={TensorId::DG},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=true,
        .tp_allgather_A=true, .tp_allgather_B=false, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DG"
    },

    // k=105: gelu' approx from H1 (uses FSS+LIFT internally)
    Op{
      .k=105, .phase=Phase::BWD, .code=OpCode::GELU_DERIV_APPROX,
      .in={TensorId::H1},
      .out={TensorId::DH1}, // we’ll overwrite DH1 later after mul; this holds gelu'(H1) for now
      .nonlin=NonlinearParams{ .q_count=q_count, .fss_calls=1 },
      .name="BWD_GELU_DERIV"
    },

    // k=106: dH1 = dG ⊙ gelu'(H1)
    Op{
      .k=106, .phase=Phase::BWD, .code=OpCode::ELEM_MUL,
      .in={TensorId::DG, TensorId::DH1}, // DH1 currently stores gelu'(H1)
      .out={TensorId::DH1},              // overwrite with actual dH1
      .name="BWD_ELEM_MUL_DH1"
    },

    // k=107: db1 = reduce(dH1)
    Op{
      .k=107, .phase=Phase::BWD, .code=OpCode::REDUCE_BIAS_GRAD,
      .in={TensorId::DH1},
      .out={TensorId::DB1_G},
      .name="BWD_DB1"
    },

    // k=108: dW1 = XN2^T * dH1
    Op{
      .k=108, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::XN2, TensorId::DH1},
      .out={TensorId::DW1},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=true,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=true,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DW1"
    },

    // k=109: dXN2 = dH1 * W1^T
    Op{
      .k=109, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::DH1, TensorId::W1},
      .out={TensorId::DXN2},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=true,
        .tp_allgather_A=true, .tp_allgather_B=false, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DXN2"
    },

    // k=110: RMSNorm2 backward: (X1, inv_rms2, dXN2, gamma2) -> add into dX1, and produce dGamma2
    Op{
      .k=110, .phase=Phase::BWD, .code=OpCode::RMSNORM_BWD,
      .in={TensorId::X1, TensorId::INV_RMS2, TensorId::DXN2, TensorId::GAMMA2},
      .out={TensorId::DX1, TensorId::DGAMMA2}, // DX1 is updated/accumulated
      .rms=RMSNormParams{ .q_count=q_count, .eps=1e-5f, .use_fss_rsqrt=false, .fss_calls=0 },
      .name="BWD_RMSNORM2"
    },

    // k=111: residual1 split: X1 = X_IN + Y => dX_IN += dX1 ; dY += dX1
    Op{
      .k=111, .phase=Phase::BWD, .code=OpCode::RESIDUAL_SPLIT,
      .in={TensorId::DX1},
      .out={TensorId::DX_IN, TensorId::DY},
      .name="BWD_RESIDUAL1_SPLIT"
    },

    // k=112: dbo = reduce(dY)
    Op{
      .k=112, .phase=Phase::BWD, .code=OpCode::REDUCE_BIAS_GRAD,
      .in={TensorId::DY},
      .out={TensorId::DB_O},
      .name="BWD_DBO"
    },

    // k=113: dWo = O^T * dY
    Op{
      .k=113, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::O, TensorId::DY},
      .out={TensorId::DW_O},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=true,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=true,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DWO"
    },

    // k=114: dO = dY * Wo^T
    Op{
      .k=114, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::DY, TensorId::W_O},
      .out={TensorId::DO},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=true,
        .tp_allgather_A=true, .tp_allgather_B=false, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DO"
    },

    // k=115: dP = dO * V^T
    Op{
      .k=115, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::DO, TensorId::V},
      .out={TensorId::DP},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=true,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DP"
    },

    // k=116: dV = P^T * dO
    Op{
      .k=116, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::P, TensorId::DO},
      .out={TensorId::DV},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=true,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DV"
    },

    // k=117: softmax backward (arithmetic-only): (P, dP) -> dSscaled
    Op{
      .k=117, .phase=Phase::BWD, .code=OpCode::SOFTMAX_BWD,
      .in={TensorId::P, TensorId::DP},
      .out={TensorId::DSCALED},
      .name="BWD_SOFTMAX_BWD"
    },

    // k=118: scale backward: dS = dSscaled * (1/sqrt(d)) (public)
    Op{
      .k=118, .phase=Phase::BWD, .code=OpCode::SCALE_PUBLIC,
      .in={TensorId::DSCALED},
      .out={TensorId::DS},
      .scale=ScaleParams{ .scalar=1.0f }, // same scalar as forward
      .name="BWD_SCALE"
    },

    // k=119: dQ = dS * K
    Op{
      .k=119, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::DS, TensorId::K},
      .out={TensorId::DQ},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DQ"
    },

    // k=120: dK = dS^T * Q
    Op{
      .k=120, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::DS, TensorId::Q},
      .out={TensorId::DK},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=true,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DK"
    },

    // k=121: merge (DQ,DK,DV) -> DTQKV (view/concat)
    Op{
      .k=121, .phase=Phase::BWD, .code=OpCode::VIEW_MERGE_DQKV,
      .in={TensorId::DQ, TensorId::DK, TensorId::DV},
      .out={TensorId::DTQKV},
      .name="BWD_MERGE_DTQKV"
    },

    // k=122: dbqkv = reduce(dTQKV)
    Op{
      .k=122, .phase=Phase::BWD, .code=OpCode::REDUCE_BIAS_GRAD,
      .in={TensorId::DTQKV},
      .out={TensorId::DB_QKV},
      .name="BWD_DBQKV"
    },

    // k=123: dWqkv = XN1^T * dTQKV
    Op{
      .k=123, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::XN1, TensorId::DTQKV},
      .out={TensorId::DW_QKV},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=true,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=true,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DWQKV"
    },

    // k=124: dXN1 = dTQKV * Wqkv^T
    Op{
      .k=124, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::DTQKV, TensorId::W_QKV},
      .out={TensorId::DXN1},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=true,
        .tp_allgather_A=true, .tp_allgather_B=false, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DXN1"
    },

    // k=125: RMSNorm1 backward: (X_in, inv_rms1, dXN1, gamma1) -> add into dX_in, and produce dGamma1
    Op{
      .k=125, .phase=Phase::BWD, .code=OpCode::RMSNORM_BWD,
      .in={TensorId::X_IN, TensorId::INV_RMS1, TensorId::DXN1, TensorId::GAMMA1},
      .out={TensorId::DX_IN, TensorId::DGAMMA1}, // DX_IN accumulates
      .rms=RMSNormParams{ .q_count=q_count, .eps=1e-5f, .use_fss_rsqrt=false, .fss_calls=0 },
      .name="BWD_RMSNORM1"
    },

    // k=126: send dX_in to previous PP stage
    Op{
      .k=126, .phase=Phase::BWD, .code=OpCode::SEND_PP,
      .in={TensorId::DX_IN}, .out={},
      .name="BWD_SEND_DX_IN"
    },
  };

  return p;
}
That is the exact k-index table in a form you can paste and compile (once you wire your own slot mapping and param unions/variants).

B) Exact dependency graph rules (slots + protocol futures + PP events)

You asked: “the exact dependency graph rules each op registers (which slot/future gates which next op).”

Below is the precise rule-set I recommend implementing, and then I’ll list the per-op dependencies for this exact program.

B.1 Canonical “what is a dependency” in this runtime

Model each op as:

Prereqs (must be satisfied to start op)

Internal futures (created on start; must complete before op is DONE)

Signals (tensors marked ready when op DONE; PP sends posted, etc.)

Dependency primitives

enum class DepKind : u8 {
  TENSOR_READY,     // slot exists + filled for this (mb,tensor)
  PP_INBOX_READY,   // activation/grad arrived from prev/next stage
  TRIPLE_READY,     // Beaver triple tile(s) available
  OPEN_DONE,        // OpenEngine has result for (stream_id, round)
  LIFT_DONE,        // LiftEngine reports is_lift_done(fss_id)==true
  NCCL_DONE,        // local collective completed (tp gather/reduce)
};

struct Dep {
  DepKind kind;
  TensorId tensor;     // for TENSOR_READY/PP_INBOX_READY
  StreamId64 stream;   // for OPEN_DONE
  u16 round;
  FssId64 fss_id;      // for LIFT_DONE
  u32 tile_idx;        // for TRIPLE_READY/OPEN streams, if you want explicit tiles
};
Two important invariants for determinism

Outputs become “ready” only when the op is DONE.
So later ops don’t directly depend on OPEN/LIFT; they depend on output tensor readiness.

Protocol futures are deterministic in naming/count.
For GEMM_SS, the number of tiles and the exact stream tags must be fixed → you get stable transcript binding.

B.2 How each OpCode declares deps + internal futures

This is the exact “rule function” you implement once, and it covers the whole block.

struct OpDeps {
  std::vector<Dep> prereqs;     // to start
  std::vector<Dep> internal;    // must complete before DONE
};

// Helpers you already have:
SgirOpId32 op_id_for(const Sid32& sid_sub, u32 step, u16 mb, Phase ph, u16 k);
StreamId64 open_stream(const Sid32& sid_sub, SgirOpId32 op, const char* tag, u32 tile);
FssId64    fss_id_for(const Sid32& sid_sub, SgirOpId32 op, u16 fss_call_idx);

static OpDeps ComputeDepsForOp(
  const Sid32& sid_sub,
  u32 step_id,
  u16 mb,
  const Op& op
) {
  OpDeps d;

  // Base prereqs: every input tensor must be ready (weights are “always ready” because PERSIST)
  for (auto t : op.in) {
    d.prereqs.push_back(Dep{ .kind=DepKind::TENSOR_READY, .tensor=t });
  }

  switch (op.code) {
    case OpCode::RECV_PP: {
      // Must have PP inbox event for that output tensor
      // (RECV_PP has no inputs; it produces one output tensor)
      d.prereqs.clear();
      d.prereqs.push_back(Dep{ .kind=DepKind::PP_INBOX_READY, .tensor=op.out.at(0) });
      break;
    }

    case OpCode::SEND_PP: {
      // Must have the input tensor ready. No internal futures required to mark op DONE;
      // posting the send is enough. (Delivery is tracked by receiver’s PP_INBOX_READY.)
      break;
    }

    case OpCode::GEMM_SS: {
      // GEMM requires triples per tile, then OPEN(E/F) per tile.
      // The op is DONE only when all OPEN(E/F) streams are complete
      // AND any required TP collectives are complete.
      auto opid = op_id_for(sid_sub, step_id, mb, op.phase, op.k);

      for (u32 tile = 0; tile < op.gemm.tile_count; ++tile) {
        d.prereqs.push_back(Dep{ .kind=DepKind::TRIPLE_READY, .tile_idx=tile });

        // internal OPEN futures
        d.internal.push_back(Dep{
          .kind=DepKind::OPEN_DONE,
          .stream=open_stream(sid_sub, opid, "GEMM_E", tile),
          .round=0, .tile_idx=tile
        });
        d.internal.push_back(Dep{
          .kind=DepKind::OPEN_DONE,
          .stream=open_stream(sid_sub, opid, "GEMM_F", tile),
          .round=0, .tile_idx=tile
        });
      }

      // TP collectives (if your GEMM implementation does them asynchronously)
      if (op.gemm.tp_allgather_A) d.internal.push_back(Dep{ .kind=DepKind::NCCL_DONE });
      if (op.gemm.tp_allgather_B) d.internal.push_back(Dep{ .kind=DepKind::NCCL_DONE });
      if (op.gemm.tp_allreduce_C) d.internal.push_back(Dep{ .kind=DepKind::NCCL_DONE });

      break;
    }

    case OpCode::RMSNORM_FWD: {
      // If rsqrt uses FSS+LIFT, then inv_rms is gated by LiftDone for each FSS call.
      auto opid = op_id_for(sid_sub, step_id, mb, op.phase, op.k);
      if (op.rms.use_fss_rsqrt) {
        for (u16 i=0;i<op.rms.fss_calls;i++) {
          auto fssid = fss_id_for(sid_sub, opid, i);
          d.internal.push_back(Dep{ .kind=DepKind::LIFT_DONE, .fss_id=fssid });
        }
      }

      // RMSNorm also contains secret*secret multiplies internally (x*x, x*inv, *gamma),
      // so it will internally post OPENs. Model it like GEMM: deterministic stream tags.
      // (Exact count depends on your tiling; keep it deterministic!)
      // Example: 2 tiles for vector ops:
      for (u32 tile=0; tile<2; ++tile) {
        d.internal.push_back(Dep{
          .kind=DepKind::OPEN_DONE,
          .stream=open_stream(sid_sub, opid, "RMS_MUL", tile),
          .round=0, .tile_idx=tile
        });
      }
      break;
    }

    case OpCode::RMSNORM_BWD: {
      // We designed it to be arithmetic-only by consuming INV_RMS saved from forward.
      // Still has secret multiplies => internal OPENs, deterministic tags:
      auto opid = op_id_for(sid_sub, step_id, mb, op.phase, op.k);
      for (u32 tile=0; tile<2; ++tile) {
        d.internal.push_back(Dep{
          .kind=DepKind::OPEN_DONE,
          .stream=open_stream(sid_sub, opid, "RMSBWD_MUL", tile),
          .round=0, .tile_idx=tile
        });
      }
      // plus any TP allreduces for reductions if you do them across TP
      d.internal.push_back(Dep{ .kind=DepKind::NCCL_DONE });
      break;
    }

    case OpCode::SOFTMAX_APPROX_FWD: {
      // Forward softmax uses FSS+LIFT (exp + invsum), then secret multiplies.
      auto opid = op_id_for(sid_sub, step_id, mb, op.phase, op.k);

      for (u16 i=0;i<op.nonlin.fss_calls;i++) {
        auto fssid = fss_id_for(sid_sub, opid, i);
        d.internal.push_back(Dep{ .kind=DepKind::LIFT_DONE, .fss_id=fssid });
      }

      // secret multiplies => OPEN futures (deterministic tiling)
      for (u32 tile=0; tile<2; ++tile) {
        d.internal.push_back(Dep{
          .kind=DepKind::OPEN_DONE,
          .stream=open_stream(sid_sub, opid, "SOFT_MUL", tile),
          .round=0, .tile_idx=tile
        });
      }
      break;
    }

    case OpCode::SOFTMAX_BWD: {
      // Arithmetic-only from P and dP, but still secret multiplies => OPENs.
      auto opid = op_id_for(sid_sub, step_id, mb, op.phase, op.k);
      for (u32 tile=0; tile<2; ++tile) {
        d.internal.push_back(Dep{
          .kind=DepKind::OPEN_DONE,
          .stream=open_stream(sid_sub, opid, "SOFTBWD_MUL", tile),
          .round=0, .tile_idx=tile
        });
      }
      break;
    }

    case OpCode::GELU_APPROX_FWD:
    case OpCode::GELU_DERIV_APPROX: {
      // Uses FSS+LIFT internally then multiplies.
      auto opid = op_id_for(sid_sub, step_id, mb, op.phase, op.k);
      for (u16 i=0;i<op.nonlin.fss_calls;i++) {
        auto fssid = fss_id_for(sid_sub, opid, i);
        d.internal.push_back(Dep{ .kind=DepKind::LIFT_DONE, .fss_id=fssid });
      }
      for (u32 tile=0; tile<2; ++tile) {
        d.internal.push_back(Dep{
          .kind=DepKind::OPEN_DONE,
          .stream=open_stream(sid_sub, opid, "GELU_MUL", tile),
          .round=0, .tile_idx=tile
        });
      }
      break;
    }

    case OpCode::ELEM_MUL: {
      // secret*secret elementwise multiply => internal OPENs.
      auto opid = op_id_for(sid_sub, step_id, mb, op.phase, op.k);
      for (u32 tile=0; tile<2; ++tile) {
        d.internal.push_back(Dep{
          .kind=DepKind::OPEN_DONE,
          .stream=open_stream(sid_sub, opid, "ELEM_MUL", tile),
          .round=0, .tile_idx=tile
        });
      }
      break;
    }

    case OpCode::SKS_GEMM_CHECK: {
      // SKS has internal opens, but it produces no tensors the mainline depends on.
      // Dependency: it can start once its inputs exist. Completion is only required
      // before end-of-step finalize (audit), not before next op.
      auto opid = op_id_for(sid_sub, step_id, mb, op.phase, op.k);
      d.internal.push_back(Dep{
        .kind=DepKind::OPEN_DONE,
        .stream=open_stream(sid_sub, opid, "SKS_Z", 0),
        .round=0
      });
      break;
    }

    case OpCode::BIAS_ADD:
    case OpCode::RESIDUAL_ADD:
    case OpCode::RESIDUAL_SPLIT:
    case OpCode::VIEW_SPLIT_QKV:
    case OpCode::VIEW_MERGE_DQKV:
    case OpCode::SCALE_PUBLIC:
    case OpCode::REDUCE_BIAS_GRAD: {
      // Purely local / linear (no LIFT/OPEN), except REDUCE may do TP allreduce.
      if (op.code == OpCode::REDUCE_BIAS_GRAD) {
        d.internal.push_back(Dep{ .kind=DepKind::NCCL_DONE });
      }
      break;
    }

    default: break;
  }

  return d;
}
That is the exact rule engine. Now I’ll apply it to the exact program so you see the concrete “which gates which”.

B.3 Concrete dependencies for every op in the program

This is the “expanded view” you can use to verify you didn’t miss anything.

Forward dependencies (mb fixed)

FWD k=0 RECV_PP X_IN

Prereqs: PP_INBOX_READY(X_IN)

Internal futures: none

Signals: TENSOR_READY(X_IN)

Gates next: k=1

FWD k=1 RMSNORM1_FWD (X_IN,GAMMA1 → XN1,INV_RMS1)

Prereqs: TENSOR_READY(X_IN), TENSOR_READY(GAMMA1)

Internal futures:

If rsqrt via FSS+LIFT: LIFT_DONE(fss_id(op=1,call=0))

internal OPENs: OPEN_DONE(stream "RMS_MUL", tile=0..1) (exact count must be fixed)

Signals: TENSOR_READY(XN1), TENSOR_READY(INV_RMS1)

Gates next: k=2, and later k=125 uses X_IN+INV_RMS1

FWD k=2 GEMM_QKV (XN1,W_QKV → TQKV)

Prereqs: TENSOR_READY(XN1), TENSOR_READY(W_QKV), plus TRIPLE_READY(tile=0..tile_count-1)

Internal futures: for each tile:

OPEN_DONE("GEMM_E", tile)

OPEN_DONE("GEMM_F", tile)

plus any async TP collectives if you expose them as NCCL futures

Signals: TENSOR_READY(TQKV)

Gates next: k=4, k=5 (and k=3 SKS can start too)

FWD k=3 SKS_QKV (optional)

Prereqs: TENSOR_READY(XN1), TENSOR_READY(W_QKV), TENSOR_READY(TQKV)

Internal futures: OPEN_DONE("SKS_Z",0) (and any SKS internal opens you model)

Signals: none needed by mainline

Gates next: none (only “end-of-step finalize” must wait for SKS completion if enabled)

FWD k=4 BIAS_QKV

Prereqs: TENSOR_READY(TQKV), TENSOR_READY(B_QKV)

Internal futures: none

Signals: TENSOR_READY(TQKV) (in-place)

Gates next: k=5

FWD k=5 SPLIT_QKV (TQKV → Q,K,V)

Prereqs: TENSOR_READY(TQKV)

Signals: TENSOR_READY(Q), TENSOR_READY(K), TENSOR_READY(V)

Gates next: k=6, k=9, backward k=115/116/119/120 later

FWD k=6 GEMM_QK_T (Q,K → S)

Prereqs: TENSOR_READY(Q), TENSOR_READY(K), triples

Internal futures: OPEN(E/F) per tile

Signals: TENSOR_READY(S)

Gates next: k=7

FWD k=7 SCALE_PUBLIC (S → SSCALED)

Prereqs: TENSOR_READY(S)

Signals: TENSOR_READY(SSCALED)

Gates next: k=8

FWD k=8 SOFTMAX_APPROX_FWD (SSCALED → P)

Prereqs: TENSOR_READY(SSCALED)

Internal futures:

LIFT_DONE(fss_id(call=0)) // exp approx

LIFT_DONE(fss_id(call=1)) // invsum approx

internal OPENs for secret multiplies: OPEN_DONE("SOFT_MUL", tile=0..1)

Signals: TENSOR_READY(P)

Gates next: k=9 and backward k=116/117 later

FWD k=9 GEMM_PV (P,V → O)

Prereqs: TENSOR_READY(P), TENSOR_READY(V), triples

Internal: OPEN(E/F) per tile

Signals: TENSOR_READY(O)

Gates next: k=10, backward k=113/115/116

FWD k=10 GEMM_O_PROJ (O,W_O → Y)

Prereqs: TENSOR_READY(O), TENSOR_READY(W_O), triples

Internal: OPEN(E/F) per tile

Signals: TENSOR_READY(Y)

Gates next: k=12, k=13, and SKS k=11

FWD k=11 SKS_O_PROJ (optional)

Prereqs: TENSOR_READY(O), TENSOR_READY(W_O), TENSOR_READY(Y)

Internal: OPEN_DONE("SKS_Z",0)

Signals: none

Gates next: none

FWD k=12 BIAS_O

Prereqs: TENSOR_READY(Y), TENSOR_READY(B_O)

Signals: TENSOR_READY(Y) (in-place)

Gates next: k=13

FWD k=13 RESIDUAL1 (X_IN,Y → X1)

Prereqs: TENSOR_READY(X_IN), TENSOR_READY(Y)

Signals: TENSOR_READY(X1)

Gates next: k=14 and backward k=110/111

FWD k=14 RMSNORM2_FWD (X1,GAMMA2 → XN2,INV_RMS2)

Prereqs: TENSOR_READY(X1), TENSOR_READY(GAMMA2)

Internal: LIFT_DONE(call0) if FSS rsqrt; OPEN_DONE("RMS_MUL", tiles)

Signals: TENSOR_READY(XN2), TENSOR_READY(INV_RMS2)

Gates next: k=15, backward k=110

FWD k=15 GEMM_MLP1 (XN2,W1 → H)

Prereqs: TENSOR_READY(XN2), TENSOR_READY(W1), triples

Internal: OPEN(E/F) per tile

Signals: TENSOR_READY(H)

Gates next: k=17, SKS k=16

FWD k=16 SKS_MLP1

Prereqs: XN2,W1,H ready

Internal: OPEN_DONE("SKS_Z")

Signals: none

FWD k=17 BIAS_MLP1 (H,B1 → H1)

Prereqs: H ready, B1 ready

Signals: TENSOR_READY(H1)

Gates next: k=18 and backward k=105

FWD k=18 GELU_APPROX_FWD (H1 → G)

Prereqs: H1 ready

Internal: LIFT_DONE(call0) + OPEN_DONE("GELU_MUL", tile=0..1)

Signals: G ready

Gates next: k=19 and backward k=103

FWD k=19 GEMM_MLP2 (G,W2 → Z)

Prereqs: G,W2 ready, triples

Internal: OPEN(E/F)

Signals: Z ready

Gates next: k=21, SKS k=20, backward k=104

FWD k=20 SKS_MLP2

Prereqs: G,W2,Z ready

Internal: OPEN_DONE("SKS_Z")

Signals: none

FWD k=21 BIAS_MLP2 (Z,B2 → Z1)

Prereqs: Z,B2 ready

Signals: Z1 ready

Gates next: k=22 and backward k=101/102

FWD k=22 RESIDUAL2 (X1,Z1 → X_OUT)

Prereqs: X1,Z1 ready

Signals: X_OUT ready

Gates next: k=23 and backward k=100 depends on DX_OUT, not X_OUT

FWD k=23 SEND_PP X_OUT

Prereqs: X_OUT ready

Signals: posts PP send event (receiver will get PP_INBOX_READY(X_OUT))

Gates next: none inside this stage

Backward dependencies (same mb)

BWD k=100 RECV_PP DX_OUT

Prereqs: PP_INBOX_READY(DX_OUT)

Signals: DX_OUT ready

Gates next: k=101

BWD k=101 RESIDUAL2_SPLIT (DX_OUT → DX1,DZ1)

Prereqs: DX_OUT ready

Signals: DX1 ready, DZ1 ready

Gates next: k=102,k=103,k=104 and k=110 consumes DX1 later

BWD k=102 DB2 = reduce(DZ1)

Prereqs: DZ1 ready

Internal: NCCL_DONE if reduced across TP

Signals: DB2_G ready

Gates next: none (optimizer/update later)

BWD k=103 DW2 = G^T * DZ1

Prereqs: G ready (from FWD k=18), DZ1 ready, triples

Internal: OPEN(E/F) per tile, NCCL_DONE if TP allreduce for grad shard

Signals: DW2 ready

BWD k=104 DG = DZ1 * W2^T

Prereqs: DZ1 ready, W2 ready, triples

Internal: OPEN(E/F)

Signals: DG ready

Gates next: k=105, k=106

BWD k=105 GELU_DERIV (H1 → DH1=gelu’)

Prereqs: H1 ready (FWD k=17)

Internal: LIFT_DONE(call0) + OPEN_DONE("GELU_MUL", tiles) if needed

Signals: DH1 ready (contains gelu’ temporarily)

Gates next: k=106

BWD k=106 ELEM_MUL (DG, DH1(gelu’) → DH1(dH1))

Prereqs: DG ready, DH1 ready

Internal: OPEN_DONE("ELEM_MUL", tiles)

Signals: DH1 ready (now true dH1)

Gates next: k=107,k=108,k=109

BWD k=107 DB1 = reduce(DH1)

Prereqs: DH1 ready

Internal: NCCL_DONE if TP reduce

Signals: DB1_G ready

BWD k=108 DW1 = XN2^T * DH1

Prereqs: XN2 ready (FWD k=14), DH1 ready, triples

Internal: OPEN(E/F) + NCCL_DONE if TP allreduce for grads

Signals: DW1 ready

BWD k=109 DXN2 = DH1 * W1^T

Prereqs: DH1 ready, W1 ready, triples

Internal: OPEN(E/F)

Signals: DXN2 ready

Gates next: k=110

BWD k=110 RMSNORM2_BWD

Prereqs: X1 ready (FWD k=13), INV_RMS2 ready (FWD k=14), DXN2 ready, GAMMA2 ready

Internal: OPEN_DONE("RMSBWD_MUL", tiles) + NCCL_DONE for reductions if any

Signals: DX1 ready (accumulated), DGAMMA2 ready

Gates next: k=111

BWD k=111 RESIDUAL1_SPLIT (DX1 → DX_IN, DY)

Prereqs: DX1 ready

Signals: DX_IN ready, DY ready

Gates next: k=112,k=113,k=114 and later k=125 accumulates into DX_IN

BWD k=112 DBO = reduce(DY)

prereqs: DY ready; internal NCCL maybe; signals DB_O

BWD k=113 DW_O = O^T * DY

prereqs: O ready (FWD k=9), DY ready, triples; internal OPEN + NCCL if TP grad allreduce; signals DW_O

BWD k=114 DO = DY * W_O^T

prereqs: DY, W_O; internal OPEN; signals DO

gates next: k=115,k=116

BWD k=115 DP = DO * V^T

prereqs: DO ready, V ready (FWD k=5); internal OPEN; signals DP

gates next: k=117

BWD k=116 DV = P^T * DO

prereqs: P ready (FWD k=8), DO ready; internal OPEN; signals DV

gates next: k=121 (merge)

BWD k=117 SOFTMAX_BWD (P,DP → DSCALED)

prereqs: P ready, DP ready

internal: OPEN_DONE("SOFTBWD_MUL", tiles) (because secret multiplies)

signals: DSCALED ready

gates next: k=118

BWD k=118 SCALE_PUBLIC (DSCALED → DS)

prereqs: DSCALED ready

signals: DS ready

gates next: k=119,k=120

BWD k=119 DQ = DS * K

prereqs: DS ready, K ready (FWD k=5), triples

internal: OPEN(E/F)

signals: DQ ready

gates next: k=121

BWD k=120 DK = DS^T * Q

prereqs: DS ready, Q ready, triples

internal: OPEN(E/F)

signals: DK ready

gates next: k=121

BWD k=121 MERGE_DTQKV (DQ,DK,DV → DTQKV)

prereqs: DQ, DK, DV ready

signals: DTQKV ready

gates next: k=122,k=123,k=124

BWD k=122 DB_QKV = reduce(DTQKV)

prereqs: DTQKV ready; internal NCCL maybe; signals DB_QKV

BWD k=123 DW_QKV = XN1^T * DTQKV

prereqs: XN1 ready (FWD k=1), DTQKV ready, triples

internal OPEN + NCCL if TP allreduce for grads

signals: DW_QKV ready

BWD k=124 DXN1 = DTQKV * W_QKV^T

prereqs: DTQKV ready, W_QKV ready, triples

internal OPEN

signals: DXN1 ready

gates next: k=125

BWD k=125 RMSNORM1_BWD

prereqs: X_IN ready, INV_RMS1 ready, DXN1 ready, GAMMA1 ready

internal: OPEN_DONE("RMSBWD_MUL", tiles) + NCCL_DONE as needed

signals: DX_IN ready (accumulated), DGAMMA1 ready

gates next: k=126

BWD k=126 SEND_PP DX_IN

prereqs: DX_IN ready

signals: PP send posted

C) Two implementation notes that prevent “subtle missing things”

C.1 SKS must be “non-gating” but still deterministic

No forward/backward tensor depends on SKS outputs → it should not block the pipeline.

But your step finalization (building transcript root / signing) must wait for all SKS ops that were scheduled in that step to complete (or fail).

So: SKS gates “FinalizeStep()”, not “next op”.

C.2 Composite ops must expose deterministic internal futures

For RMSNorm / Softmax / GELU:

you must fix:

exact fss_calls count

deterministic fss_call_idx order

deterministic OPEN stream tags and tile counts for their internal multiplies

That’s what makes transcript binding stable even when you overlap microbatches.

Below is exactly what you asked for:

An exact k-index table for a realistic transformer block as a pasteable StageProgram literal (forward + backward), with explicit opcodes, tensor IDs, and per-op params placeholders.

The exact dependency rules each op registers:

which tensors/slots must be ready

which PP inbox events must be present

which protocol futures (OPEN / LIFT) must complete before the op is “DONE”

which next ops are therefore gated (via tensor readiness)

I’m going to assume your runtime is like we discussed:

StageProgram is per stage (PP stage), and runs per microbatch mb.

Weights are Life::PERSIST (one per worker (r,s,t)).

Activations/grads are Life::EPOCH (per microbatch).

Each op completion marks output tensors ready; later ops only depend on “tensor ready”.

Protocol waits are modeled as futures inside the op, i.e. an op can be PENDING until its own OPEN/LIFT futures complete.

A) Pasteable StageProgram literal

This is “literal code” you can paste into your uvcc/sgir layer (names/types match the earlier scaffolding; if your structs differ, this is still the exact content you want to represent).

A.1 Tensor IDs used by this block

// transformer_block_stage_program.h

enum class TensorId : u16 {
  // ---- persistent weights/params (Life::PERSIST) ----
  W_QKV,   B_QKV,
  W_O,     B_O,
  W1,      B1,
  W2,      B2,
  GAMMA1,  // RMSNorm1 scale
  GAMMA2,  // RMSNorm2 scale

  // ---- forward activations (Life::EPOCH, per microbatch) ----
  X_IN,
  INV_RMS1,   // saved for RMSNorm1 backward
  XN1,
  TQKV,
  Q, K, V,
  S, SSCALED,
  P,
  O,
  Y,
  X1,
  INV_RMS2,   // saved for RMSNorm2 backward
  XN2,
  H, H1,
  G,
  Z, Z1,
  X_OUT,

  // ---- backward grads (Life::EPOCH, per microbatch) ----
  DX_OUT,
  DX1,      // gradient wrt X1 (after residual2 split + rmsnorm2)
  DZ1,
  DG,
  DH1,
  DXN2,
  DY,
  DO,
  DP,
  DV,
  DSCALED,
  DS,
  DQ,
  DK,
  DTQKV,
  DXN1,
  DX_IN,

  // ---- weight grads (Life::EPOCH, aggregated later by DP) ----
  DW_QKV, DB_QKV,
  DW_O,   DB_O,
  DW1,    DB1_G,
  DW2,    DB2_G,
  DGAMMA1,
  DGAMMA2,
};
A.2 Op codes + param variants

enum class Phase : u8 { FWD=0, BWD=1, UPD=2 };

enum class OpCode : u16 {
  RECV_PP,
  SEND_PP,

  RMSNORM_FWD,   // produces (Xn, inv_rms) so backward can be arithmetic-only
  RMSNORM_BWD,   // consumes inv_rms + X_in + dXn -> dX + dGamma

  GEMM_SS,       // secure GEMM: internally does OPEN(E/F) per tile
  SKS_GEMM_CHECK, // optional; produces no tensors used by mainline

  BIAS_ADD,
  RESIDUAL_ADD,
  RESIDUAL_SPLIT, // for backprop through residual

  VIEW_SPLIT_QKV, // TQKV -> Q,K,V (views)
  VIEW_MERGE_DQKV, // DQ,DK,DV -> DTQKV (view/concat)

  SCALE_PUBLIC,   // multiply by public scalar

  SOFTMAX_APPROX_FWD, // uses FSS+LIFT internally, then secure multiplies
  SOFTMAX_BWD,        // arithmetic-only from P and dP (no new FSS)
  GELU_APPROX_FWD,    // uses FSS+LIFT internally, then secure multiplies
  GELU_DERIV_APPROX,  // uses FSS+LIFT internally to get gelu'(H1)
  ELEM_MUL,           // elementwise multiply (secret*secret) => internal OPENs

  REDUCE_BIAS_GRAD,   // sum over batch/token dims (linear)
};

struct GemmParams {
  // minimal knobs; fill with your real layout/tiling/TP flags
  u32 m, n, k;
  bool transA;
  bool transB;

  // TP strategy hints (implementation can ignore if fixed)
  bool tp_allgather_A;
  bool tp_allgather_B;
  bool tp_allreduce_C;

  // tiling for “how many OPEN(E/F) futures exist”
  u32 tile_count; // number of tiles this op breaks into (deterministic)
};

struct RMSNormParams {
  u32 q_count;        // number of elements in vector
  float eps;
  bool use_fss_rsqrt; // if true, inv_rms produced via FSS+LIFT inside op
  u16 fss_calls;      // typically 1
};

struct NonlinearParams {
  u32 q_count;
  u16 fss_calls; // how many FSS calls this composite issues
};

struct ScaleParams {
  float scalar;
};

struct SKSParams {
  bool enabled;
  u32  shard_idx; // for deterministic stream tags if you want
};

struct Op {
  u16   k;
  Phase phase;
  OpCode code;

  // tensors are symbolic; runtime maps (mb,tensor)->Slot*
  std::vector<TensorId> in;
  std::vector<TensorId> out;

  // exactly one of these is used depending on opcode
  // (use std::variant in real code; kept simple here)
  GemmParams gemm;
  RMSNormParams rms;
  NonlinearParams nonlin;
  ScaleParams scale;
  SKSParams sks;

  const char* name;
};

struct StageProgram {
  u16 stage_id;
  std::vector<Op> ops_fwd;
  std::vector<Op> ops_bwd;
};
A.3 The exact k-index program

This is the exact forward/backward k table as a literal StageProgram.

// MakeTransformerBlockStageProgram(...) - pasteable literal
static StageProgram MakeTransformerBlockStageProgram(
  u16 stage_id,
  u32 q_count,              // elements in activation vectors for this stage/tp shard
  u32 tile_count_gemm,       // deterministic tiling count for GEMMs in this stage
  bool enable_sks
) {
  StageProgram p;
  p.stage_id = stage_id;

  // -------------------------
  // Forward (Phase::FWD)
  // -------------------------
  p.ops_fwd = {

    // k=0: receive X_in from previous PP stage (or input feeder for stage 0)
    Op{
      .k=0, .phase=Phase::FWD, .code=OpCode::RECV_PP,
      .in={}, .out={TensorId::X_IN},
      .name="FWD_RECV_X_IN"
    },

    // k=1: RMSNorm1: X_in -> (XN1, INV_RMS1)
    Op{
      .k=1, .phase=Phase::FWD, .code=OpCode::RMSNORM_FWD,
      .in={TensorId::X_IN, TensorId::GAMMA1},
      .out={TensorId::XN1, TensorId::INV_RMS1},
      .rms=RMSNormParams{ .q_count=q_count, .eps=1e-5f, .use_fss_rsqrt=true, .fss_calls=1 },
      .name="FWD_RMSNORM1"
    },

    // k=2: GEMM QKV: XN1 * W_QKV -> TQKV
    Op{
      .k=2, .phase=Phase::FWD, .code=OpCode::GEMM_SS,
      .in={TensorId::XN1, TensorId::W_QKV},
      .out={TensorId::TQKV},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=false, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="FWD_GEMM_QKV"
    },

    // k=3: optional SKS check of QKV GEMM (does NOT gate next ops; see deps section)
    Op{
      .k=3, .phase=Phase::FWD, .code=OpCode::SKS_GEMM_CHECK,
      .in={TensorId::XN1, TensorId::W_QKV, TensorId::TQKV},
      .out={},
      .sks=SKSParams{ .enabled=enable_sks, .shard_idx=0 },
      .name="FWD_SKS_QKV"
    },

    // k=4: bias add for QKV
    Op{
      .k=4, .phase=Phase::FWD, .code=OpCode::BIAS_ADD,
      .in={TensorId::TQKV, TensorId::B_QKV},
      .out={TensorId::TQKV}, // in-place OK
      .name="FWD_BIAS_QKV"
    },

    // k=5: view split TQKV -> Q,K,V
    Op{
      .k=5, .phase=Phase::FWD, .code=OpCode::VIEW_SPLIT_QKV,
      .in={TensorId::TQKV},
      .out={TensorId::Q, TensorId::K, TensorId::V},
      .name="FWD_SPLIT_QKV"
    },

    // k=6: GEMM scores: S = Q * K^T
    Op{
      .k=6, .phase=Phase::FWD, .code=OpCode::GEMM_SS,
      .in={TensorId::Q, TensorId::K},
      .out={TensorId::S},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=true,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="FWD_GEMM_QK_T"
    },

    // k=7: scale scores by public scalar
    Op{
      .k=7, .phase=Phase::FWD, .code=OpCode::SCALE_PUBLIC,
      .in={TensorId::S},
      .out={TensorId::SSCALED},
      .scale=ScaleParams{ .scalar=1.0f }, // fill with 1/sqrt(d)
      .name="FWD_SCALE_SCORES"
    },

    // k=8: softmax approx: SSCALED -> P
    Op{
      .k=8, .phase=Phase::FWD, .code=OpCode::SOFTMAX_APPROX_FWD,
      .in={TensorId::SSCALED},
      .out={TensorId::P},
      .nonlin=NonlinearParams{ .q_count=q_count, .fss_calls=2 }, // exp + invsum
      .name="FWD_SOFTMAX_APPROX"
    },

    // k=9: GEMM weighted sum: O = P * V
    Op{
      .k=9, .phase=Phase::FWD, .code=OpCode::GEMM_SS,
      .in={TensorId::P, TensorId::V},
      .out={TensorId::O},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="FWD_GEMM_PV"
    },

    // k=10: GEMM output projection: Y = O * W_O
    Op{
      .k=10, .phase=Phase::FWD, .code=OpCode::GEMM_SS,
      .in={TensorId::O, TensorId::W_O},
      .out={TensorId::Y},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=false, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="FWD_GEMM_O_PROJ"
    },

    // k=11: optional SKS for output projection
    Op{
      .k=11, .phase=Phase::FWD, .code=OpCode::SKS_GEMM_CHECK,
      .in={TensorId::O, TensorId::W_O, TensorId::Y},
      .out={},
      .sks=SKSParams{ .enabled=enable_sks, .shard_idx=1 },
      .name="FWD_SKS_O_PROJ"
    },

    // k=12: add output proj bias
    Op{
      .k=12, .phase=Phase::FWD, .code=OpCode::BIAS_ADD,
      .in={TensorId::Y, TensorId::B_O},
      .out={TensorId::Y}, // in-place
      .name="FWD_BIAS_O"
    },

    // k=13: residual1: X1 = X_IN + Y
    Op{
      .k=13, .phase=Phase::FWD, .code=OpCode::RESIDUAL_ADD,
      .in={TensorId::X_IN, TensorId::Y},
      .out={TensorId::X1},
      .name="FWD_RESIDUAL1"
    },

    // k=14: RMSNorm2: X1 -> (XN2, INV_RMS2)
    Op{
      .k=14, .phase=Phase::FWD, .code=OpCode::RMSNORM_FWD,
      .in={TensorId::X1, TensorId::GAMMA2},
      .out={TensorId::XN2, TensorId::INV_RMS2},
      .rms=RMSNormParams{ .q_count=q_count, .eps=1e-5f, .use_fss_rsqrt=true, .fss_calls=1 },
      .name="FWD_RMSNORM2"
    },

    // k=15: GEMM MLP1: H = XN2 * W1
    Op{
      .k=15, .phase=Phase::FWD, .code=OpCode::GEMM_SS,
      .in={TensorId::XN2, TensorId::W1},
      .out={TensorId::H},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=false, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="FWD_GEMM_MLP1"
    },

    // k=16: optional SKS MLP1
    Op{
      .k=16, .phase=Phase::FWD, .code=OpCode::SKS_GEMM_CHECK,
      .in={TensorId::XN2, TensorId::W1, TensorId::H},
      .out={},
      .sks=SKSParams{ .enabled=enable_sks, .shard_idx=2 },
      .name="FWD_SKS_MLP1"
    },

    // k=17: bias add MLP1: H1 = H + B1
    Op{
      .k=17, .phase=Phase::FWD, .code=OpCode::BIAS_ADD,
      .in={TensorId::H, TensorId::B1},
      .out={TensorId::H1},
      .name="FWD_BIAS_MLP1"
    },

    // k=18: GELU approx: H1 -> G
    Op{
      .k=18, .phase=Phase::FWD, .code=OpCode::GELU_APPROX_FWD,
      .in={TensorId::H1},
      .out={TensorId::G},
      .nonlin=NonlinearParams{ .q_count=q_count, .fss_calls=1 },
      .name="FWD_GELU_APPROX"
    },

    // k=19: GEMM MLP2: Z = G * W2
    Op{
      .k=19, .phase=Phase::FWD, .code=OpCode::GEMM_SS,
      .in={TensorId::G, TensorId::W2},
      .out={TensorId::Z},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=false, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="FWD_GEMM_MLP2"
    },

    // k=20: optional SKS MLP2
    Op{
      .k=20, .phase=Phase::FWD, .code=OpCode::SKS_GEMM_CHECK,
      .in={TensorId::G, TensorId::W2, TensorId::Z},
      .out={},
      .sks=SKSParams{ .enabled=enable_sks, .shard_idx=3 },
      .name="FWD_SKS_MLP2"
    },

    // k=21: bias add MLP2: Z1 = Z + B2
    Op{
      .k=21, .phase=Phase::FWD, .code=OpCode::BIAS_ADD,
      .in={TensorId::Z, TensorId::B2},
      .out={TensorId::Z1},
      .name="FWD_BIAS_MLP2"
    },

    // k=22: residual2: X_OUT = X1 + Z1
    Op{
      .k=22, .phase=Phase::FWD, .code=OpCode::RESIDUAL_ADD,
      .in={TensorId::X1, TensorId::Z1},
      .out={TensorId::X_OUT},
      .name="FWD_RESIDUAL2"
    },

    // k=23: send X_OUT to next PP stage
    Op{
      .k=23, .phase=Phase::FWD, .code=OpCode::SEND_PP,
      .in={TensorId::X_OUT}, .out={},
      .name="FWD_SEND_X_OUT"
    },
  };

  // -------------------------
  // Backward (Phase::BWD)
  // -------------------------
  p.ops_bwd = {

    // k=100: receive dX_out from next PP stage (or loss grad feeder if last stage)
    Op{
      .k=100, .phase=Phase::BWD, .code=OpCode::RECV_PP,
      .in={}, .out={TensorId::DX_OUT},
      .name="BWD_RECV_DX_OUT"
    },

    // k=101: residual2 split: X_OUT = X1 + Z1
    // so dX1 += dX_out ; dZ1 += dX_out
    Op{
      .k=101, .phase=Phase::BWD, .code=OpCode::RESIDUAL_SPLIT,
      .in={TensorId::DX_OUT},
      .out={TensorId::DX1, TensorId::DZ1},
      .name="BWD_RESIDUAL2_SPLIT"
    },

    // k=102: db2 = reduce(dZ1)
    Op{
      .k=102, .phase=Phase::BWD, .code=OpCode::REDUCE_BIAS_GRAD,
      .in={TensorId::DZ1},
      .out={TensorId::DB2_G},
      .name="BWD_DB2"
    },

    // k=103: dW2 = G^T * dZ1
    Op{
      .k=103, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::G, TensorId::DZ1},
      .out={TensorId::DW2},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=true,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=true,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DW2"
    },

    // k=104: dG = dZ1 * W2^T
    Op{
      .k=104, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::DZ1, TensorId::W2},
      .out={TensorId::DG},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=true,
        .tp_allgather_A=true, .tp_allgather_B=false, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DG"
    },

    // k=105: gelu' approx from H1 (uses FSS+LIFT internally)
    Op{
      .k=105, .phase=Phase::BWD, .code=OpCode::GELU_DERIV_APPROX,
      .in={TensorId::H1},
      .out={TensorId::DH1}, // we’ll overwrite DH1 later after mul; this holds gelu'(H1) for now
      .nonlin=NonlinearParams{ .q_count=q_count, .fss_calls=1 },
      .name="BWD_GELU_DERIV"
    },

    // k=106: dH1 = dG ⊙ gelu'(H1)
    Op{
      .k=106, .phase=Phase::BWD, .code=OpCode::ELEM_MUL,
      .in={TensorId::DG, TensorId::DH1}, // DH1 currently stores gelu'(H1)
      .out={TensorId::DH1},              // overwrite with actual dH1
      .name="BWD_ELEM_MUL_DH1"
    },

    // k=107: db1 = reduce(dH1)
    Op{
      .k=107, .phase=Phase::BWD, .code=OpCode::REDUCE_BIAS_GRAD,
      .in={TensorId::DH1},
      .out={TensorId::DB1_G},
      .name="BWD_DB1"
    },

    // k=108: dW1 = XN2^T * dH1
    Op{
      .k=108, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::XN2, TensorId::DH1},
      .out={TensorId::DW1},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=true,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=true,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DW1"
    },

    // k=109: dXN2 = dH1 * W1^T
    Op{
      .k=109, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::DH1, TensorId::W1},
      .out={TensorId::DXN2},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=true,
        .tp_allgather_A=true, .tp_allgather_B=false, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DXN2"
    },

    // k=110: RMSNorm2 backward: (X1, inv_rms2, dXN2, gamma2) -> add into dX1, and produce dGamma2
    Op{
      .k=110, .phase=Phase::BWD, .code=OpCode::RMSNORM_BWD,
      .in={TensorId::X1, TensorId::INV_RMS2, TensorId::DXN2, TensorId::GAMMA2},
      .out={TensorId::DX1, TensorId::DGAMMA2}, // DX1 is updated/accumulated
      .rms=RMSNormParams{ .q_count=q_count, .eps=1e-5f, .use_fss_rsqrt=false, .fss_calls=0 },
      .name="BWD_RMSNORM2"
    },

    // k=111: residual1 split: X1 = X_IN + Y => dX_IN += dX1 ; dY += dX1
    Op{
      .k=111, .phase=Phase::BWD, .code=OpCode::RESIDUAL_SPLIT,
      .in={TensorId::DX1},
      .out={TensorId::DX_IN, TensorId::DY},
      .name="BWD_RESIDUAL1_SPLIT"
    },

    // k=112: dbo = reduce(dY)
    Op{
      .k=112, .phase=Phase::BWD, .code=OpCode::REDUCE_BIAS_GRAD,
      .in={TensorId::DY},
      .out={TensorId::DB_O},
      .name="BWD_DBO"
    },

    // k=113: dWo = O^T * dY
    Op{
      .k=113, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::O, TensorId::DY},
      .out={TensorId::DW_O},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=true,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=true,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DWO"
    },

    // k=114: dO = dY * Wo^T
    Op{
      .k=114, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::DY, TensorId::W_O},
      .out={TensorId::DO},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=true,
        .tp_allgather_A=true, .tp_allgather_B=false, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DO"
    },

    // k=115: dP = dO * V^T
    Op{
      .k=115, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::DO, TensorId::V},
      .out={TensorId::DP},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=true,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DP"
    },

    // k=116: dV = P^T * dO
    Op{
      .k=116, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::P, TensorId::DO},
      .out={TensorId::DV},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=true,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DV"
    },

    // k=117: softmax backward (arithmetic-only): (P, dP) -> dSscaled
    Op{
      .k=117, .phase=Phase::BWD, .code=OpCode::SOFTMAX_BWD,
      .in={TensorId::P, TensorId::DP},
      .out={TensorId::DSCALED},
      .name="BWD_SOFTMAX_BWD"
    },

    // k=118: scale backward: dS = dSscaled * (1/sqrt(d)) (public)
    Op{
      .k=118, .phase=Phase::BWD, .code=OpCode::SCALE_PUBLIC,
      .in={TensorId::DSCALED},
      .out={TensorId::DS},
      .scale=ScaleParams{ .scalar=1.0f }, // same scalar as forward
      .name="BWD_SCALE"
    },

    // k=119: dQ = dS * K
    Op{
      .k=119, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::DS, TensorId::K},
      .out={TensorId::DQ},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DQ"
    },

    // k=120: dK = dS^T * Q
    Op{
      .k=120, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::DS, TensorId::Q},
      .out={TensorId::DK},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=true,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DK"
    },

    // k=121: merge (DQ,DK,DV) -> DTQKV (view/concat)
    Op{
      .k=121, .phase=Phase::BWD, .code=OpCode::VIEW_MERGE_DQKV,
      .in={TensorId::DQ, TensorId::DK, TensorId::DV},
      .out={TensorId::DTQKV},
      .name="BWD_MERGE_DTQKV"
    },

    // k=122: dbqkv = reduce(dTQKV)
    Op{
      .k=122, .phase=Phase::BWD, .code=OpCode::REDUCE_BIAS_GRAD,
      .in={TensorId::DTQKV},
      .out={TensorId::DB_QKV},
      .name="BWD_DBQKV"
    },

    // k=123: dWqkv = XN1^T * dTQKV
    Op{
      .k=123, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::XN1, TensorId::DTQKV},
      .out={TensorId::DW_QKV},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=true,.transB=false,
        .tp_allgather_A=true, .tp_allgather_B=true, .tp_allreduce_C=true,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DWQKV"
    },

    // k=124: dXN1 = dTQKV * Wqkv^T
    Op{
      .k=124, .phase=Phase::BWD, .code=OpCode::GEMM_SS,
      .in={TensorId::DTQKV, TensorId::W_QKV},
      .out={TensorId::DXN1},
      .gemm=GemmParams{
        .m=0,.n=0,.k=0, .transA=false,.transB=true,
        .tp_allgather_A=true, .tp_allgather_B=false, .tp_allreduce_C=false,
        .tile_count=tile_count_gemm
      },
      .name="BWD_GEMM_DXN1"
    },

    // k=125: RMSNorm1 backward: (X_in, inv_rms1, dXN1, gamma1) -> add into dX_in, and produce dGamma1
    Op{
      .k=125, .phase=Phase::BWD, .code=OpCode::RMSNORM_BWD,
      .in={TensorId::X_IN, TensorId::INV_RMS1, TensorId::DXN1, TensorId::GAMMA1},
      .out={TensorId::DX_IN, TensorId::DGAMMA1}, // DX_IN accumulates
      .rms=RMSNormParams{ .q_count=q_count, .eps=1e-5f, .use_fss_rsqrt=false, .fss_calls=0 },
      .name="BWD_RMSNORM1"
    },

    // k=126: send dX_in to previous PP stage
    Op{
      .k=126, .phase=Phase::BWD, .code=OpCode::SEND_PP,
      .in={TensorId::DX_IN}, .out={},
      .name="BWD_SEND_DX_IN"
    },
  };

  return p;
}
That is the exact k-index table in a form you can paste and compile (once you wire your own slot mapping and param unions/variants).

B) Exact dependency graph rules (slots + protocol futures + PP events)

You asked: “the exact dependency graph rules each op registers (which slot/future gates which next op).”

Below is the precise rule-set I recommend implementing, and then I’ll list the per-op dependencies for this exact program.

B.1 Canonical “what is a dependency” in this runtime

Model each op as:

Prereqs (must be satisfied to start op)

Internal futures (created on start; must complete before op is DONE)

Signals (tensors marked ready when op DONE; PP sends posted, etc.)

Dependency primitives

enum class DepKind : u8 {
  TENSOR_READY,     // slot exists + filled for this (mb,tensor)
  PP_INBOX_READY,   // activation/grad arrived from prev/next stage
  TRIPLE_READY,     // Beaver triple tile(s) available
  OPEN_DONE,        // OpenEngine has result for (stream_id, round)
  LIFT_DONE,        // LiftEngine reports is_lift_done(fss_id)==true
  NCCL_DONE,        // local collective completed (tp gather/reduce)
};

struct Dep {
  DepKind kind;
  TensorId tensor;     // for TENSOR_READY/PP_INBOX_READY
  StreamId64 stream;   // for OPEN_DONE
  u16 round;
  FssId64 fss_id;      // for LIFT_DONE
  u32 tile_idx;        // for TRIPLE_READY/OPEN streams, if you want explicit tiles
};
Two important invariants for determinism

Outputs become “ready” only when the op is DONE.
So later ops don’t directly depend on OPEN/LIFT; they depend on output tensor readiness.

Protocol futures are deterministic in naming/count.
For GEMM_SS, the number of tiles and the exact stream tags must be fixed → you get stable transcript binding.

B.2 How each OpCode declares deps + internal futures

This is the exact “rule function” you implement once, and it covers the whole block.

struct OpDeps {
  std::vector<Dep> prereqs;     // to start
  std::vector<Dep> internal;    // must complete before DONE
};

// Helpers you already have:
SgirOpId32 op_id_for(const Sid32& sid_sub, u32 step, u16 mb, Phase ph, u16 k);
StreamId64 open_stream(const Sid32& sid_sub, SgirOpId32 op, const char* tag, u32 tile);
FssId64    fss_id_for(const Sid32& sid_sub, SgirOpId32 op, u16 fss_call_idx);

static OpDeps ComputeDepsForOp(
  const Sid32& sid_sub,
  u32 step_id,
  u16 mb,
  const Op& op
) {
  OpDeps d;

  // Base prereqs: every input tensor must be ready (weights are “always ready” because PERSIST)
  for (auto t : op.in) {
    d.prereqs.push_back(Dep{ .kind=DepKind::TENSOR_READY, .tensor=t });
  }

  switch (op.code) {
    case OpCode::RECV_PP: {
      // Must have PP inbox event for that output tensor
      // (RECV_PP has no inputs; it produces one output tensor)
      d.prereqs.clear();
      d.prereqs.push_back(Dep{ .kind=DepKind::PP_INBOX_READY, .tensor=op.out.at(0) });
      break;
    }

    case OpCode::SEND_PP: {
      // Must have the input tensor ready. No internal futures required to mark op DONE;
      // posting the send is enough. (Delivery is tracked by receiver’s PP_INBOX_READY.)
      break;
    }

    case OpCode::GEMM_SS: {
      // GEMM requires triples per tile, then OPEN(E/F) per tile.
      // The op is DONE only when all OPEN(E/F) streams are complete
      // AND any required TP collectives are complete.
      auto opid = op_id_for(sid_sub, step_id, mb, op.phase, op.k);

      for (u32 tile = 0; tile < op.gemm.tile_count; ++tile) {
        d.prereqs.push_back(Dep{ .kind=DepKind::TRIPLE_READY, .tile_idx=tile });

        // internal OPEN futures
        d.internal.push_back(Dep{
          .kind=DepKind::OPEN_DONE,
          .stream=open_stream(sid_sub, opid, "GEMM_E", tile),
          .round=0, .tile_idx=tile
        });
        d.internal.push_back(Dep{
          .kind=DepKind::OPEN_DONE,
          .stream=open_stream(sid_sub, opid, "GEMM_F", tile),
          .round=0, .tile_idx=tile
        });
      }

      // TP collectives (if your GEMM implementation does them asynchronously)
      if (op.gemm.tp_allgather_A) d.internal.push_back(Dep{ .kind=DepKind::NCCL_DONE });
      if (op.gemm.tp_allgather_B) d.internal.push_back(Dep{ .kind=DepKind::NCCL_DONE });
      if (op.gemm.tp_allreduce_C) d.internal.push_back(Dep{ .kind=DepKind::NCCL_DONE });

      break;
    }

    case OpCode::RMSNORM_FWD: {
      // If rsqrt uses FSS+LIFT, then inv_rms is gated by LiftDone for each FSS call.
      auto opid = op_id_for(sid_sub, step_id, mb, op.phase, op.k);
      if (op.rms.use_fss_rsqrt) {
        for (u16 i=0;i<op.rms.fss_calls;i++) {
          auto fssid = fss_id_for(sid_sub, opid, i);
          d.internal.push_back(Dep{ .kind=DepKind::LIFT_DONE, .fss_id=fssid });
        }
      }

      // RMSNorm also contains secret*secret multiplies internally (x*x, x*inv, *gamma),
      // so it will internally post OPENs. Model it like GEMM: deterministic stream tags.
      // (Exact count depends on your tiling; keep it deterministic!)
      // Example: 2 tiles for vector ops:
      for (u32 tile=0; tile<2; ++tile) {
        d.internal.push_back(Dep{
          .kind=DepKind::OPEN_DONE,
          .stream=open_stream(sid_sub, opid, "RMS_MUL", tile),
          .round=0, .tile_idx=tile
        });
      }
      break;
    }

    case OpCode::RMSNORM_BWD: {
      // We designed it to be arithmetic-only by consuming INV_RMS saved from forward.
      // Still has secret multiplies => internal OPENs, deterministic tags:
      auto opid = op_id_for(sid_sub, step_id, mb, op.phase, op.k);
      for (u32 tile=0; tile<2; ++tile) {
        d.internal.push_back(Dep{
          .kind=DepKind::OPEN_DONE,
          .stream=open_stream(sid_sub, opid, "RMSBWD_MUL", tile),
          .round=0, .tile_idx=tile
        });
      }
      // plus any TP allreduces for reductions if you do them across TP
      d.internal.push_back(Dep{ .kind=DepKind::NCCL_DONE });
      break;
    }

    case OpCode::SOFTMAX_APPROX_FWD: {
      // Forward softmax uses FSS+LIFT (exp + invsum), then secret multiplies.
      auto opid = op_id_for(sid_sub, step_id, mb, op.phase, op.k);

      for (u16 i=0;i<op.nonlin.fss_calls;i++) {
        auto fssid = fss_id_for(sid_sub, opid, i);
        d.internal.push_back(Dep{ .kind=DepKind::LIFT_DONE, .fss_id=fssid });
      }

      // secret multiplies => OPEN futures (deterministic tiling)
      for (u32 tile=0; tile<2; ++tile) {
        d.internal.push_back(Dep{
          .kind=DepKind::OPEN_DONE,
          .stream=open_stream(sid_sub, opid, "SOFT_MUL", tile),
          .round=0, .tile_idx=tile
        });
      }
      break;
    }

    case OpCode::SOFTMAX_BWD: {
      // Arithmetic-only from P and dP, but still secret multiplies => OPENs.
      auto opid = op_id_for(sid_sub, step_id, mb, op.phase, op.k);
      for (u32 tile=0; tile<2; ++tile) {
        d.internal.push_back(Dep{
          .kind=DepKind::OPEN_DONE,
          .stream=open_stream(sid_sub, opid, "SOFTBWD_MUL", tile),
          .round=0, .tile_idx=tile
        });
      }
      break;
    }

    case OpCode::GELU_APPROX_FWD:
    case OpCode::GELU_DERIV_APPROX: {
      // Uses FSS+LIFT internally then multiplies.
      auto opid = op_id_for(sid_sub, step_id, mb, op.phase, op.k);
      for (u16 i=0;i<op.nonlin.fss_calls;i++) {
        auto fssid = fss_id_for(sid_sub, opid, i);
        d.internal.push_back(Dep{ .kind=DepKind::LIFT_DONE, .fss_id=fssid });
      }
      for (u32 tile=0; tile<2; ++tile) {
        d.internal.push_back(Dep{
          .kind=DepKind::OPEN_DONE,
          .stream=open_stream(sid_sub, opid, "GELU_MUL", tile),
          .round=0, .tile_idx=tile
        });
      }
      break;
    }

    case OpCode::ELEM_MUL: {
      // secret*secret elementwise multiply => internal OPENs.
      auto opid = op_id_for(sid_sub, step_id, mb, op.phase, op.k);
      for (u32 tile=0; tile<2; ++tile) {
        d.internal.push_back(Dep{
          .kind=DepKind::OPEN_DONE,
          .stream=open_stream(sid_sub, opid, "ELEM_MUL", tile),
          .round=0, .tile_idx=tile
        });
      }
      break;
    }

    case OpCode::SKS_GEMM_CHECK: {
      // SKS has internal opens, but it produces no tensors the mainline depends on.
      // Dependency: it can start once its inputs exist. Completion is only required
      // before end-of-step finalize (audit), not before next op.
      auto opid = op_id_for(sid_sub, step_id, mb, op.phase, op.k);
      d.internal.push_back(Dep{
        .kind=DepKind::OPEN_DONE,
        .stream=open_stream(sid_sub, opid, "SKS_Z", 0),
        .round=0
      });
      break;
    }

    case OpCode::BIAS_ADD:
    case OpCode::RESIDUAL_ADD:
    case OpCode::RESIDUAL_SPLIT:
    case OpCode::VIEW_SPLIT_QKV:
    case OpCode::VIEW_MERGE_DQKV:
    case OpCode::SCALE_PUBLIC:
    case OpCode::REDUCE_BIAS_GRAD: {
      // Purely local / linear (no LIFT/OPEN), except REDUCE may do TP allreduce.
      if (op.code == OpCode::REDUCE_BIAS_GRAD) {
        d.internal.push_back(Dep{ .kind=DepKind::NCCL_DONE });
      }
      break;
    }

    default: break;
  }

  return d;
}
That is the exact rule engine. Now I’ll apply it to the exact program so you see the concrete “which gates which”.

B.3 Concrete dependencies for every op in the program

This is the “expanded view” you can use to verify you didn’t miss anything.

Forward dependencies (mb fixed)

FWD k=0 RECV_PP X_IN

Prereqs: PP_INBOX_READY(X_IN)

Internal futures: none

Signals: TENSOR_READY(X_IN)

Gates next: k=1

FWD k=1 RMSNORM1_FWD (X_IN,GAMMA1 → XN1,INV_RMS1)

Prereqs: TENSOR_READY(X_IN), TENSOR_READY(GAMMA1)

Internal futures:

If rsqrt via FSS+LIFT: LIFT_DONE(fss_id(op=1,call=0))

internal OPENs: OPEN_DONE(stream "RMS_MUL", tile=0..1) (exact count must be fixed)

Signals: TENSOR_READY(XN1), TENSOR_READY(INV_RMS1)

Gates next: k=2, and later k=125 uses X_IN+INV_RMS1

FWD k=2 GEMM_QKV (XN1,W_QKV → TQKV)

Prereqs: TENSOR_READY(XN1), TENSOR_READY(W_QKV), plus TRIPLE_READY(tile=0..tile_count-1)

Internal futures: for each tile:

OPEN_DONE("GEMM_E", tile)

OPEN_DONE("GEMM_F", tile)

plus any async TP collectives if you expose them as NCCL futures

Signals: TENSOR_READY(TQKV)

Gates next: k=4, k=5 (and k=3 SKS can start too)

FWD k=3 SKS_QKV (optional)

Prereqs: TENSOR_READY(XN1), TENSOR_READY(W_QKV), TENSOR_READY(TQKV)

Internal futures: OPEN_DONE("SKS_Z",0) (and any SKS internal opens you model)

Signals: none needed by mainline

Gates next: none (only “end-of-step finalize” must wait for SKS completion if enabled)

FWD k=4 BIAS_QKV

Prereqs: TENSOR_READY(TQKV), TENSOR_READY(B_QKV)

Internal futures: none

Signals: TENSOR_READY(TQKV) (in-place)

Gates next: k=5

FWD k=5 SPLIT_QKV (TQKV → Q,K,V)

Prereqs: TENSOR_READY(TQKV)

Signals: TENSOR_READY(Q), TENSOR_READY(K), TENSOR_READY(V)

Gates next: k=6, k=9, backward k=115/116/119/120 later

FWD k=6 GEMM_QK_T (Q,K → S)

Prereqs: TENSOR_READY(Q), TENSOR_READY(K), triples

Internal futures: OPEN(E/F) per tile

Signals: TENSOR_READY(S)

Gates next: k=7

FWD k=7 SCALE_PUBLIC (S → SSCALED)

Prereqs: TENSOR_READY(S)

Signals: TENSOR_READY(SSCALED)

Gates next: k=8

FWD k=8 SOFTMAX_APPROX_FWD (SSCALED → P)

Prereqs: TENSOR_READY(SSCALED)

Internal futures:

LIFT_DONE(fss_id(call=0)) // exp approx

LIFT_DONE(fss_id(call=1)) // invsum approx

internal OPENs for secret multiplies: OPEN_DONE("SOFT_MUL", tile=0..1)

Signals: TENSOR_READY(P)

Gates next: k=9 and backward k=116/117 later

FWD k=9 GEMM_PV (P,V → O)

Prereqs: TENSOR_READY(P), TENSOR_READY(V), triples

Internal: OPEN(E/F) per tile

Signals: TENSOR_READY(O)

Gates next: k=10, backward k=113/115/116

FWD k=10 GEMM_O_PROJ (O,W_O → Y)

Prereqs: TENSOR_READY(O), TENSOR_READY(W_O), triples

Internal: OPEN(E/F) per tile

Signals: TENSOR_READY(Y)

Gates next: k=12, k=13, and SKS k=11

FWD k=11 SKS_O_PROJ (optional)

Prereqs: TENSOR_READY(O), TENSOR_READY(W_O), TENSOR_READY(Y)

Internal: OPEN_DONE("SKS_Z",0)

Signals: none

Gates next: none

FWD k=12 BIAS_O

Prereqs: TENSOR_READY(Y), TENSOR_READY(B_O)

Signals: TENSOR_READY(Y) (in-place)

Gates next: k=13

FWD k=13 RESIDUAL1 (X_IN,Y → X1)

Prereqs: TENSOR_READY(X_IN), TENSOR_READY(Y)

Signals: TENSOR_READY(X1)

Gates next: k=14 and backward k=110/111

FWD k=14 RMSNORM2_FWD (X1,GAMMA2 → XN2,INV_RMS2)

Prereqs: TENSOR_READY(X1), TENSOR_READY(GAMMA2)

Internal: LIFT_DONE(call0) if FSS rsqrt; OPEN_DONE("RMS_MUL", tiles)

Signals: TENSOR_READY(XN2), TENSOR_READY(INV_RMS2)

Gates next: k=15, backward k=110

FWD k=15 GEMM_MLP1 (XN2,W1 → H)

Prereqs: TENSOR_READY(XN2), TENSOR_READY(W1), triples

Internal: OPEN(E/F) per tile

Signals: TENSOR_READY(H)

Gates next: k=17, SKS k=16

FWD k=16 SKS_MLP1

Prereqs: XN2,W1,H ready

Internal: OPEN_DONE("SKS_Z")

Signals: none

FWD k=17 BIAS_MLP1 (H,B1 → H1)

Prereqs: H ready, B1 ready

Signals: TENSOR_READY(H1)

Gates next: k=18 and backward k=105

FWD k=18 GELU_APPROX_FWD (H1 → G)

Prereqs: H1 ready

Internal: LIFT_DONE(call0) + OPEN_DONE("GELU_MUL", tile=0..1)

Signals: G ready

Gates next: k=19 and backward k=103

FWD k=19 GEMM_MLP2 (G,W2 → Z)

Prereqs: G,W2 ready, triples

Internal: OPEN(E/F)

Signals: Z ready

Gates next: k=21, SKS k=20, backward k=104

FWD k=20 SKS_MLP2

Prereqs: G,W2,Z ready

Internal: OPEN_DONE("SKS_Z")

Signals: none

FWD k=21 BIAS_MLP2 (Z,B2 → Z1)

Prereqs: Z,B2 ready

Signals: Z1 ready

Gates next: k=22 and backward k=101/102

FWD k=22 RESIDUAL2 (X1,Z1 → X_OUT)

Prereqs: X1,Z1 ready

Signals: X_OUT ready

Gates next: k=23 and backward k=100 depends on DX_OUT, not X_OUT

FWD k=23 SEND_PP X_OUT

Prereqs: X_OUT ready

Signals: posts PP send event (receiver will get PP_INBOX_READY(X_OUT))

Gates next: none inside this stage

Backward dependencies (same mb)

BWD k=100 RECV_PP DX_OUT

Prereqs: PP_INBOX_READY(DX_OUT)

Signals: DX_OUT ready

Gates next: k=101

BWD k=101 RESIDUAL2_SPLIT (DX_OUT → DX1,DZ1)

Prereqs: DX_OUT ready

Signals: DX1 ready, DZ1 ready

Gates next: k=102,k=103,k=104 and k=110 consumes DX1 later

BWD k=102 DB2 = reduce(DZ1)

Prereqs: DZ1 ready

Internal: NCCL_DONE if reduced across TP

Signals: DB2_G ready

Gates next: none (optimizer/update later)

BWD k=103 DW2 = G^T * DZ1

Prereqs: G ready (from FWD k=18), DZ1 ready, triples

Internal: OPEN(E/F) per tile, NCCL_DONE if TP allreduce for grad shard

Signals: DW2 ready

BWD k=104 DG = DZ1 * W2^T

Prereqs: DZ1 ready, W2 ready, triples

Internal: OPEN(E/F)

Signals: DG ready

Gates next: k=105, k=106

BWD k=105 GELU_DERIV (H1 → DH1=gelu’)

Prereqs: H1 ready (FWD k=17)

Internal: LIFT_DONE(call0) + OPEN_DONE("GELU_MUL", tiles) if needed

Signals: DH1 ready (contains gelu’ temporarily)

Gates next: k=106

BWD k=106 ELEM_MUL (DG, DH1(gelu’) → DH1(dH1))

Prereqs: DG ready, DH1 ready

Internal: OPEN_DONE("ELEM_MUL", tiles)

Signals: DH1 ready (now true dH1)

Gates next: k=107,k=108,k=109

BWD k=107 DB1 = reduce(DH1)

Prereqs: DH1 ready

Internal: NCCL_DONE if TP reduce

Signals: DB1_G ready

BWD k=108 DW1 = XN2^T * DH1

Prereqs: XN2 ready (FWD k=14), DH1 ready, triples

Internal: OPEN(E/F) + NCCL_DONE if TP allreduce for grads

Signals: DW1 ready

BWD k=109 DXN2 = DH1 * W1^T

Prereqs: DH1 ready, W1 ready, triples

Internal: OPEN(E/F)

Signals: DXN2 ready

Gates next: k=110

BWD k=110 RMSNORM2_BWD

Prereqs: X1 ready (FWD k=13), INV_RMS2 ready (FWD k=14), DXN2 ready, GAMMA2 ready

Internal: OPEN_DONE("RMSBWD_MUL", tiles) + NCCL_DONE for reductions if any

Signals: DX1 ready (accumulated), DGAMMA2 ready

Gates next: k=111

BWD k=111 RESIDUAL1_SPLIT (DX1 → DX_IN, DY)

Prereqs: DX1 ready

Signals: DX_IN ready, DY ready

Gates next: k=112,k=113,k=114 and later k=125 accumulates into DX_IN

BWD k=112 DBO = reduce(DY)

prereqs: DY ready; internal NCCL maybe; signals DB_O

BWD k=113 DW_O = O^T * DY

prereqs: O ready (FWD k=9), DY ready, triples; internal OPEN + NCCL if TP grad allreduce; signals DW_O

BWD k=114 DO = DY * W_O^T

prereqs: DY, W_O; internal OPEN; signals DO

gates next: k=115,k=116

BWD k=115 DP = DO * V^T

prereqs: DO ready, V ready (FWD k=5); internal OPEN; signals DP

gates next: k=117

BWD k=116 DV = P^T * DO

prereqs: P ready (FWD k=8), DO ready; internal OPEN; signals DV

gates next: k=121 (merge)

BWD k=117 SOFTMAX_BWD (P,DP → DSCALED)

prereqs: P ready, DP ready

internal: OPEN_DONE("SOFTBWD_MUL", tiles) (because secret multiplies)

signals: DSCALED ready

gates next: k=118

BWD k=118 SCALE_PUBLIC (DSCALED → DS)

prereqs: DSCALED ready

signals: DS ready

gates next: k=119,k=120

BWD k=119 DQ = DS * K

prereqs: DS ready, K ready (FWD k=5), triples

internal: OPEN(E/F)

signals: DQ ready

gates next: k=121

BWD k=120 DK = DS^T * Q

prereqs: DS ready, Q ready, triples

internal: OPEN(E/F)

signals: DK ready

gates next: k=121

BWD k=121 MERGE_DTQKV (DQ,DK,DV → DTQKV)

prereqs: DQ, DK, DV ready

signals: DTQKV ready

gates next: k=122,k=123,k=124

BWD k=122 DB_QKV = reduce(DTQKV)

prereqs: DTQKV ready; internal NCCL maybe; signals DB_QKV

BWD k=123 DW_QKV = XN1^T * DTQKV

prereqs: XN1 ready (FWD k=1), DTQKV ready, triples

internal OPEN + NCCL if TP allreduce for grads

signals: DW_QKV ready

BWD k=124 DXN1 = DTQKV * W_QKV^T

prereqs: DTQKV ready, W_QKV ready, triples

internal OPEN

signals: DXN1 ready

gates next: k=125

BWD k=125 RMSNORM1_BWD

prereqs: X_IN ready, INV_RMS1 ready, DXN1 ready, GAMMA1 ready

internal: OPEN_DONE("RMSBWD_MUL", tiles) + NCCL_DONE as needed

signals: DX_IN ready (accumulated), DGAMMA1 ready

gates next: k=126

BWD k=126 SEND_PP DX_IN

prereqs: DX_IN ready

signals: PP send posted

C) Two implementation notes that prevent “subtle missing things”

C.1 SKS must be “non-gating” but still deterministic

No forward/backward tensor depends on SKS outputs → it should not block the pipeline.

But your step finalization (building transcript root / signing) must wait for all SKS ops that were scheduled in that step to complete (or fail).

So: SKS gates “FinalizeStep()”, not “next op”.

C.2 Composite ops must expose deterministic internal futures

For RMSNorm / Softmax / GELU:

you must fix:

exact fss_calls count

deterministic fss_call_idx order

deterministic OPEN stream tags and tile counts for their internal multiplies

That’s what makes transcript binding stable even when you overlap microbatches.

Here’s exactly what I’m going to give you (all in this message):

A concrete alias / in-place policy (so we agree what “a tensor” means vs “a buffer”).

A liveness table for one microbatch for the transformer-block StageProgram we pinned (FWD k=0..23, BWD k=100..126):

which tensors must persist until backward (checkpoint set)

which can be freed early (forward ephemerals + backward ephemerals)

the exact last-use k and the exact free-after point

which arena: LIFE_PERSIST, LIFE_EPOCH, LIFE_STEP, plus one special case for inflight async scratch

A weight-grad accumulation plan (so you don’t allocate per-microbatch dW buffers).

The exact runtime release strategy (refcount / last-use) so you don’t leak arenas under PP (microbatches in flight).

Everything below is for one stage worker (p,r,s,t) running one microbatch mb under the StageProgram I gave you.

1) Physical buffers vs logical tensors (alias policy)

To keep memory sane, you need to distinguish:

Logical TensorId (what the IR refers to)

Physical buffer/slot (what actually owns GPU memory for two RSS components)

I’m going to give you a safe, implementable baseline alias policy that does not create correctness hazards with async protocols as long as you follow the two rules after this list.

1.1 Alias groups (recommended baseline)

Group A: QKV backing + views

Physical buffer: BUF_QKV (allocated once)

Logical TensorIds sharing it:

TQKV owns memory

Q, K, V are views into TQKV (metadata only; no allocation)

Why: avoids extra copies; keeps exact deterministic layouts.

Group B: Attention scratch becomes P

Physical buffer: BUF_ATTN

Logical TensorIds sharing it:

S and SSCALED (scale in-place)

P (softmax writes in-place over SSCALED)

If you can’t do in-place softmax, then allocate separate BUF_P and treat BUF_ATTN freed earlier. (I’ll mark that as an option.)

Group C: H → H1 in-place

Physical buffer: BUF_H1

Logical TensorIds sharing it:

H and H1 (bias add in-place)

Group D: Z → Z1 → X_OUT in-place

Physical buffer: BUF_ZOUT

Logical TensorIds sharing it:

Z, Z1, X_OUT (bias then residual writes in-place into Z buffer; then send)

Group E: Y becomes X1 (optional but recommended)

Physical buffer: BUF_X1

Logical TensorIds sharing it:

Y and then X1 (residual writes in-place into Y buffer)

This saves one activation-sized buffer.

1.2 Two rules you must enforce for safety

Rule R1: “No op may overwrite a physical buffer until all consumers have started reading it”

This matters mainly for SKS and any “side computations” you might schedule out-of-order.

Rule R2 (the key trick): implement SKS as START + WAIT

To keep SKS non-gating and allow in-place reuse:

SKS_START (reads tensors, computes projections, emits OPEN sends)
→ completes quickly and releases inputs

SKS_WAIT (waits for OPEN(z) result and verifies)
→ has no tensor inputs, and only gates FinalizeStep()

With that, BUF_X1 aliasing is safe even if SKS is enabled.

If you implement SKS as a single op that can yield before reading inputs, you will eventually corrupt memory under PP.

2) What “LIFE_STEP / LIFE_EPOCH / LIFE_PERSIST” mean here

I’m assuming the convention we already discussed:

LIFE_PERSIST: weights, optimizer state, long-lived tables (across steps)

LIFE_EPOCH: one training step (covers all microbatches in that step). This is the right scope for:

forward checkpoints that must survive until backward for that microbatch

gradient accumulators (one per weight shard per step)

inflight async protocol states that may outlive a single GPU kernel

LIFE_STEP: pure scratch that never survives a yield
(only safe if the op is guaranteed not to suspend while holding it)

Because your runtime is async (OPEN/LIFT waits), you also need:

LIFE_INFLIGHT (practically implemented as LIFE_EPOCH pool) for op-local scratch that must survive until a future completes.

3) Liveness table: forward checkpoints (must persist until backward)

This is the heart of what you asked for.

Legend

Alloc at: first op that produces/receives it

Last use: last op that reads it (often in backward)

Free after: the moment you can return the physical buffer to your epoch pool

Arena: what pool it should come from

Size: described symbolically; you’ll plug your shapes

Assume:

activation shard shape: X = [Tok_mb, H_local]

RSS storage per tensor per party = 2 components

bytes per element = 8 (u64 ring)

so bytes(X) = 2 * Tok_mb * H_local * 8

3.1 Checkpoint buffers (per microbatch)

(A) BUF_XIN : X_IN

Physical buffer	Logical tensor(s)	Alloc at	Last use	Free after	Arena	Notes
BUF_XIN	X_IN	FWD k=0 (RECV_PP)	BWD k=125(RMSNORM1_BWD)	after BWD k=125 completes	LIFE_EPOCH	Must persist: used by RMSNorm1 backward.
(B) BUF_INV1 : INV_RMS1

Physical buffer	Logical tensor(s)	Alloc at	Last use	Free after	Arena	Notes
BUF_INV1	INV_RMS1	FWD k=1	BWD k=125	after BWD k=125	LIFE_EPOCH	Save inv_rms so RMSNorm1_BWD is arithmetic-only (no new FSS).
(C) BUF_XN1 : XN1

Physical buffer	Logical tensor(s)	Alloc at	Last use	Free after	Arena	Notes
BUF_XN1	XN1	FWD k=1	BWD k=123 (DW_QKV GEMM)	after BWD k=123	LIFE_EPOCH	Required for dWqkv.
(D) BUF_QKV : TQKV backing + Q/K/V views

Physical buffer	Logical tensor(s)	Alloc at	Last use	Free after	Arena	Notes
BUF_QKV	TQKV (owns), Q,K,V (views)	FWD k=2	BWD k=120 (DK GEMM uses Q)	after BWD k=120	LIFE_EPOCH	Q/K/V must exist for attention backward. Using views means TQKV must persist.
(E) BUF_ATTN : S / SSCALED / P

Physical buffer	Logical tensor(s)	Alloc at	Last use	Free after	Arena	Notes
BUF_ATTN	S → SSCALED→ P	FWD k=6	BWD k=117(SOFTMAX_BWD consumes P)	after BWD k=117	LIFE_EPOCH	This is big if you materialize full attention matrix. If you implement block/flash attention, this buffer changes completely (see §7).
(F) BUF_O : O

Physical buffer	Logical tensor(s)	Alloc at	Last use	Free after	Arena	Notes
BUF_O	O	FWD k=9	BWD k=113 (DW_O GEMM uses O)	after BWD k=113	LIFE_EPOCH	Needed for dWo.
(G) BUF_X1 : Y → X1 (alias)

Physical buffer	Logical tensor(s)	Alloc at	Last use	Free after	Arena	Notes
BUF_X1	Y then X1	FWD k=10 (Y produced)	BWD k=110(RMSNorm2_BWD uses X1)	after BWD k=110	LIFE_EPOCH	Implement residual1 as in-place: Y += X_IN, then rename to X1. Safe if SKS_O_PROJ is SKS_START+WAIT.
(H) BUF_INV2 : INV_RMS2

Physical buffer	Logical tensor(s)	Alloc at	Last use	Free after	Arena	Notes
BUF_INV2	INV_RMS2	FWD k=14	BWD k=110	after BWD k=110	LIFE_EPOCH	Needed for RMSNorm2 backward.
(I) BUF_XN2 : XN2

Physical buffer	Logical tensor(s)	Alloc at	Last use	Free after	Arena	Notes
BUF_XN2	XN2	FWD k=14	BWD k=108 (DW1 GEMM uses XN2)	after BWD k=108	LIFE_EPOCH	Needed for dW1.
(J) BUF_H1 : H → H1 (alias)

Physical buffer	Logical tensor(s)	Alloc at	Last use	Free after	Arena	Notes
BUF_H1	H then H1	FWD k=15	BWD k=105(GELU_DERIV uses H1)	after BWD k=105	LIFE_EPOCH	Bias add in-place. Safe if SKS_MLP1 is SKS_START+WAIT.
(K) BUF_G : G

Physical buffer	Logical tensor(s)	Alloc at	Last use	Free after	Arena	Notes
BUF_G	G	FWD k=18	BWD k=103 (DW2 GEMM uses G)	after BWD k=103	LIFE_EPOCH	Avoid recomputing GELU forward (would require extra FSS+LIFT).
3.2 Forward ephemeral (can be freed early)

These do not need to survive to backward.

BUF_ZOUT : Z → Z1 → X_OUT

Physical buffer	Logical tensor(s)	Alloc at	Last use	Free after	Arena	Notes
BUF_ZOUT	Z, Z1, X_OUT	FWD k=19	FWD k=23 (SEND_PP reads X_OUT)	after SEND_PP enqueued	LIFE_EPOCH (or LIFE_INFLIGHT)	After sending, you can free once the PP send has staged/captured bytes (or after DMA completion if you require).
4) Liveness table: backward intermediates (free aggressively)

These are per-microbatch backward temps. Allocate from LIFE_EPOCH but free immediately at last use (pool reuse), or from LIFE_INFLIGHT if the op can yield.

Important: any op that can suspend on OPEN/LIFT must not hold LIFE_STEP buffers.

4.1 Backward temp liveness

Tensor	Produced at	Last use	Free after	Arena	Notes
DX_OUT	BWD k=100	BWD k=101	after k=101	LIFE_EPOCH	From PP recv
DZ1	BWD k=101	BWD k=104	after k=104	LIFE_EPOCH	Used for DB2, DW2, DG
DG	BWD k=104	BWD k=106	after k=106	LIFE_EPOCH	
gelu'(H1) temp	BWD k=105	BWD k=106	after k=106	LIFE_EPOCH	Best stored in the same buffer as DH1 then overwritten
DH1 (true)	BWD k=106	BWD k=109	after k=109	LIFE_EPOCH	Used for DB1, DW1, DXN2
DXN2	BWD k=109	BWD k=110	after k=110	LIFE_EPOCH	
DX1(accumulator)	BWD k=101	BWD k=111	after k=111	LIFE_EPOCH	Must be inout to RMSNORM2_BWD (accumulate)
DY	BWD k=111	BWD k=114	after k=114	LIFE_EPOCH	
DO	BWD k=114	BWD k=116	after k=116	LIFE_EPOCH	
DP	BWD k=115	BWD k=117	after k=117	LIFE_EPOCH	
DSCALED	BWD k=117	BWD k=118	after k=118	LIFE_EPOCH	
DS	BWD k=118	BWD k=120	after k=120	LIFE_EPOCH	
DQ	BWD k=119	BWD k=121	after k=121 (if packed)	LIFE_EPOCH	see DTQKV packing below
DK	BWD k=120	BWD k=121	after k=121 (if packed)	LIFE_EPOCH	
DV	BWD k=116	BWD k=121	after k=121 (if packed)	LIFE_EPOCH	
DTQKV	BWD k=121	BWD k=124	after k=124	LIFE_EPOCH	Pack/concat target; simplifies GEMMs
DXN1	BWD k=124	BWD k=125	after k=125	LIFE_EPOCH	
DX_IN(accumulator)	BWD k=111	BWD k=126	after k=126	LIFE_EPOCH	Must be inout to RMSNORM1_BWD
DTQKV packing decision (recommended)

At BWD k=121, do a linear pack:

allocate DTQKV (3× activation width)

write DQ, DK, DV into its 3 segments

then free DQ/DK/DV immediately after pack

This avoids accidental long retention of three separate buffers and tends to reduce fragmentation.

Packing is not MPC-critical; it’s just moving RSS shares locally.

5) Weight gradient buffers: do NOT allocate per microbatch

If you allocate DW2 etc per microbatch you will explode memory.

Instead, for each weight shard owned by this stage:

allocate one gradient accumulator slot per step (Life = LIFE_EPOCH)

each microbatch backward adds into it

5.1 Grad accumulators and lifetimes

Grad accumulator	Alloc at	Updated by	Consumed by	Free after	Arena
dW_QKV_accum	step begin	BWD k=123 each mb	DP-reduce + optimizer update	after update	LIFE_EPOCH
db_QKV_accum	step begin	BWD k=122 each mb	DP-reduce + update	after update	LIFE_EPOCH
dW_O_accum	step begin	BWD k=113 each mb	DP-reduce + update	after update	LIFE_EPOCH
db_O_accum	step begin	BWD k=112 each mb	DP-reduce + update	after update	LIFE_EPOCH
dW1_accum	step begin	BWD k=108 each mb	DP-reduce + update	after update	LIFE_EPOCH
db1_accum	step begin	BWD k=107 each mb	DP-reduce + update	after update	LIFE_EPOCH
dW2_accum	step begin	BWD k=103 each mb	DP-reduce + update	after update	LIFE_EPOCH
db2_accum	step begin	BWD k=102 each mb	DP-reduce + update	after update	LIFE_EPOCH
dGamma1_accum	step begin	BWD k=125 each mb	DP-reduce + update	after update	LIFE_EPOCH
dGamma2_accum	step begin	BWD k=110 each mb	DP-reduce + update	after update	LIFE_EPOCH
How to implement “add into accumulator” safely

Each producing op should output into a temporary (tile-local or full tensor), then do:

accum += tmp

free tmp

Or, if your secure GEMM supports “accumulate into destination”:

set destination = accum and run accum += gemm_result in one op (still deterministic)

Either way: accumulator is one per weight, not one per microbatch.

6) The exact “don’t leak” release strategy (what to implement)

To avoid arena leaks and over-allocation under PP (many microbatches in flight), you need deterministic frees.

6.1 Precompute last-use per TensorId (per program)

At compile time (or program load time):

compute last_use_pos[tensor] = max(pos of any op that reads it)

treat views as reads of the backing buffer

if Q is read, that counts as a read of TQKV buffer

Define pos ordering as:

(phase_order, k) with FWD < BWD (so backward uses are “later”)

6.2 Runtime: per microbatch, maintain physical buffer refcounts

For each physical buffer instance in this microbatch:

initialize live_refs = number of remaining reads (you can compute via a per-tensor refcount too)

After each op completes:

decrement refcount of each input tensor (really: input physical buffers)

if a buffer’s refcount hits 0 and it’s LIFE_EPOCH:

return to epoch pool (free list)

This is better than “free at the very end of backward” and is exactly what prevents over-alloc.

6.3 Special case: async ops and LIFE_STEP

Any op that can yield (OPEN/LIFT waiting) must not hold LIFE_STEP scratch.

So:

all OPEN/LIFT packing buffers, per-tile E/F buffers, lift m-vectors, etc:

either serialize immediately into TX cache (then free device scratch)

or allocate from LIFE_INFLIGHT and free on future completion

7) One critical realism note: attention S/P buffers can be enormous

In the literal program we pinned, S and P are materialized. If Tok_mb is large, S is [Tok_mb, Tok_mb], which is huge.

You have two implementation choices:

Option 7A (simple, matches the current StageProgram literally)

Materialize S/P fully.

liveness above is correct as written

peak memory can be very high

Option 7B (recommended “secure flash attention” style)

Do not materialize S and P fully:

compute softmax + PV in blocks

checkpoint only the statistics needed for backward (like row-wise normalizers)

backward recomputes blocks as needed

This changes the tensor list and liveness table substantially.
If you want, I can give you the exact modified k-table + liveness for blockwise attention too, but it’s a different StageProgram than the one you asked to keep literal.

8) Summary: the minimal checkpoint set you must keep per microbatch

If you implement the exact StageProgram we pinned with no recomputation, the per-microbatch forward checkpoint set is:

Must persist until backward completes:

X_IN

INV_RMS1

XN1

TQKV backing (covers Q,K,V)

P (in-place in BUF_ATTN)

O

X1 (can reuse Y buffer)

INV_RMS2

XN2

H1

G

Can be freed in forward immediately after send:

Z/Z1/X_OUT buffer

Backward intermediates: allocate and free exactly as in §4.

Weight grads: step accumulators only (§5).

